{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d4bb4a",
   "metadata": {},
   "source": [
    "# Random Sample Analysis: Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972f3444",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Tasks\n",
    "\n",
    "- Define query\n",
    "- Define functions\n",
    "- Describe current random sample for selected segments\n",
    "- Define necessary sample size for selected segment\n",
    "- Run simulations and then compare metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "60cce1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages\n",
    "import redshift_connector\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b400317f",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d6749568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params\n",
    "#do you need to download an existing data?\n",
    "download = False\n",
    "\n",
    "# Define the end date for your analysis (e.g., 7 days ago to allow for QA review time)s\n",
    "START_DATE = '2025-06-01'\n",
    "END_DATE = '2025-06-30'\n",
    "\n",
    "#OR Relative date range\n",
    "# days_back_for_sql = 30  # Number of days back from the end date for SQL\n",
    "# START_DATE = (end_date - timedelta(days=days_back_for_sql)).strftime('%Y-%m-%d')\n",
    "# END_DATE = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "# Other query parameters\n",
    "REVIEW_TYPE = ['random_sample', 'authentication_product'] #['random_sample', 'customer_reporting_1', 'authentication_product']\n",
    "REVIEW_TYPE_FOR_QUERY = ', '.join([\"'{}'\".format(review_type) for review_type in REVIEW_TYPE])\n",
    "\n",
    "# for Redshift connections\n",
    "REDSHIFT_HOST = \"wg-serverless-1.875431798560.eu-west-1.redshift-serverless.amazonaws.com\"\n",
    "REDSHIFT_DATABASE = \"vaire\"\n",
    "REDSHIFT_USER = os.environ.get(\"REDSHIFT_USER\")\n",
    "REDSHIFT_PASSWORD = os.environ.get(\"REDSHIFT_PASSWORD\")\n",
    "REDSHIFT_PORT = 5439\n",
    "\n",
    "## ready file if any\n",
    "file_path = 'sample_data_monthly.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfe707d",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b4bd487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "WITH \n",
    "customers as (\n",
    "select\n",
    "    fvs.vendor_display_name as customer\n",
    "    ,fvs.vendor_dim_id\n",
    "    ,COUNT(*) AS sessions\n",
    "from\n",
    "    analytics_mart_kpi.mart_kpi__verification_session_metrics fvs\n",
    "where true\n",
    "    and date(fvs.verified_at) >= CAST('{START_DATE}' AS DATE)\n",
    "    and date(fvs.verified_at) <= CAST('{END_DATE}' AS DATE)\n",
    "group by\n",
    "    1,2\n",
    ")\n",
    ", customer_ranked as (\n",
    "select \n",
    "    c.customer\n",
    "    , c.vendor_dim_id\n",
    "    , ROW_NUMBER() OVER (ORDER BY c.sessions DESC) as customer_rank\n",
    "from customers c\n",
    ")\n",
    ", data AS (\n",
    "select \n",
    "--dimensions\n",
    "    vs.document_country_sf_region as region\n",
    "    , coalesce(vs.document_specimen_country_name, 'UNKNOWN') as document_country\n",
    "    , coalesce(vs.geoip_country_iso2, 'UNKNOWN') as geoip_country\n",
    "    , coalesce(vs.platform, 'UNKNOWN') as platform\n",
    "    , coalesce(vs.primary_product, 'UNKNOWN') as primary_product\n",
    "    , coalesce(vs.primary_industry, 'UNKNOWN') as industry\n",
    "    , coalesce(vs.sf_customer_priority, 'UNKNOWN') as customer_priority\n",
    "    , coalesce(vs.vendor_display_name, 'UNKNOWN') as customer\n",
    "    , CASE WHEN c.customer_rank <= 20 THEN vs.vendor_display_name ELSE 'Other' END as customer_top_20\n",
    "    , CASE\n",
    "        WHEN vs.primary_product IN ('IDV', 'Document Verification', 'AIC') THEN 'IDV'\n",
    "        WHEN vs.primary_product LIKE 'Full Auto%' THEN 'Full Auto'\n",
    "        WHEN vs.primary_product LIKE '%Biometric%' OR vs.primary_product IN ('Face Match', 'Age Estimation') THEN 'Biometric Solutions'\n",
    "        WHEN vs.primary_product = 'Unknown' THEN 'Unknown'\n",
    "        ELSE 'Data Verification'\n",
    "    END as product_portfolio\n",
    "    --session related\n",
    "    , vs.verification_session_uuid\n",
    "    , CAST(vs.verified_at as DATE) as verified_date\n",
    "    , vs.verification_session_status\n",
    "    , vs.verification_session_status = 'declined' as is_decline\n",
    "    , vs.verification_session_status = 'approved' as is_approve\n",
    "    , vs.is_full_auto_session\n",
    "    --fraud\n",
    "    , vs.is_fraudulent_sessions_other or vs.is_fraudulent_sessions_velocity as is_declined_due_to_fraud\n",
    "    --qa related\n",
    "    , qa.verification_session_uuid as qa_verification_session_uuid\n",
    "    , qa.qa_review_type\n",
    "    , qa.flag_session_has_decision_error\n",
    "    , qa.flag_session_has_critical_approve_error\n",
    "    , qa.flag_session_has_critical_decline_error\n",
    "    , qa.flag_session_has_extraction_error\n",
    "    , qa.is_session_approved_fraud\n",
    "    , qa.is_session_fraud\n",
    "    , qa.qa_decision\n",
    "    , qa.original_decision\n",
    "    , qa.qa_decision = 'declined' as qa_declinable\n",
    "    , qa.qa_decision = 'approved' as qa_approvable\n",
    "from analytics_mart_kpi.mart_kpi__verification_session_metrics vs\n",
    "left join analytics_mart_automation.mart_automation__verification_session_qa_review qa\n",
    "    on vs.verification_session_uuid = qa.verification_session_uuid\n",
    "    and vs.shard_key = qa.shard_key\n",
    "    and qa.qa_review_type IN ({REVIEW_TYPE_FOR_QUERY})\n",
    "left join customer_ranked c on vs.vendor_dim_id = c.vendor_dim_id\n",
    "where true\n",
    "    and date(verified_at) >= CAST('{START_DATE}' AS DATE)\n",
    "    and date(verified_at) <= CAST('{END_DATE}' AS DATE)\n",
    "    --and vs.service_agreement_stage NOT IN ('test', 'test_low')\n",
    ")\n",
    ", unique_dates_count AS (\n",
    "    SELECT COUNT(DISTINCT verified_date) AS days_count\n",
    "    FROM data\n",
    ")\n",
    ", aggregated_data AS (\n",
    "select \n",
    "    udc.days_count as days_count\n",
    "    , 'Overall' as overall\n",
    "    , a.product_portfolio\n",
    "    , a.primary_product\n",
    "    , a.platform\n",
    "    , a.region\n",
    "    , a.document_country\n",
    "    , a.geoip_country\n",
    "    , a.industry\n",
    "    , a.customer\n",
    "    , a.customer_top_20\n",
    "    , a.customer_priority\n",
    "    , COALESCE(COUNT(DISTINCT a.verification_session_uuid),0) as volume\n",
    "    , COALESCE(COUNT(DISTINCT a.qa_verification_session_uuid),0) as qa_reviewed\n",
    "    --from total population\n",
    "    , SUM(a.is_full_auto_session::int) as full_auto_sessions\n",
    "    , SUM(a.is_decline::int) as declined_sessions\n",
    "    , SUM(a.is_approve::int) as approved_sessions\n",
    "    , SUM(a.is_declined_due_to_fraud::int) as declined_due_to_fraud_sessions\n",
    "    --from QA \n",
    "    , SUM(a.flag_session_has_decision_error::int) as decision_error_sessions\n",
    "    , SUM(a.flag_session_has_extraction_error::int) as extraction_error_sessions\n",
    "    , SUM(a.flag_session_has_critical_approve_error::int) as false_approves\n",
    "    , SUM(a.flag_session_has_critical_decline_error::int) as false_declines\n",
    "    , SUM(a.is_session_approved_fraud::int) as missed_fraud_sessions\n",
    "    , SUM(a.qa_declinable::int) as declinable_sessions\n",
    "    , SUM(a.qa_approvable::int) as approvable_sessions\n",
    "    , SUM(a.is_session_fraud::int) as fraud_sessions\n",
    "    , 1.00 * full_auto_sessions / NULLIF(volume, 0) as automation_rate \n",
    "    , 1.00 * declined_sessions / NULLIF(volume, 0) as decline_rate_in_population\n",
    "    , 1.00 * approved_sessions / NULLIF(volume, 0) as approve_rate_in_population\n",
    "    , 1.00 * declined_due_to_fraud_sessions / NULLIF(volume, 0) as fraud_rate_in_population\n",
    "    , 1.00 * decision_error_sessions / NULLIF(qa_reviewed, 0) as decision_error_rate\n",
    "    , 1.00 * extraction_error_sessions / NULLIF(qa_reviewed, 0) as extraction_error_rate\n",
    "    , 1.00 * false_approves / NULLIF(declinable_sessions, 0) as false_approve_rate\n",
    "    , 1.00 * false_declines / NULLIF(approvable_sessions, 0) as false_decline_rate\n",
    "    , 1.00 * missed_fraud_sessions / NULLIF(fraud_sessions, 0) as missed_fraud_rate\n",
    "    , 1.00 * declinable_sessions / NULLIF(qa_reviewed, 0) as decline_rate_in_qa\n",
    "    , 1.00 * approvable_sessions / NULLIF(qa_reviewed, 0) as approve_rate_in_qa\n",
    "    , 1.00 * fraud_sessions / NULLIF(qa_reviewed, 0) as fraud_rate_in_qa\n",
    "    , sum(volume) over () as total_volume\n",
    "    , sum(qa_reviewed) over () as total_qa_reviewed\n",
    "    , 1.00 * volume / NULLIF(total_volume,0) as proportion_of_total_volume\n",
    "    , 1.00 * qa_reviewed / NULLIF(total_qa_reviewed,0) as proportion_of_qa_sample\n",
    "from data a\n",
    "cross join unique_dates_count udc\n",
    "group by 1,2,3,4,5,6,7,8,9,10,11,12\n",
    "order by volume desc\n",
    ")\n",
    "SELECT *\n",
    "from aggregated_data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382fb25",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0234103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redshift Connector \n",
    "\n",
    "import redshift_connector\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Configure logging for better error visibility\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def get_redshift_dataframe(\n",
    "    sql_query: str,\n",
    "    host: str,\n",
    "    database: str,\n",
    "    user: str,\n",
    "    password: str,\n",
    "    port: int = 5439 # Default Redshift port\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Connects to Amazon Redshift, executes a SQL query, and returns the result\n",
    "    as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        sql_query (str): The SQL query string to execute.\n",
    "        host (str): The Redshift cluster endpoint (e.g., 'your-cluster.xxxx.region.redshift.amazonaws.com').\n",
    "        database (str): The name of the database to connect to.\n",
    "        user (str): The username for Redshift authentication.\n",
    "        password (str): The password for Redshift authentication.\n",
    "        port (int): The port number for the Redshift connection (default is 5439).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the query results.\n",
    "                      Returns an empty DataFrame if an error occurs or no data is found.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Re-raises any connection or query execution errors after logging.\n",
    "    \"\"\"\n",
    "    conn = None # Initialize connection to None\n",
    "    try:\n",
    "        logging.info(f\"Attempting to connect to Redshift database: {database}@{host}:{port} with user: {user}\")\n",
    "        conn = redshift_connector.connect(\n",
    "            host=host,\n",
    "            database=database,\n",
    "            user=user,\n",
    "            password=password,\n",
    "            port=port\n",
    "        )\n",
    "        logging.info(\"Successfully connected to Redshift.\")\n",
    "\n",
    "        logging.info(\"Executing SQL query...\")\n",
    "        # Use pandas read_sql for direct DataFrame conversion\n",
    "        df = pd.read_sql(sql_query, conn)\n",
    "        logging.info(f\"Query executed successfully. Fetched {len(df)} rows.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except redshift_connector.Error as e:\n",
    "        logging.error(f\"Redshift connection or query error: {e}\")\n",
    "        return pd.DataFrame() # Return empty DataFrame on specific Redshift errors\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "        # Optionally re-raise the exception if you want calling code to handle it\n",
    "        raise\n",
    "    finally:\n",
    "        if conn:\n",
    "            logging.info(\"Closing Redshift connection.\")\n",
    "            conn.close()\n",
    "        else:\n",
    "            logging.warning(\"Connection was not established, so nothing to close.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b450b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8ccafe5",
   "metadata": {},
   "source": [
    "## Run Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1c1b5a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 17:04:19,259 - INFO - Attempting to connect to Redshift database: vaire@wg-serverless-1.875431798560.eu-west-1.redshift-serverless.amazonaws.com:5439 with user: gadir_mamedov\n",
      "2025-07-29 17:04:19,768 - INFO - Successfully connected to Redshift.\n",
      "2025-07-29 17:04:19,769 - INFO - Executing SQL query...\n",
      "2025-07-29 17:06:12,116 - INFO - Query executed successfully. Fetched 84286 rows.\n",
      "2025-07-29 17:06:12,117 - INFO - Closing Redshift connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully fetched data from Redshift:\n",
      "   days_count  overall    product_portfolio                    primary_product platform region          document_country geoip_country        industry           customer    customer_top_20 customer_priority   volume  qa_reviewed  full_auto_sessions  declined_sessions  approved_sessions  declined_due_to_fraud_sessions  decision_error_sessions  extraction_error_sessions  false_approves  false_declines  missed_fraud_sessions  declinable_sessions  approvable_sessions  fraud_sessions  automation_rate  decline_rate_in_population  approve_rate_in_population  fraud_rate_in_population  decision_error_rate  extraction_error_rate  false_approve_rate  false_decline_rate  missed_fraud_rate  decline_rate_in_qa  approve_rate_in_qa  fraud_rate_in_qa  total_volume  total_qa_reviewed  proportion_of_total_volume  proportion_of_qa_sample\n",
      "0          30  Overall            Full Auto  Full Auto IDV (Selfie & Document)      api   AMER  United States of America       UNKNOWN        Software              ID.me              ID.me     Tier Platinum  2267663         1914           2267663.0              40508            2037775                              15                     22.0                        0.0             2.0             1.0                    2.0                 22.0               1796.0            25.0         1.000000                    0.017863                    0.898623                  0.000007             0.011494               0.000000            0.090909            0.000557           0.080000            0.011494            0.938349          0.013062      21678143              41530                    0.104606                 0.046087\n",
      "1          30  Overall  Biometric Solutions           Biometric Authentication      sdk   None                   UNKNOWN       US               Retail          Instacart          Instacart     Tier Platinum  1548649          646           1548192.0              20281            1528368                            6091                      0.0                        NaN             0.0             0.0                    0.0                 12.0                628.0            12.0         0.999705                    0.013096                    0.986904                  0.003933             0.000000                    NaN            0.000000            0.000000           0.000000            0.018576            0.972136          0.018576      21678143              41530                    0.071438                 0.015555\n",
      "2          30  Overall            Full Auto  Full Auto IDV (Selfie & Document)      api  LATAM                    Mexico       UNKNOWN  Transportation  Uber Main Account  Uber Main Account     Tier Platinum   658530         1416            658530.0              27412             526940                             115                    127.0                       12.0            23.0             4.0                    2.0                 72.0               1068.0            48.0         1.000000                    0.041626                    0.800176                  0.000175             0.089689               0.008475            0.319444            0.003745           0.041667            0.050847            0.754237          0.033898      21678143              41530                    0.030378                 0.034096\n",
      "3          30  Overall  Biometric Solutions         Biometric Passive Liveness      web   None                   UNKNOWN       US       Transportation  Uber Main Account  Uber Main Account     Tier Platinum   595247          106            575254.0              18728             557172                            2799                      2.0                        NaN             0.0             0.0                    0.0                  0.0                 88.0             0.0         0.966412                    0.031463                    0.936035                  0.004702             0.018868                    NaN                 NaN            0.000000                NaN            0.000000            0.830189          0.000000      21678143              41530                    0.027458                 0.002552\n",
      "4          30  Overall                  IDV              Document Verification      api   AMER  United States of America       UNKNOWN  Transportation  Uber Main Account  Uber Main Account     Tier Platinum   527278          904            496502.0              16808             471936                            1371                     63.0                       11.0             1.0             3.0                    1.0                 21.0                790.0            21.0         0.941632                    0.031877                    0.895042                  0.002600             0.069690               0.012168            0.047619            0.003797           0.047619            0.023230            0.873894          0.023230      21678143              41530                    0.024323                 0.021767\n",
      "\n",
      "DataFrame shape: (84286, 42)\n"
     ]
    }
   ],
   "source": [
    "if download:\n",
    "    df = pd.read_csv(file_path)\n",
    "else:\n",
    "    df = get_redshift_dataframe(\n",
    "                sql_query=query, # Pass the multi-line query variable here\n",
    "                host=REDSHIFT_HOST,\n",
    "                database=REDSHIFT_DATABASE,\n",
    "                user=REDSHIFT_USER,\n",
    "                password=REDSHIFT_PASSWORD,\n",
    "                port=REDSHIFT_PORT\n",
    "            )\n",
    "\n",
    "    if not df.empty:\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(\"\\nSuccessfully fetched data from Redshift:\")\n",
    "        print(df.head())\n",
    "        print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "    else:\n",
    "        print(\"\\nNo data fetched or an error occurred.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b103a68",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "34e32c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN CODE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy import stats\n",
    "import math\n",
    "import warnings\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from typing import List, Dict, Any, Union, Tuple, Callable\n",
    "\n",
    "np.random.seed(20)\n",
    "\n",
    "# Suppress all warnings for cleaner output during execution.\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration Parameters (Default Values for UI) ---\n",
    "# These parameters serve as initial default values for the interactive UI.\n",
    "# They will be updated by the UI input and passed via the 'params' dictionary.\n",
    "DEFAULT_SELECTED_METRIC = 'Missed Fraud Rate'      \n",
    "DEFAULT_TARGET_OVERALL_ERROR_PERCENT = 2.0         \n",
    "DEFAULT_TARGET_DIMENSION_ERROR_PERCENT = 2.0       \n",
    "DEFAULT_CONFIDENCE_LEVEL = 0.95                    \n",
    "DEFAULT_REVIEW_COST_PER_SESSION = 0.6       \n",
    "DEFAULT_PREDEFINED_TOTAL_COST_BUDGET = 60000.0   \n",
    "DEFAULT_WEIGHT_FACTOR_SMALL_CUSTOMERS = 10        \n",
    "DEFAULT_SMALL_CUSTOMER_VOLUME_QUANTILE_THRESHOLD = 0.99\n",
    "DEFAULT_ERROR_MARGIN_LOGIC = 'Absolute Error Margin'\n",
    "DEFAULT_ABSOLUTE_ERROR_MARGIN_VALUE = 0.02         \n",
    "DEFAULT_RELATIVE_ERROR_MARGIN_VALUE = 0.2         \n",
    "DEFAULT_MIN_ABSOLUTE_ERROR_MARGIN = 0.005          \n",
    "DEFAULT_REPORTING_DIMENSIONS = ['customer', 'customer_top_20', 'primary_product', 'industry', 'region','document_country', 'platform', 'customer_priority', 'product_portfolio']        \n",
    "DEFAULT_STRATIFY_BY_DIMENSION = ['primary_product', 'industry', 'region', 'platform', 'customer_top_20', 'document_country']\n",
    "DEFAULT_AVAILABLE_PRODUCTS = ['Full Auto IDV (Document Only)', 'Full Auto IDV (Selfie & Document)', 'Document Verification', 'IDV', 'Biometric Authentication', 'Biometric Enrolment', 'Biometric Passive Liveness']\n",
    "DEFAULT_TOP_N = 10                                 \n",
    "DEFAULT_SAVE_PLOTS_TO_HTML = False          \n",
    "DEFAULT_MIN_PER_STRATUM = 1000       \n",
    "\n",
    "# --- Global Variables (will be set by UI or default values in generate_report) ---\n",
    "# These are defined globally for convenience but will be effectively\n",
    "# managed by the 'params' dictionary passed to generate_report.\n",
    "SELECTED_METRIC = DEFAULT_SELECTED_METRIC\n",
    "TARGET_OVERALL_ERROR_PERCENT = DEFAULT_TARGET_OVERALL_ERROR_PERCENT\n",
    "TARGET_DIMENSION_ERROR_PERCENT = DEFAULT_TARGET_DIMENSION_ERROR_PERCENT\n",
    "CONFIDENCE_LEVEL = DEFAULT_CONFIDENCE_LEVEL\n",
    "REVIEW_COST_PER_SESSION = DEFAULT_REVIEW_COST_PER_SESSION\n",
    "PREDEFINED_TOTAL_COST_BUDGET = DEFAULT_PREDEFINED_TOTAL_COST_BUDGET\n",
    "WEIGHT_FACTOR_SMALL_CUSTOMERS = DEFAULT_WEIGHT_FACTOR_SMALL_CUSTOMERS\n",
    "SMALL_CUSTOMER_VOLUME_QUANTILE_THRESHOLD = DEFAULT_SMALL_CUSTOMER_VOLUME_QUANTILE_THRESHOLD\n",
    "ERROR_MARGIN_LOGIC = DEFAULT_ERROR_MARGIN_LOGIC\n",
    "ABSOLUTE_ERROR_MARGIN_VALUE = DEFAULT_ABSOLUTE_ERROR_MARGIN_VALUE\n",
    "RELATIVE_ERROR_MARGIN_VALUE = DEFAULT_RELATIVE_ERROR_MARGIN_VALUE\n",
    "MIN_ABSOLUTE_ERROR_MARGIN = DEFAULT_MIN_ABSOLUTE_ERROR_MARGIN\n",
    "REPORTING_DIMENSIONS = DEFAULT_REPORTING_DIMENSIONS\n",
    "STRATIFY_BY_DIMENSION = DEFAULT_STRATIFY_BY_DIMENSION\n",
    "TOP_N = DEFAULT_TOP_N\n",
    "SAVE_PLOTS_TO_HTML = DEFAULT_SAVE_PLOTS_TO_HTML\n",
    "\n",
    "TARGET_QA_PROPORTIONS = {\n",
    "    'Tier Platinum': 0.20,\n",
    "    'Tier 1': 0.15,\n",
    "    'Tier 2': 0.15,\n",
    "    'Tier 3': 0.15,\n",
    "    'Tier 4': 0.15,\n",
    "    'Tier 5': 0.10,\n",
    "    'UNKNOWN': 0.10,\n",
    "    '': 0.10  # For unknown/blank\n",
    "}\n",
    "\n",
    "# Pre-calculate Z-score for the given confidence level.\n",
    "# This Z-score is used in confidence interval calculations (e.g., Wilson Score).\n",
    "Z_SCORE = stats.norm.ppf(1 - (1 - CONFIDENCE_LEVEL) / 2)\n",
    "\n",
    "# --- Metric Properties ---\n",
    "# This dictionary defines the columns associated with each metric,\n",
    "# including numerator, denominator, and base for frequency calculations.\n",
    "METRIC_PROPERTIES = {\n",
    "    'Decision Error Rate': {\n",
    "        'numerator_col': 'decision_error_sessions',\n",
    "        'denominator_col': 'qa_reviewed',\n",
    "        'freq_denominator_base_col': 'qa_reviewed',\n",
    "    },\n",
    "    'Extraction Error Rate': {\n",
    "        'numerator_col': 'extraction_error_sessions',\n",
    "        'denominator_col': 'qa_reviewed',\n",
    "        'freq_denominator_base_col': 'qa_reviewed',\n",
    "    },\n",
    "    'False Approve Rate': {\n",
    "        'numerator_col': 'false_approves',\n",
    "        'denominator_col': 'declinable_sessions',\n",
    "        'freq_denominator_base_col': 'qa_reviewed',\n",
    "    },\n",
    "    'False Decline Rate': {\n",
    "        'numerator_col': 'false_declines',\n",
    "        'denominator_col': 'approvable_sessions',\n",
    "        'freq_denominator_base_col': 'qa_reviewed',\n",
    "    },\n",
    "    'Missed Fraud Rate': {\n",
    "        'numerator_col': 'missed_fraud_sessions',\n",
    "        'denominator_col': 'fraud_sessions',\n",
    "        'freq_denominator_base_col': 'qa_reviewed',\n",
    "    },\n",
    "    'automation_rate': {\n",
    "        'numerator_col': 'full_auto_sessions',\n",
    "        'denominator_col': 'volume',\n",
    "        'freq_denominator_base_col': 'volume',\n",
    "    },\n",
    "    'decline_rate_in_population': {\n",
    "        'numerator_col': 'declined_sessions',\n",
    "        'denominator_col': 'volume',\n",
    "        'freq_denominator_base_col': 'volume'\n",
    "    },\n",
    "    'approve_rate_in_population': {\n",
    "        'numerator_col': 'approved_sessions',\n",
    "        'denominator_col': 'volume',\n",
    "        'freq_denominator_base_col': 'volume'\n",
    "    },\n",
    "    'fraud_rate_in_population': {\n",
    "        'numerator_col': 'declined_due_to_fraud_sessions',\n",
    "        'denominator_col': 'volume',\n",
    "        'freq_denominator_base_col': 'volume'\n",
    "    },\n",
    "    'fraud_rate_in_qa': {\n",
    "        'numerator_col': 'fraud_sessions',\n",
    "        'denominator_col': 'qa_reviewed',\n",
    "        'freq_denominator_base_col': 'qa_reviewed'\n",
    "    },\n",
    "    'decline_rate_in_qa': {\n",
    "        'numerator_col': 'declinable_sessions',\n",
    "        'denominator_col': 'qa_reviewed',\n",
    "        'freq_denominator_base_col': 'qa_reviewed'\n",
    "    },\n",
    "    'approve_rate_in_qa': {\n",
    "        'numerator_col': 'approvable_sessions',\n",
    "        'denominator_col': 'qa_reviewed',\n",
    "        'freq_denominator_base_col': 'qa_reviewed'\n",
    "    },\n",
    "}\n",
    "\n",
    "# --- Statistical Calculation Functions ---\n",
    "\n",
    "def assign_true_random_sample(df: pd.DataFrame, total_qa_reviewed: int, random_state: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assigns a truly random sample (proportional to volume) to the DataFrame.\n",
    "    Adds a column 'true_random_qa_reviewed' with the number of QA-reviewed sessions per row.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    df = df.copy()\n",
    "    if df['volume'].sum() == 0:\n",
    "        df['true_random_qa_reviewed'] = 0\n",
    "        return df\n",
    "    probs = df['volume'] / df['volume'].sum()\n",
    "    df['true_random_qa_reviewed'] = np.random.multinomial(total_qa_reviewed, probs)\n",
    "    return df\n",
    "\n",
    "def calculate_adjusted_proportion_agresti_coull(numerator: Union[float, np.ndarray], denominator: Union[float, np.ndarray], z_score: float) -> Union[float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculates the Agresti-Coull adjusted proportion.\n",
    "    \"\"\"\n",
    "    numerator = np.asarray(numerator).astype(float)\n",
    "    denominator = np.asarray(denominator).astype(float)\n",
    "    \n",
    "    z_squared = z_score**2\n",
    "    adjusted_num = numerator + z_squared / 2\n",
    "    adjusted_den = denominator + z_squared\n",
    "\n",
    "    result = np.where(adjusted_den == 0, 0.0, adjusted_num / adjusted_den)\n",
    "    return result\n",
    "\n",
    "# def calculate_margin_wilson(k: Union[float, np.ndarray], n: Union[float, np.ndarray], z_score: float, min_required_n=None) -> Dict[str, Union[float, np.ndarray]]:\n",
    "#     \"\"\"\n",
    "#     Calculates the error margin for a proportion using the Wilson Score Interval.\n",
    "#     \"\"\"\n",
    "#     k = np.asarray(k).astype(float)\n",
    "#     n = np.asarray(n).astype(float)\n",
    "\n",
    "#     p_hat = np.where(n > 0, k / n, 0.0)\n",
    "\n",
    "#     z_squared = z_score**2\n",
    "    \n",
    "#     denominator_wilson = np.where(n > 0, 1.0 + z_squared / n, np.nan) \n",
    "\n",
    "#     sqrt_content = np.where(n > 0, \n",
    "#                             p_hat * (1 - p_hat) / n + z_squared / (4 * n**2), \n",
    "#                             0.0) \n",
    "\n",
    "#     numerator_wilson_base = np.where(n > 0, p_hat + z_squared / (2 * n), np.nan) \n",
    "\n",
    "#     lower_bound = np.where(\n",
    "#         (denominator_wilson > 0) & np.isfinite(numerator_wilson_base),\n",
    "#         (numerator_wilson_base - z_score * np.sqrt(sqrt_content)) / denominator_wilson,\n",
    "#         np.nan\n",
    "#     )\n",
    "#     upper_bound = np.where(\n",
    "#         (denominator_wilson > 0) & np.isfinite(numerator_wilson_base),\n",
    "#         (numerator_wilson_base + z_score * np.sqrt(sqrt_content)) / denominator_wilson,\n",
    "#         np.nan\n",
    "#     )\n",
    "\n",
    "#     lower_bound = np.maximum(0.0, lower_bound)\n",
    "#     upper_bound = np.minimum(1.0, upper_bound)\n",
    "\n",
    "#     margin_abs = (upper_bound - lower_bound) / 2\n",
    "\n",
    "#     # # If denominator is zero, margin_abs should be np.nan\n",
    "#     # margin_abs = np.where(n == 0, np.nan, margin_abs)\n",
    "#     # # margin_rel: if margin_abs is nan or p_hat==0, set to np.nan\n",
    "#     # margin_rel = np.where((margin_abs == 0) & (p_hat > 0), 0.0, np.where((margin_abs == 0) & (p_hat == 0), np.nan, margin_abs / p_hat))\n",
    "#     # margin_rel = np.where(n == 0, np.nan, margin_rel)\n",
    "\n",
    "#     # return {'margin': margin_abs, 'margin_rel': margin_rel}\n",
    "\n",
    "#     margin_rel = np.where(p_hat > 0, margin_abs / p_hat, np.inf)\n",
    "\n",
    "#     # --- PATCH: If n < min_required_n, set margin to np.nan or flag ---\n",
    "#     if min_required_n is not None:\n",
    "#         margin_abs = np.where(n < min_required_n, np.nan, margin_abs)\n",
    "#         margin_rel = np.where(n < min_required_n, np.nan, margin_rel)\n",
    "\n",
    "#     margin_abs = np.nan_to_num(margin_abs, nan=0.9, posinf=0.9, neginf=0.9)\n",
    "#     margin_rel = np.where(margin_abs == 0, np.nan, np.nan_to_num(margin_rel, nan=0.9, posinf=0.9, neginf=0.9))\n",
    "\n",
    "#     return {'margin': margin_abs, 'margin_rel': margin_rel}\n",
    "\n",
    "def calculate_margin_wilson(k: Union[float, np.ndarray], n: Union[float, np.ndarray], z_score: float, min_required_n=None) -> Dict[str, Union[float, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Calculates the error margin for a proportion using the Wilson Score Interval.\n",
    "    If denominator is zero, assigns a large error margin (e.g., 0.9).\n",
    "    \"\"\"\n",
    "    k = np.asarray(k).astype(float)\n",
    "    n = np.asarray(n).astype(float)\n",
    "\n",
    "    p_hat = np.where(n > 0, k / n, 0.0)\n",
    "    z_squared = z_score**2\n",
    "\n",
    "    denominator_wilson = np.where(n > 0, 1.0 + z_squared / n, np.nan)\n",
    "    sqrt_content = np.where(n > 0, p_hat * (1 - p_hat) / n + z_squared / (4 * n**2), 0.0)\n",
    "    numerator_wilson_base = np.where(n > 0, p_hat + z_squared / (2 * n), np.nan)\n",
    "\n",
    "    lower_bound = np.where(\n",
    "        (denominator_wilson > 0) & np.isfinite(numerator_wilson_base),\n",
    "        (numerator_wilson_base - z_score * np.sqrt(sqrt_content)) / denominator_wilson,\n",
    "        np.nan\n",
    "    )\n",
    "    upper_bound = np.where(\n",
    "        (denominator_wilson > 0) & np.isfinite(numerator_wilson_base),\n",
    "        (numerator_wilson_base + z_score * np.sqrt(sqrt_content)) / denominator_wilson,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    lower_bound = np.maximum(0.0, lower_bound)\n",
    "    upper_bound = np.minimum(1.0, upper_bound)\n",
    "\n",
    "    margin_abs = (upper_bound - lower_bound) / 2\n",
    "\n",
    "    # If denominator is zero, assign large error margin (e.g., 0.9)\n",
    "    margin_abs = np.where(n == 0, 0.9, margin_abs)\n",
    "    margin_abs = np.nan_to_num(margin_abs, nan=0.9, posinf=0.9, neginf=0.9)\n",
    "\n",
    "    margin_rel = np.where(p_hat > 0, margin_abs / p_hat, 0.9)\n",
    "    margin_rel = np.where(n == 0, 0.9, margin_rel)\n",
    "    margin_rel = np.nan_to_num(margin_rel, nan=0.9, posinf=0.9, neginf=0.9)\n",
    "\n",
    "    # Optionally: If min_required_n is set, set margin to 0.9 if n < min_required_n\n",
    "    if min_required_n is not None:\n",
    "        margin_abs = np.where(n < min_required_n, 0.9, margin_abs)\n",
    "        margin_rel = np.where(n < min_required_n, 0.9, margin_rel)\n",
    "\n",
    "    return {'margin': margin_abs, 'margin_rel': margin_rel}\n",
    "\n",
    "def get_margins_df(df: pd.DataFrame, k_col: str, n_col: str, z_score: float,  min_required_n=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies the Wilson Score interval calculation to DataFrame columns to get error margins.\n",
    "    \"\"\"\n",
    "    k = df[k_col].values\n",
    "    n = df[n_col].values\n",
    "    results = calculate_margin_wilson(k, n, z_score, min_required_n=min_required_n)\n",
    "    \n",
    "    results_df = pd.DataFrame(results, index=df.index)\n",
    "     # Flag problematic rows\n",
    "    results_df['margin_flag'] = np.where(\n",
    "        (df[n_col] == 0) | (np.isnan(results_df['margin'])) | (np.isinf(results_df['margin'])),\n",
    "        'Problematic', 'OK'\n",
    "    )\n",
    "    return pd.concat([df, results_df], axis=1)\n",
    "\n",
    "def calculate_required_sample_size_proportion(population_proportion: float, absolute_margin_of_error: float, confidence_level: float, population_size: int, z_score: float) -> Union[int, float]:\n",
    "    \"\"\"\n",
    "    Calculates the required sample size for estimating a population proportion\n",
    "    with a specified absolute margin of error and confidence level, using the finite population correction factor.\n",
    "    \"\"\"\n",
    "    z = z_score\n",
    "    p = population_proportion\n",
    "    e = absolute_margin_of_error\n",
    "\n",
    "    if p == 0: p = 1e-9\n",
    "    if p == 1: p = 1 - 1e-9\n",
    "\n",
    "    numerator_val = (z**2 * p * (1 - p)) / e**2\n",
    "\n",
    "    if population_size <= 1:\n",
    "        denominator_fpc = 1.0\n",
    "    else:\n",
    "        denominator_fpc = 1.0 + (numerator_val / (population_size - 1))\n",
    "\n",
    "    if denominator_fpc == 0:\n",
    "        return float('inf')\n",
    "\n",
    "    n = numerator_val / denominator_fpc\n",
    "\n",
    "    # --- FIX: handle NaN or inf ---\n",
    "    if not np.isfinite(n) or np.isnan(n):\n",
    "        return float('inf')\n",
    "    return max(1, math.ceil(n))\n",
    "\n",
    "# --- Data Preparation Functions ---\n",
    "\n",
    "def get_tier_weights_by_target_proportion(df, target_proportions):\n",
    "    # Defensive: If 'customer_priority' or 'volume' missing, return all weights as 1\n",
    "    if 'customer_priority' not in df.columns or 'volume' not in df.columns:\n",
    "        # Return a dict with all tiers set to 1.0\n",
    "        return {tier: 1.0 for tier in target_proportions}\n",
    "    # Calculate actual volume proportions\n",
    "    actual = df.groupby('customer_priority')['volume'].sum()\n",
    "    total = actual.sum()\n",
    "    actual_proportions = (actual / total).to_dict()\n",
    "    # Calculate weights as ratio of target to actual\n",
    "    weights = {}\n",
    "    for tier, target in target_proportions.items():\n",
    "        actual_prop = actual_proportions.get(tier, 0.01)  # avoid zero\n",
    "        weights[tier] = target / actual_prop\n",
    "    return weights\n",
    "\n",
    "def calculate_all_rates(df: pd.DataFrame, metric_properties: Dict[str, Dict[str, str]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates various rates (e.g., automation rate, error rates) based on the provided DataFrame.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    rates_to_calculate = {\n",
    "        'automation_rate': ('full_auto_sessions', 'volume'),\n",
    "        'decline_rate_in_population': ('declined_sessions', 'volume'),\n",
    "        'approve_rate_in_population': ('approved_sessions', 'volume'),\n",
    "        'fraud_rate_in_population': ('declined_due_to_fraud_sessions', 'volume'),\n",
    "        'decision_error_rate': ('decision_error_sessions', 'qa_reviewed'),\n",
    "        'extraction_error_rate': ('extraction_error_sessions', 'qa_reviewed'),\n",
    "        'false_approve_rate': ('false_approves', 'declinable_sessions'),\n",
    "        'false_decline_rate': ('false_declines', 'approvable_sessions'),\n",
    "        'missed_fraud_rate': ('missed_fraud_sessions', 'fraud_sessions'),\n",
    "        'decline_rate_in_qa': ('declinable_sessions', 'qa_reviewed'),\n",
    "        'approve_rate_in_qa': ('approvable_sessions', 'qa_reviewed'),\n",
    "        'fraud_rate_in_qa': ('fraud_sessions', 'qa_reviewed'),\n",
    "    }\n",
    "\n",
    "    for rate_name, (numerator_col, denominator_col) in rates_to_calculate.items():\n",
    "        if numerator_col in df_copy.columns and denominator_col in df_copy.columns:\n",
    "            df_copy.loc[:, rate_name] = df_copy[numerator_col] / df_copy[denominator_col].replace(0, np.nan)\n",
    "        else:\n",
    "            warnings.warn(f\"Columns '{numerator_col}' or '{denominator_col}' not found for calculating '{rate_name}'. Skipping rate calculation.\")\n",
    "            df_copy.loc[:, rate_name] = np.nan\n",
    "            \n",
    "    df_copy.fillna(0, inplace=True)\n",
    "    return df_copy\n",
    "\n",
    "def aggregate_population_df(df, reporting_dimensions, metric_properties):\n",
    "    agg_dict = {col: 'sum' for props in metric_properties.values() for col in [props['numerator_col'], props['denominator_col'], props['freq_denominator_base_col']] if col in df.columns}\n",
    "    agg_dict['volume'] = 'sum'\n",
    "    return df.groupby(reporting_dimensions, as_index=False).agg(agg_dict)\n",
    "\n",
    "def prepare_population_data(df: pd.DataFrame, selected_metric: str, metric_properties: Dict[str, Dict[str, str]], z_score: float, reporting_dimensions: list = None) -> Tuple[pd.DataFrame, Dict[str, float], Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Prepares the aggregated population data by calculating all rates\n",
    "    and global average metrics/frequencies.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Aggregate by reporting dimensions if provided\n",
    "    if reporting_dimensions is not None and len(reporting_dimensions) > 0:\n",
    "        df = aggregate_population_df(df, reporting_dimensions, metric_properties)\n",
    "    df = calculate_all_rates(df, metric_properties)\n",
    "\n",
    "    global_avg_metrics = {}\n",
    "    for metric_name, props in metric_properties.items():\n",
    "        num_col = props['numerator_col']\n",
    "        den_col = props['denominator_col']\n",
    "        if num_col in df.columns and den_col in df.columns:\n",
    "            total_numerator = df[num_col].sum()\n",
    "            total_denominator = df[den_col].sum()\n",
    "            if total_denominator > 0:\n",
    "                global_avg_metrics[metric_name] = calculate_adjusted_proportion_agresti_coull(total_numerator, total_denominator, z_score)\n",
    "            else:\n",
    "                global_avg_metrics[metric_name] = 0\n",
    "        else:\n",
    "            global_avg_metrics[metric_name] = 0\n",
    "\n",
    "    overall_avg_frequencies_in_qa = {}\n",
    "    for metric_name, props in metric_properties.items():\n",
    "        base_col = props['freq_denominator_base_col']\n",
    "        den_col = props['denominator_col']\n",
    "        if base_col in df.columns and den_col in df.columns:\n",
    "            base_col_sum = df[base_col].sum()\n",
    "            denominator_col_sum = df[den_col].sum()\n",
    "            overall_avg_frequencies_in_qa[metric_name] = (denominator_col_sum / base_col_sum) if base_col_sum > 0 else 0\n",
    "        else:\n",
    "            overall_avg_frequencies_in_qa[metric_name] = 0\n",
    "\n",
    "    return df, global_avg_metrics, overall_avg_frequencies_in_qa\n",
    "\n",
    "def calculate_weighted_metric(df: pd.DataFrame, metric: str, weight_col: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates weighted average for a metric using provided weights.\n",
    "    \"\"\"\n",
    "    props = METRIC_PROPERTIES[metric]\n",
    "    num = df[props['numerator_col']]\n",
    "    den = df[props['denominator_col']]\n",
    "    weights = df[weight_col]\n",
    "    weighted_num = (num * weights).sum()\n",
    "    weighted_den = (den * weights).sum()\n",
    "    return weighted_num / weighted_den if weighted_den > 0 else np.nan\n",
    "\n",
    "# def prepare_population_data(df: pd.DataFrame, selected_metric: str, metric_properties: Dict[str, Dict[str, str]], z_score: float) -> Tuple[pd.DataFrame, Dict[str, float], Dict[str, float]]:\n",
    "#     \"\"\"\n",
    "#     Prepares the aggregated population data by calculating all rates\n",
    "#     and global average metrics/frequencies.\n",
    "#     \"\"\"\n",
    "#     df = df.copy()\n",
    "#     df = df.\n",
    "#     df = calculate_all_rates(df, metric_properties)\n",
    "\n",
    "#     global_avg_metrics = {}\n",
    "#     for metric_name, props in metric_properties.items():\n",
    "#         num_col = props['numerator_col']\n",
    "#         den_col = props['denominator_col']\n",
    "#         if num_col in df.columns and den_col in df.columns:\n",
    "#             total_numerator = df[num_col].sum()\n",
    "#             total_denominator = df[den_col].sum()\n",
    "#             if total_denominator > 0:\n",
    "#                 global_avg_metrics[metric_name] = calculate_adjusted_proportion_agresti_coull(total_numerator, total_denominator, z_score)\n",
    "#             else:\n",
    "#                 global_avg_metrics[metric_name] = 0\n",
    "#         else:\n",
    "#             global_avg_metrics[metric_name] = 0\n",
    "    \n",
    "#     overall_avg_frequencies_in_qa = {}\n",
    "#     for metric_name, props in metric_properties.items():\n",
    "#         base_col = props['freq_denominator_base_col']\n",
    "#         den_col = props['denominator_col']\n",
    "#         if base_col in df.columns and den_col in df.columns:\n",
    "#             base_col_sum = df[base_col].sum()\n",
    "#             denominator_col_sum = df[den_col].sum()\n",
    "#             overall_avg_frequencies_in_qa[metric_name] = (denominator_col_sum / base_col_sum) if base_col_sum > 0 else 0\n",
    "#         else:\n",
    "#             overall_avg_frequencies_in_qa[metric_name] = 0\n",
    "    \n",
    "#     return df, global_avg_metrics, overall_avg_frequencies_in_qa\n",
    "\n",
    "def get_frequency_of_denominator(df: pd.DataFrame, metric: str, metric_properties: Dict[str, Dict[str, str]], overall_avg_frequencies_in_qa: Dict[str, float]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the frequency of the denominator for a given metric within the DataFrame.\n",
    "    \"\"\"\n",
    "    props = metric_properties[metric]\n",
    "    freq_base_col = props['freq_denominator_base_col']\n",
    "    den_col = props['denominator_col']\n",
    "\n",
    "    if freq_base_col not in df.columns or den_col not in df.columns:\n",
    "        warnings.warn(f\"Frequency base column '{freq_base_col}' or denominator column '{den_col}' not found in DataFrame for metric '{metric}'. Using global frequency as fallback for all rows.\")\n",
    "        return np.full(len(df), overall_avg_frequencies_in_qa.get(metric, 0))\n",
    "\n",
    "    freq = df[den_col] / df[freq_base_col].replace(0, np.nan)\n",
    "    freq = freq.fillna(0.0)\n",
    "\n",
    "    MIN_FREQ = 0.01  # Set a reasonable minimum frequency (e.g., 1%)\n",
    "    global_freq_val = max(overall_avg_frequencies_in_qa.get(metric, MIN_FREQ), MIN_FREQ)\n",
    "    freq = np.where(freq < MIN_FREQ, global_freq_val, freq)\n",
    "    freq = np.maximum(freq, MIN_FREQ)\n",
    "\n",
    "    return freq\n",
    "\n",
    "\n",
    "# def calculate_required_sessions(\n",
    "#     df: pd.DataFrame,\n",
    "#     metric: str,\n",
    "#     global_rates: Dict[str, float],\n",
    "#     global_freq: Dict[str, float],\n",
    "#     error_margin_logic: str,\n",
    "#     absolute_error_margin_value: float,\n",
    "#     relative_error_margin_value: float,\n",
    "#     min_absolute_error_margin: float,\n",
    "#     confidence_level: float,\n",
    "#     z_score: float\n",
    "# ) -> Tuple[List[Union[int, float]], List[Union[int, float]], np.ndarray]:\n",
    "#     \"\"\"\n",
    "#     Calculates the required QA reviewed sessions for each group (row in df) to achieve\n",
    "#     the target error margin for the selected metric.\n",
    "#     If denominator is zero, estimate denominator as group volume * global_freq,\n",
    "#     and numerator as estimated_denominator * global_rate.\n",
    "#     Returns: (required_sample_size, extra_reviews, adj_error_margin)\n",
    "#     \"\"\"\n",
    "#     props = METRIC_PROPERTIES[metric]\n",
    "#     num_col = props['numerator_col']\n",
    "#     den_col = props['denominator_col']\n",
    "#     freq_base_col = props['freq_denominator_base_col']\n",
    "\n",
    "#     if den_col not in df.columns or num_col not in df.columns:\n",
    "#         warnings.warn(f\"Numerator '{num_col}' or Denominator '{den_col}' column not found in input DataFrame for `calculate_required_sessions`. Cannot proceed.\")\n",
    "#         return [float('inf')] * len(df), [float('inf')] * len(df), np.full(len(df), np.nan)\n",
    "\n",
    "#     denominator_n_series = df[den_col].astype(float)\n",
    "#     numerator_series = df[num_col].astype(float)\n",
    "#     volume_series = df['volume'].astype(float) if 'volume' in df.columns else denominator_n_series\n",
    "\n",
    "#     global_rate = global_rates.get(metric, 0)\n",
    "#     global_freq_val = global_freq.get(metric, 0)\n",
    "\n",
    "#     adjusted_p_estimates = []\n",
    "#     population_size_series = []\n",
    "\n",
    "#     for i in range(len(df)):\n",
    "#         if denominator_n_series.iloc[i] == 0:\n",
    "#             safe_freq = max(global_freq_val, 0.01)\n",
    "#             est_denominator = max(1, int(volume_series.iloc[i] * safe_freq))\n",
    "#             est_numerator = int(est_denominator * global_rate)\n",
    "#             p_val = global_rate\n",
    "#             group_population_size = volume_series.iloc[i]\n",
    "#         else:\n",
    "#             est_denominator = denominator_n_series.iloc[i]\n",
    "#             est_numerator = numerator_series.iloc[i]\n",
    "#             p_val = calculate_adjusted_proportion_agresti_coull(est_numerator, est_denominator, z_score)\n",
    "#             group_population_size = volume_series.iloc[i]\n",
    "\n",
    "#         adjusted_p_estimates.append(p_val)\n",
    "#         population_size_series.append(group_population_size)\n",
    "\n",
    "#     adjusted_p_estimates = np.array(adjusted_p_estimates)\n",
    "#     population_size_series = np.array(population_size_series)\n",
    "\n",
    "#     if error_margin_logic == 'Relative Error Margin':\n",
    "#         adj_error_margin = np.maximum(relative_error_margin_value * adjusted_p_estimates, min_absolute_error_margin)\n",
    "#     else:\n",
    "#         adj_error_margin = np.full_like(adjusted_p_estimates, max(absolute_error_margin_value, min_absolute_error_margin))\n",
    "\n",
    "#     req_denom_sessions = []\n",
    "#     for i in range(len(df)):\n",
    "#         p_val = adjusted_p_estimates[i]\n",
    "#         e_val = adj_error_margin[i]\n",
    "#         group_population_size = population_size_series[i]\n",
    "#         req_denom_sessions.append(\n",
    "#             calculate_required_sample_size_proportion(\n",
    "#                 p_val, e_val, confidence_level, group_population_size, z_score\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#     freq = get_frequency_of_denominator(df, metric, METRIC_PROPERTIES, global_freq)\n",
    "\n",
    "#     if freq_base_col not in df.columns:\n",
    "#         warnings.warn(f\"Warning: '{freq_base_col}' (frequency base column) not found in DataFrame for current reviews. Assuming 0 for all current reviews.\")\n",
    "#         current_qa_reviewed_sum = np.zeros(len(df))\n",
    "#     else:\n",
    "#         current_qa_reviewed_sum = df[freq_base_col].values\n",
    "\n",
    "#     req_sample_size = []\n",
    "#     extra_reviews = []\n",
    "\n",
    "#     for r_denom, f_val, c_reviews in zip(req_denom_sessions, freq, current_qa_reviewed_sum):\n",
    "#         if not np.isfinite(r_denom) or not np.isfinite(f_val) or f_val <= 0:\n",
    "#             req_qa_sessions = float('inf') if r_denom > 0 else 0\n",
    "#         else:\n",
    "#             req_qa_sessions = math.ceil(r_denom / f_val)\n",
    "\n",
    "#         if np.isinf(req_qa_sessions):\n",
    "#             req_sample_size.append(np.inf)\n",
    "#             extra_reviews.append(np.inf)\n",
    "#         else:\n",
    "#             req_sample_size.append(max(1, int(req_qa_sessions)))\n",
    "#             extra_reviews.append(max(0, req_sample_size[-1] - c_reviews))\n",
    "\n",
    "#     return req_sample_size, extra_reviews, adj_error_margin\n",
    "\n",
    "def calculate_required_sessions(\n",
    "    df: pd.DataFrame,\n",
    "    metric: str,\n",
    "    global_rates: Dict[str, float],\n",
    "    global_freq: Dict[str, float],\n",
    "    error_margin_logic: str,\n",
    "    absolute_error_margin_value: float,\n",
    "    relative_error_margin_value: float,\n",
    "    min_absolute_error_margin: float,\n",
    "    confidence_level: float,\n",
    "    z_score: float\n",
    ") -> Tuple[List[Union[int, float]], List[Union[int, float]], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculates the required QA reviewed sessions for each group (row in df) to achieve\n",
    "    the target error margin for the selected metric.\n",
    "    If denominator is zero, numerator is zero, or metric rate is very small (<=0.001),\n",
    "    estimate denominator as group volume * global_freq, and numerator as estimated_denominator * global_rate.\n",
    "    Returns: (required_sample_size, extra_reviews, adj_error_margin)\n",
    "    \"\"\"\n",
    "    props = METRIC_PROPERTIES[metric]\n",
    "    num_col = props['numerator_col']\n",
    "    den_col = props['denominator_col']\n",
    "    freq_base_col = props['freq_denominator_base_col']\n",
    "\n",
    "    if den_col not in df.columns or num_col not in df.columns:\n",
    "        warnings.warn(f\"Numerator '{num_col}' or Denominator '{den_col}' column not found in input DataFrame for `calculate_required_sessions`. Cannot proceed.\")\n",
    "        return [float('inf')] * len(df), [float('inf')] * len(df), np.full(len(df), np.nan)\n",
    "\n",
    "    denominator_n_series = df[den_col].astype(float)\n",
    "    numerator_series = df[num_col].astype(float)\n",
    "    volume_series = df['volume'].astype(float) if 'volume' in df.columns else denominator_n_series\n",
    "\n",
    "    global_rate = global_rates.get(metric, 0)\n",
    "    global_freq_val = global_freq.get(metric, 0)\n",
    "\n",
    "    adjusted_p_estimates = []\n",
    "    population_size_series = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        den = denominator_n_series.iloc[i]\n",
    "        num = numerator_series.iloc[i]\n",
    "        vol = volume_series.iloc[i]\n",
    "        # Calculate observed metric rate\n",
    "        metric_rate = num / den if den > 0 else 0\n",
    "\n",
    "        # Use global approximation if no data or metric rate is very small\n",
    "        if (den == 0) or (num == 0) or (metric_rate <= 0.001):\n",
    "            safe_freq = max(global_freq_val, 0.01)\n",
    "            est_denominator = max(1, int(vol * safe_freq))\n",
    "            est_numerator = int(est_denominator * global_rate)\n",
    "            p_val = global_rate\n",
    "            group_population_size = vol\n",
    "        else:\n",
    "            est_denominator = den\n",
    "            est_numerator = num\n",
    "            p_val = calculate_adjusted_proportion_agresti_coull(est_numerator, est_denominator, z_score)\n",
    "            group_population_size = vol\n",
    "\n",
    "        adjusted_p_estimates.append(p_val)\n",
    "        population_size_series.append(group_population_size)\n",
    "\n",
    "    adjusted_p_estimates = np.array(adjusted_p_estimates)\n",
    "    population_size_series = np.array(population_size_series)\n",
    "\n",
    "    if error_margin_logic == 'Relative Error Margin':\n",
    "        adj_error_margin = np.maximum(relative_error_margin_value * adjusted_p_estimates, min_absolute_error_margin)\n",
    "    else:\n",
    "        adj_error_margin = np.full_like(adjusted_p_estimates, max(absolute_error_margin_value, min_absolute_error_margin))\n",
    "\n",
    "    req_denom_sessions = []\n",
    "    for i in range(len(df)):\n",
    "        p_val = adjusted_p_estimates[i]\n",
    "        e_val = adj_error_margin[i]\n",
    "        group_population_size = population_size_series[i]\n",
    "        req_denom_sessions.append(\n",
    "            calculate_required_sample_size_proportion(\n",
    "                p_val, e_val, confidence_level, group_population_size, z_score\n",
    "            )\n",
    "        )\n",
    "\n",
    "    freq = get_frequency_of_denominator(df, metric, METRIC_PROPERTIES, global_freq)\n",
    "\n",
    "    if freq_base_col not in df.columns:\n",
    "        warnings.warn(f\"Warning: '{freq_base_col}' (frequency base column) not found in DataFrame for current reviews. Assuming 0 for all current reviews.\")\n",
    "        current_qa_reviewed_sum = np.zeros(len(df))\n",
    "    else:\n",
    "        current_qa_reviewed_sum = df[freq_base_col].values\n",
    "\n",
    "    req_sample_size = []\n",
    "    extra_reviews = []\n",
    "\n",
    "    for r_denom, f_val, c_reviews in zip(req_denom_sessions, freq, current_qa_reviewed_sum):\n",
    "        if not np.isfinite(r_denom) or not np.isfinite(f_val) or f_val <= 0:\n",
    "            req_qa_sessions = float('inf') if r_denom > 0 else 0\n",
    "        else:\n",
    "            req_qa_sessions = math.ceil(r_denom / f_val)\n",
    "\n",
    "        if np.isinf(req_qa_sessions):\n",
    "            req_sample_size.append(np.inf)\n",
    "            extra_reviews.append(np.inf)\n",
    "        else:\n",
    "            req_sample_size.append(max(1, int(req_qa_sessions)))\n",
    "            extra_reviews.append(max(0, req_sample_size[-1] - c_reviews))\n",
    "\n",
    "    return req_sample_size, extra_reviews, adj_error_margin\n",
    "\n",
    "\n",
    "# def calculate_sample_counts(population_df: pd.DataFrame, total_target_sessions: int, groupby_dimensions: List[str],\n",
    "#                             sampling_type: str, exponent: float, volume_col_for_sampling: str,\n",
    "#                             selected_metric: str, global_rates: Dict[str, float], overall_avg_frequencies_in_qa: Dict[str, float],\n",
    "#                             z_score: float, min_volume: int = 100000, min_den_n: int = 100, min_required_n_for_margin: int = 10) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Simulates sampling sessions from aggregated population data and calculates\n",
    "#     simulated numerator, denominator counts, and error margins for the SELECTED_METRIC.\n",
    "    \n",
    "#     New Params:\n",
    "#     - min_volume: Minimum original volume for a group to be included (filters small groups upstream).\n",
    "#     - min_den_n: Minimum simulated_den_n for viability (used in fallback and post-filter).\n",
    "#     - min_required_n_for_margin: Passed to calculate_margin_wilson to force high margin if n < this.\n",
    "#     \"\"\"\n",
    "#     props = METRIC_PROPERTIES[selected_metric]\n",
    "#     num_col_original = props['numerator_col']\n",
    "#     den_col_original = props['denominator_col']\n",
    "\n",
    "#     df_for_sim = population_df.copy()\n",
    "\n",
    "#     if num_col_original not in df_for_sim.columns or den_col_original not in df_for_sim.columns:\n",
    "#         warnings.warn(f\"Original numerator '{num_col_original}' or denominator '{den_col_original}' column not found for selected metric '{selected_metric}'. Simulation cannot proceed accurately.\")\n",
    "#         empty_cols = list(set(groupby_dimensions + ['simulated_total_sessions', 'simulated_num_x', 'simulated_den_n', 'margin', 'margin_rel']))\n",
    "#         return pd.DataFrame(columns=empty_cols)\n",
    "\n",
    "#     # --- NEW: Pre-filter small groups based on original volume to avoid simulating them ---\n",
    "#     if 'volume' in df_for_sim.columns:\n",
    "#         df_for_sim = df_for_sim[df_for_sim['volume'] >= min_volume].copy()\n",
    "#         if df_for_sim.empty:\n",
    "#             warnings.warn(\"All groups filtered out due to min_volume threshold. Returning empty DataFrame.\")\n",
    "#             return pd.DataFrame(columns=groupby_dimensions + ['simulated_total_sessions', 'simulated_num_x', 'simulated_den_n', 'margin', 'margin_rel'])\n",
    "\n",
    "#     df_for_sim['original_metric_rate'] = df_for_sim[num_col_original] / df_for_sim[den_col_original].replace(0, np.nan)\n",
    "    \n",
    "#     # --- IMPROVED: Enhance fallback to use global rate if original den < min_den_n (prevents unreliable small-n rates) ---\n",
    "#     is_small_den = (df_for_sim[den_col_original] < min_den_n) | (df_for_sim[den_col_original] == 0) | (df_for_sim['original_metric_rate'].fillna(0) == 0)\n",
    "#     df_for_sim['original_metric_rate'] = np.where(\n",
    "#         is_small_den,\n",
    "#         global_rates[selected_metric],\n",
    "#         calculate_adjusted_proportion_agresti_coull(df_for_sim[num_col_original], df_for_sim[den_col_original], z_score)\n",
    "#     )\n",
    "\n",
    "#     df_for_sim['original_freq_den'] = get_frequency_of_denominator(df_for_sim, selected_metric, METRIC_PROPERTIES, overall_avg_frequencies_in_qa)\n",
    "    \n",
    "#     # Simulation logic (unchanged)\n",
    "#     if sampling_type == 'random':\n",
    "#         total_sampling_volume = df_for_sim[volume_col_for_sampling].sum()\n",
    "#         if total_sampling_volume > 0:\n",
    "#             df_for_sim['simulated_total_sessions'] = (df_for_sim[volume_col_for_sampling] / total_sampling_volume * total_target_sessions)\n",
    "#         else:\n",
    "#             df_for_sim['simulated_total_sessions'] = 0 \n",
    "#     elif sampling_type == 'biased':\n",
    "#         df_for_sim['rand'] = np.random.random(len(df_for_sim))\n",
    "#         safe_volume = np.maximum(df_for_sim[volume_col_for_sampling], 1e-9) \n",
    "        \n",
    "#         df_for_sim['bias_score'] = -np.log(df_for_sim['rand']) * (safe_volume ** exponent)\n",
    "        \n",
    "#         total_bias_score = df_for_sim['bias_score'].sum()\n",
    "#         if total_bias_score > 0:\n",
    "#             df_for_sim['simulated_total_sessions'] = (df_for_sim['bias_score'] / total_bias_score * total_target_sessions)\n",
    "#         else:\n",
    "#             df_for_sim['simulated_total_sessions'] = 0\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unknown sampling_type: {sampling_type}. Must be 'random' or 'biased'.\")\n",
    "    \n",
    "#     df_for_sim['simulated_total_sessions'] = df_for_sim['simulated_total_sessions'].round().astype(int)\n",
    "    \n",
    "#     df_for_sim['simulated_den_n'] = (df_for_sim['simulated_total_sessions'] * df_for_sim['original_freq_den'])\n",
    "#     df_for_sim['simulated_den_n'] = df_for_sim['simulated_den_n'].round().astype(int)\n",
    "\n",
    "#     df_for_sim['simulated_num_x'] = (df_for_sim['simulated_den_n'] * df_for_sim['original_metric_rate'])\n",
    "#     df_for_sim['simulated_num_x'] = df_for_sim['simulated_num_x'].round().astype(int)\n",
    "\n",
    "#     # --- IMPROVED: Set fallback if simulated_den_n < min_den_n (catches post-simulation small groups) ---\n",
    "#     df_for_sim['used_global_rate_fallback'] = (\n",
    "#         (df_for_sim['simulated_den_n'] < min_den_n) |\n",
    "#         (df_for_sim['simulated_den_n'] == 0) |\n",
    "#         (df_for_sim['original_metric_rate'] == global_rates[selected_metric])\n",
    "#     )\n",
    "\n",
    "#     valid_groupby_dimensions = [dim for dim in groupby_dimensions if dim in df_for_sim.columns]\n",
    "\n",
    "#     if valid_groupby_dimensions:\n",
    "#         # --- NEW: Propagate 'volume' in aggregation for downstream filtering ---\n",
    "#         agg_dict = {\n",
    "#             'simulated_total_sessions': ('simulated_total_sessions', 'sum'),\n",
    "#             'simulated_num_x': ('simulated_num_x', 'sum'),\n",
    "#             'simulated_den_n': ('simulated_den_n', 'sum'),\n",
    "#             'volume': ('volume', 'sum')  # Propagate total original volume for the group\n",
    "#         }\n",
    "#         sim_agg_df = df_for_sim.groupby(valid_groupby_dimensions).agg(**agg_dict).reset_index()\n",
    "#     else:\n",
    "#         sim_agg_df = pd.DataFrame([{\n",
    "#             'simulated_total_sessions': df_for_sim['simulated_total_sessions'].sum(),\n",
    "#             'simulated_num_x': df_for_sim['simulated_num_x'].sum(),\n",
    "#             'simulated_den_n': df_for_sim['simulated_den_n'].sum(),\n",
    "#             'volume': df_for_sim['volume'].sum()  # Propagate total volume\n",
    "#         }])\n",
    "#         for dim in REPORTING_DIMENSIONS + ['primary_product', 'customer']:\n",
    "#             if dim not in sim_agg_df.columns:\n",
    "#                 sim_agg_df[dim] = 'Overall'\n",
    "\n",
    "#     sim_agg_df['simulated_num_x'] = np.maximum(sim_agg_df['simulated_num_x'], 0)\n",
    "#     sim_agg_df['simulated_den_n'] = np.maximum(sim_agg_df['simulated_den_n'], 0)\n",
    "\n",
    "#     # --- NEW: Post-aggregation filter: Drop aggregates with low simulated_den_n or volume ---\n",
    "#     if 'volume' in sim_agg_df.columns and 'simulated_den_n' in sim_agg_df.columns:\n",
    "#         sim_agg_df = sim_agg_df[(sim_agg_df['volume'] >= min_volume) & (sim_agg_df['simulated_den_n'] >= min_den_n)].copy()\n",
    "#         if sim_agg_df.empty:\n",
    "#             warnings.warn(\"All aggregated groups filtered out due to min_volume or min_den_n thresholds. Returning empty DataFrame.\")\n",
    "#             return pd.DataFrame(columns=groupby_dimensions + ['simulated_total_sessions', 'simulated_num_x', 'simulated_den_n', 'margin', 'margin_rel'])\n",
    "\n",
    "#     # --- IMPROVED: Pass min_required_n to enforce in margin calculation ---\n",
    "#     sim_agg_df = get_margins_df(sim_agg_df, 'simulated_num_x', 'simulated_den_n', z_score, min_required_n=min_required_n_for_margin)\n",
    "    \n",
    "#     return sim_agg_df\n",
    "\n",
    "def calculate_sample_counts(population_df: pd.DataFrame, total_target_sessions: int, groupby_dimensions: List[str],\n",
    "                            sampling_type: str, exponent: float, volume_col_for_sampling: str,\n",
    "                            selected_metric: str, global_rates: Dict[str, float], overall_avg_frequencies_in_qa: Dict[str, float],\n",
    "                            z_score: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simulates sampling sessions from aggregated population data and calculates\n",
    "    simulated numerator, denominator counts, and error margins for the SELECTED_METRIC.\n",
    "    \"\"\"\n",
    "    props = METRIC_PROPERTIES[selected_metric]\n",
    "    num_col_original = props['numerator_col']\n",
    "    den_col_original = props['denominator_col']\n",
    "\n",
    "    df_for_sim = population_df.copy()\n",
    "\n",
    "    if num_col_original not in df_for_sim.columns or den_col_original not in df_for_sim.columns:\n",
    "        warnings.warn(f\"Original numerator '{num_col_original}' or denominator '{den_col_original}' column not found for selected metric '{selected_metric}'. Simulation cannot proceed accurately.\")\n",
    "        empty_cols = list(set(groupby_dimensions + ['simulated_total_sessions', 'simulated_num_x', 'simulated_den_n', 'margin', 'margin_rel']))\n",
    "        return pd.DataFrame(columns=empty_cols)\n",
    "\n",
    "    df_for_sim['original_metric_rate'] = df_for_sim[num_col_original] / df_for_sim[den_col_original].replace(0, np.nan)\n",
    "    df_for_sim['original_metric_rate'] = np.where(\n",
    "        (df_for_sim['original_metric_rate'].fillna(0) == 0) | (df_for_sim[den_col_original] == 0),\n",
    "        global_rates[selected_metric],\n",
    "        calculate_adjusted_proportion_agresti_coull(df_for_sim[num_col_original], df_for_sim[den_col_original], z_score)\n",
    "    )\n",
    "\n",
    "    df_for_sim['original_freq_den'] = get_frequency_of_denominator(df_for_sim, selected_metric, METRIC_PROPERTIES, overall_avg_frequencies_in_qa)\n",
    "    \n",
    "    if sampling_type == 'random':\n",
    "        total_sampling_volume = df_for_sim[volume_col_for_sampling].sum()\n",
    "        if total_sampling_volume > 0:\n",
    "            df_for_sim['simulated_total_sessions'] = (df_for_sim[volume_col_for_sampling] / total_sampling_volume * total_target_sessions)\n",
    "        else:\n",
    "            df_for_sim['simulated_total_sessions'] = 0 \n",
    "    elif sampling_type == 'biased':\n",
    "        df_for_sim['rand'] = np.random.random(len(df_for_sim))\n",
    "        safe_volume = np.maximum(df_for_sim[volume_col_for_sampling], 1e-9) \n",
    "        df_for_sim['bias_score'] = -np.log(df_for_sim['rand']) * (safe_volume ** exponent)\n",
    "        total_bias_score = df_for_sim['bias_score'].sum()\n",
    "        if total_bias_score > 0:\n",
    "            df_for_sim['simulated_total_sessions'] = (df_for_sim['bias_score'] / total_bias_score * total_target_sessions)\n",
    "        else:\n",
    "            df_for_sim['simulated_total_sessions'] = 0\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown sampling_type: {sampling_type}. Must be 'random' or 'biased'.\")\n",
    "\n",
    "    # --- PATCH: Cap simulated_total_sessions to not exceed available volume for each group ---\n",
    "    if 'volume' in df_for_sim.columns:\n",
    "        df_for_sim['simulated_total_sessions'] = np.minimum(df_for_sim['simulated_total_sessions'].round().astype(int), df_for_sim['volume'].astype(int))\n",
    "    else:\n",
    "        df_for_sim['simulated_total_sessions'] = df_for_sim['simulated_total_sessions'].round().astype(int)\n",
    "\n",
    "    df_for_sim['simulated_den_n'] = (df_for_sim['simulated_total_sessions'] * df_for_sim['original_freq_den'])\n",
    "    df_for_sim['simulated_den_n'] = df_for_sim['simulated_den_n'].round().astype(int)\n",
    "\n",
    "    df_for_sim['simulated_num_x'] = (df_for_sim['simulated_den_n'] * df_for_sim['original_metric_rate'])\n",
    "    df_for_sim['simulated_num_x'] = df_for_sim['simulated_num_x'].round().astype(int)\n",
    "\n",
    "    df_for_sim['used_global_rate_fallback'] = (\n",
    "        (df_for_sim['simulated_den_n'] == 0) |\n",
    "        (df_for_sim['original_metric_rate'] == global_rates[selected_metric])\n",
    "    )\n",
    "\n",
    "    valid_groupby_dimensions = [dim for dim in groupby_dimensions if dim in df_for_sim.columns]\n",
    "\n",
    "    if valid_groupby_dimensions:\n",
    "        sim_agg_df = df_for_sim.groupby(valid_groupby_dimensions).agg(\n",
    "            simulated_total_sessions=('simulated_total_sessions', 'sum'),\n",
    "            simulated_num_x=('simulated_num_x', 'sum'),\n",
    "            simulated_den_n=('simulated_den_n', 'sum')\n",
    "        ).reset_index()\n",
    "    else:\n",
    "        sim_agg_df = pd.DataFrame([{\n",
    "            'simulated_total_sessions': df_for_sim['simulated_total_sessions'].sum(),\n",
    "            'simulated_num_x': df_for_sim['simulated_num_x'].sum(),\n",
    "            'simulated_den_n': df_for_sim['simulated_den_n'].sum()\n",
    "        }])\n",
    "        for dim in REPORTING_DIMENSIONS + ['primary_product', 'customer']:\n",
    "            if dim not in sim_agg_df.columns:\n",
    "                sim_agg_df[dim] = 'Overall'\n",
    "\n",
    "    sim_agg_df['simulated_num_x'] = np.maximum(sim_agg_df['simulated_num_x'], 0)\n",
    "    sim_agg_df['simulated_den_n'] = np.maximum(sim_agg_df['simulated_den_n'], 0)\n",
    "\n",
    "    sim_agg_df = get_margins_df(sim_agg_df, 'simulated_num_x', 'simulated_den_n', z_score)\n",
    "    \n",
    "    return sim_agg_df\n",
    "\n",
    "# def calculate_sample_counts(population_df: pd.DataFrame, total_target_sessions: int, groupby_dimensions: List[str],\n",
    "#                             sampling_type: str, exponent: float, volume_col_for_sampling: str,\n",
    "#                             selected_metric: str, global_rates: Dict[str, float], overall_avg_frequencies_in_qa: Dict[str, float],\n",
    "#                             z_score: float) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Simulates sampling sessions from aggregated population data and calculates\n",
    "#     simulated numerator, denominator counts, and error margins for the SELECTED_METRIC.\n",
    "#     \"\"\"\n",
    "#     props = METRIC_PROPERTIES[selected_metric]\n",
    "#     num_col_original = props['numerator_col']\n",
    "#     den_col_original = props['denominator_col']\n",
    "\n",
    "#     df_for_sim = population_df.copy()\n",
    "\n",
    "#     if num_col_original not in df_for_sim.columns or den_col_original not in df_for_sim.columns:\n",
    "#         warnings.warn(f\"Original numerator '{num_col_original}' or denominator '{den_col_original}' column not found for selected metric '{selected_metric}'. Simulation cannot proceed accurately.\")\n",
    "#         empty_cols = list(set(groupby_dimensions + ['simulated_total_sessions', 'simulated_num_x', 'simulated_den_n', 'margin', 'margin_rel']))\n",
    "#         return pd.DataFrame(columns=empty_cols)\n",
    "\n",
    "\n",
    "#     df_for_sim['original_metric_rate'] = df_for_sim[num_col_original] / df_for_sim[den_col_original].replace(0, np.nan)\n",
    "#     df_for_sim['original_metric_rate'] = np.where(\n",
    "#         (df_for_sim['original_metric_rate'].fillna(0) == 0) | (df_for_sim[den_col_original] == 0),\n",
    "#         global_rates[selected_metric],\n",
    "#         calculate_adjusted_proportion_agresti_coull(df_for_sim[num_col_original], df_for_sim[den_col_original], z_score)\n",
    "#     )\n",
    "\n",
    "#     df_for_sim['original_freq_den'] = get_frequency_of_denominator(df_for_sim, selected_metric, METRIC_PROPERTIES, overall_avg_frequencies_in_qa)\n",
    "    \n",
    "#     if sampling_type == 'random':\n",
    "#         total_sampling_volume = df_for_sim[volume_col_for_sampling].sum()\n",
    "#         if total_sampling_volume > 0:\n",
    "#             df_for_sim['simulated_total_sessions'] = (df_for_sim[volume_col_for_sampling] / total_sampling_volume * total_target_sessions)\n",
    "#         else:\n",
    "#             df_for_sim['simulated_total_sessions'] = 0 \n",
    "#     elif sampling_type == 'biased':\n",
    "#         df_for_sim['rand'] = np.random.random(len(df_for_sim))\n",
    "#         safe_volume = np.maximum(df_for_sim[volume_col_for_sampling], 1e-9) \n",
    "        \n",
    "#         df_for_sim['bias_score'] = -np.log(df_for_sim['rand']) * (safe_volume ** exponent)\n",
    "        \n",
    "#         total_bias_score = df_for_sim['bias_score'].sum()\n",
    "#         if total_bias_score > 0:\n",
    "#             df_for_sim['simulated_total_sessions'] = (df_for_sim['bias_score'] / total_bias_score * total_target_sessions)\n",
    "#         else:\n",
    "#             df_for_sim['simulated_total_sessions'] = 0\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unknown sampling_type: {sampling_type}. Must be 'random' or 'biased'.\")\n",
    "    \n",
    "#     df_for_sim['simulated_total_sessions'] = df_for_sim['simulated_total_sessions'].round().astype(int)\n",
    "    \n",
    "#     df_for_sim['simulated_den_n'] = (df_for_sim['simulated_total_sessions'] * df_for_sim['original_freq_den'])\n",
    "#     df_for_sim['simulated_den_n'] = df_for_sim['simulated_den_n'].round().astype(int)\n",
    "\n",
    "#     df_for_sim['simulated_num_x'] = (df_for_sim['simulated_den_n'] * df_for_sim['original_metric_rate'])\n",
    "#     df_for_sim['simulated_num_x'] = df_for_sim['simulated_num_x'].round().astype(int)\n",
    "\n",
    "#     df_for_sim['used_global_rate_fallback'] = (\n",
    "#     (df_for_sim['simulated_den_n'] == 0) |\n",
    "#     (df_for_sim['original_metric_rate'] == global_rates[selected_metric])\n",
    "#     )\n",
    "\n",
    "#     valid_groupby_dimensions = [dim for dim in groupby_dimensions if dim in df_for_sim.columns]\n",
    "\n",
    "#     if valid_groupby_dimensions:\n",
    "#         sim_agg_df = df_for_sim.groupby(valid_groupby_dimensions).agg(\n",
    "#             simulated_total_sessions=('simulated_total_sessions', 'sum'),\n",
    "#             simulated_num_x=('simulated_num_x', 'sum'),\n",
    "#             simulated_den_n=('simulated_den_n', 'sum')\n",
    "#         ).reset_index()\n",
    "#     else:\n",
    "#         sim_agg_df = pd.DataFrame([{\n",
    "#             'simulated_total_sessions': df_for_sim['simulated_total_sessions'].sum(),\n",
    "#             'simulated_num_x': df_for_sim['simulated_num_x'].sum(),\n",
    "#             'simulated_den_n': df_for_sim['simulated_den_n'].sum()\n",
    "#         }])\n",
    "#         # Add all expected dimensions as 'Overall'\n",
    "#         for dim in REPORTING_DIMENSIONS + ['primary_product', 'customer']:\n",
    "#             if dim not in sim_agg_df.columns:\n",
    "#                 sim_agg_df[dim] = 'Overall'\n",
    "\n",
    "#     # if groupby_dimensions and all(dim in df_for_sim.columns for dim in groupby_dimensions):\n",
    "#     #     sim_agg_df = df_for_sim.groupby(groupby_dimensions).agg(\n",
    "#     #         simulated_total_sessions=('simulated_total_sessions', 'sum'),\n",
    "#     #         simulated_num_x=('simulated_num_x', 'sum'),\n",
    "#     #         simulated_den_n=('simulated_den_n', 'sum')\n",
    "#     #     ).reset_index()\n",
    "#     # else:\n",
    "#     #     sim_agg_df = pd.DataFrame([{\n",
    "#     #         'simulated_total_sessions': df_for_sim['simulated_total_sessions'].sum(),\n",
    "#     #         'simulated_num_x': df_for_sim['simulated_num_x'].sum(),\n",
    "#     #         'simulated_den_n': df_for_sim['simulated_den_n'].sum()\n",
    "#     #     }])\n",
    "#     #     for dim in REPORTING_DIMENSIONS + ['primary_product', 'customer']:\n",
    "#     #         if dim not in sim_agg_df.columns:\n",
    "#     #             sim_agg_df[dim] = 'Overall' \n",
    "\n",
    "#     sim_agg_df['simulated_num_x'] = np.maximum(sim_agg_df['simulated_num_x'], 0)\n",
    "#     sim_agg_df['simulated_den_n'] = np.maximum(sim_agg_df['simulated_den_n'], 0)\n",
    "\n",
    "#     sim_agg_df = get_margins_df(sim_agg_df, 'simulated_num_x', 'simulated_den_n', z_score)\n",
    "    \n",
    "#     return sim_agg_df\n",
    "\n",
    "#--- Helper Functions for Aggregation and Plotting ---\n",
    "\n",
    "def _aggregate_top_n_for_table_and_plot(df: pd.DataFrame, group_col: str, n: int, sort_col: str, \n",
    "                                        value_cols: List[str], rest_agg_logic: Dict[str, str],\n",
    "                                        recalculate_margin_func: Callable[[pd.DataFrame], float]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates data to show top N items by a sorting column, with the rest grouped as \"Rest\".\n",
    "    This version is enhanced to handle specific aggregation types for different columns in the 'Rest' group,\n",
    "    and recalculates the margin for the 'Rest' group from underlying counts for statistical accuracy.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[group_col] + value_cols)\n",
    "\n",
    "    sort_col_effective = sort_col\n",
    "    if sort_col not in df.columns or not pd.api.types.is_numeric_dtype(df[sort_col]):\n",
    "        if 'volume' in df.columns and pd.api.types.is_numeric_dtype(df['volume']):\n",
    "            sort_col_effective = 'volume'\n",
    "            warnings.warn(f\"Sorting column '{sort_col}' not found or not numeric. Falling back to 'volume' for sorting.\")\n",
    "        elif 'simulated_total_sessions' in df.columns and pd.api.types.is_numeric_dtype(df['simulated_total_sessions']):\n",
    "            sort_col_effective = 'simulated_total_sessions'\n",
    "            warnings.warn(f\"Sorting column '{sort_col}' not found or not numeric. Falling back to 'simulated_total_sessions' for sorting.\")\n",
    "        elif group_col in df.columns:\n",
    "            warnings.warn(f\"No suitable numeric sorting column found. Sorting by '{group_col}' (alphabetically).\")\n",
    "            df = df.sort_values(group_col)\n",
    "            sort_col_effective = group_col\n",
    "        else:\n",
    "            warnings.warn(f\"Cannot determine a suitable column for sorting. Returning original DataFrame.\")\n",
    "            return df\n",
    "\n",
    "    if sort_col_effective in df.columns and pd.api.types.is_numeric_dtype(df[sort_col_effective]):\n",
    "        df_sorted = df.sort_values(sort_col_effective, ascending=False).copy()\n",
    "    else:\n",
    "        df_sorted = df.copy()\n",
    "\n",
    "    numeric_value_cols = [col for col in value_cols if col in df_sorted.columns and pd.api.types.is_numeric_dtype(df_sorted[col])]\n",
    "    \n",
    "    for col in numeric_value_cols:\n",
    "        df_sorted[col] = df_sorted[col].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if len(df_sorted) > n:\n",
    "        top_n_df = df_sorted.head(n).copy()\n",
    "        rest_df = df_sorted.iloc[n:].copy()\n",
    "        rest_row_data = {group_col: 'Rest'}\n",
    "        for col, agg_method in rest_agg_logic.items():\n",
    "            if col in rest_df.columns:\n",
    "                if agg_method == 'sum':\n",
    "                    sum_val = rest_df[col].sum(min_count=1)\n",
    "                    rest_row_data[col] = sum_val if not pd.isna(sum_val) else 0.0\n",
    "                elif agg_method == 'mean':\n",
    "                    rest_row_data[col] = rest_df[col].mean()\n",
    "                elif agg_method == 'recalculate_margin' and recalculate_margin_func is not None:\n",
    "                    rest_row_data[col] = recalculate_margin_func(rest_df)\n",
    "            else:\n",
    "                rest_row_data[col] = 0.0\n",
    "        # Always recalculate margin from aggregate counts\n",
    "        if 'Current Error Margin (%)' in value_cols:\n",
    "            total_k = rest_df['numerator_x'].sum() if 'numerator_x' in rest_df.columns else 0\n",
    "            total_n = rest_df['denominator_n'].sum() if 'denominator_n' in rest_df.columns else 0\n",
    "            rest_row_data['Current Error Margin (%)'] = calculate_margin_wilson(total_k, total_n, Z_SCORE)['margin'] * 100 if total_n > 0 else np.nan\n",
    "\n",
    "        if 'Meets Target Margin' in value_cols:\n",
    "            if 'Current Error Margin (%)' in rest_row_data and not pd.isna(rest_row_data['Current Error Margin (%)']):\n",
    "                if ERROR_MARGIN_LOGIC == 'Absolute Error Margin':\n",
    "                    rest_row_data['Meets Target Margin'] = (\n",
    "                        'Yes' if rest_row_data['Current Error Margin (%)'] <= ABSOLUTE_ERROR_MARGIN_VALUE * 100 else 'No'\n",
    "                    )\n",
    "                else:  # Relative Error Margin\n",
    "                    # You need to have the current relative error margin available, e.g. 'Current Relative Error Margin (%)'\n",
    "                    rel_margin = rest_row_data.get('Current Relative Error Margin (%)')\n",
    "                    if rel_margin is not None and not pd.isna(rel_margin):\n",
    "                        rest_row_data['Meets Target Margin'] = (\n",
    "                            'Yes' if rel_margin <= RELATIVE_ERROR_MARGIN_VALUE * 100 else 'No'\n",
    "                        )\n",
    "                    else:\n",
    "                        rest_row_data['Meets Target Margin'] = 'N/A'\n",
    "            else:\n",
    "                rest_row_data['Meets Target Margin'] = 'N/A'\n",
    "\n",
    "        rest_row_df = pd.DataFrame([rest_row_data])\n",
    "        aggregated_df = pd.concat([top_n_df, rest_row_df], ignore_index=True)\n",
    "    else:\n",
    "        aggregated_df = df_sorted\n",
    "    \n",
    "    return aggregated_df\n",
    "\n",
    "def _aggregate_top_n_for_table_and_plot(\n",
    "    df: pd.DataFrame,\n",
    "    group_col: str,\n",
    "    n: int,\n",
    "    sort_col: str,\n",
    "    value_cols: List[str],\n",
    "    rest_agg_logic: Dict[str, str],\n",
    "    recalculate_margin_func: Callable[[pd.DataFrame], float]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates data to show top N items by a sorting column, with the rest grouped as \"Rest\".\n",
    "    Ensures grouping by group_col only, so each group is unique.\n",
    "    Handles 'recalculate_margin' as a post-aggregation step.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[group_col] + value_cols)\n",
    "\n",
    "    # Only use valid pandas agg methods for groupby\n",
    "    valid_agg_dict = {}\n",
    "    postprocess_cols = []\n",
    "    for col in value_cols:\n",
    "        if col == group_col:\n",
    "            continue\n",
    "        agg_method = rest_agg_logic.get(col, 'sum')\n",
    "        if agg_method in ['sum', 'mean', 'min', 'max', 'first', 'last', 'median', 'std', 'var']:\n",
    "            valid_agg_dict[col] = agg_method\n",
    "        elif agg_method == 'recalculate_margin':\n",
    "            # We'll handle this after aggregation\n",
    "            valid_agg_dict[col] = 'sum'  # placeholder, will overwrite later\n",
    "            postprocess_cols.append(col)\n",
    "        else:\n",
    "            valid_agg_dict[col] = 'sum'\n",
    "\n",
    "    grouped = df.groupby(group_col, as_index=False).agg(valid_agg_dict)\n",
    "\n",
    "    # --- Sorting logic ---\n",
    "    sort_col_effective = sort_col\n",
    "    if sort_col not in grouped.columns or not pd.api.types.is_numeric_dtype(grouped[sort_col]):\n",
    "        if 'volume' in grouped.columns and pd.api.types.is_numeric_dtype(grouped['volume']):\n",
    "            sort_col_effective = 'volume'\n",
    "            warnings.warn(f\"Sorting column '{sort_col}' not found or not numeric. Falling back to 'volume' for sorting.\")\n",
    "        elif 'simulated_total_sessions' in grouped.columns and pd.api.types.is_numeric_dtype(grouped['simulated_total_sessions']):\n",
    "            sort_col_effective = 'simulated_total_sessions'\n",
    "            warnings.warn(f\"Sorting column '{sort_col}' not found or not numeric. Falling back to 'simulated_total_sessions' for sorting.\")\n",
    "        elif group_col in grouped.columns:\n",
    "            warnings.warn(f\"No suitable numeric sorting column found. Sorting by '{group_col}' (alphabetically).\")\n",
    "            grouped = grouped.sort_values(group_col)\n",
    "            sort_col_effective = group_col\n",
    "        else:\n",
    "            warnings.warn(f\"Cannot determine a suitable column for sorting. Returning original DataFrame.\")\n",
    "            return grouped\n",
    "\n",
    "    if sort_col_effective in grouped.columns and pd.api.types.is_numeric_dtype(grouped[sort_col_effective]):\n",
    "        df_sorted = grouped.sort_values(sort_col_effective, ascending=False).copy()\n",
    "    else:\n",
    "        df_sorted = grouped.copy()\n",
    "\n",
    "    numeric_value_cols = [col for col in value_cols if col in df_sorted.columns and pd.api.types.is_numeric_dtype(df_sorted[col])]\n",
    "    for col in numeric_value_cols:\n",
    "        df_sorted[col] = df_sorted[col].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    if len(df_sorted) > n:\n",
    "        top_n_df = df_sorted.head(n).copy()\n",
    "        rest_df = df_sorted.iloc[n:].copy()\n",
    "        rest_row_data = {group_col: 'Rest'}\n",
    "        for col, agg_method in rest_agg_logic.items():\n",
    "            if col in rest_df.columns:\n",
    "                if agg_method == 'sum':\n",
    "                    sum_val = rest_df[col].sum(min_count=1)\n",
    "                    rest_row_data[col] = sum_val if not pd.isna(sum_val) else 0.0\n",
    "                elif agg_method == 'mean':\n",
    "                    rest_row_data[col] = rest_df[col].mean()\n",
    "                elif agg_method == 'recalculate_margin' and recalculate_margin_func is not None:\n",
    "                    rest_row_data[col] = recalculate_margin_func(rest_df)\n",
    "            else:\n",
    "                rest_row_data[col] = 0.0\n",
    "        # Post-process any columns that need recalculate_margin\n",
    "        for col in postprocess_cols:\n",
    "            if col in rest_row_data and recalculate_margin_func is not None:\n",
    "                rest_row_data[col] = recalculate_margin_func(rest_df)\n",
    "        rest_row_df = pd.DataFrame([rest_row_data])\n",
    "        aggregated_df = pd.concat([top_n_df, rest_row_df], ignore_index=True)\n",
    "    else:\n",
    "        aggregated_df = df_sorted\n",
    "\n",
    "    return aggregated_df\n",
    "\n",
    "# --- Plotting Functions ---\n",
    "\n",
    "# def create_bar_chart(df: pd.DataFrame, x_col_or_index_values: Union[str, pd.Series, pd.Index], y_cols: List[str], title: str,\n",
    "#                      y_axis_title: str, x_axis_title: str, barmode: str = 'group',\n",
    "#                      line_data: Dict[str, Any] = None, filename_suffix: str = \"\", save_plots_to_html: bool = False):\n",
    "#     \"\"\"\n",
    "#     Generates a Plotly bar chart with optional line data.\n",
    "#     \"\"\"\n",
    "#     fig = go.Figure()\n",
    "\n",
    "#     if isinstance(x_col_or_index_values, str):\n",
    "#         x_values = df[x_col_or_index_values]\n",
    "#         num_categories = len(df[x_col_or_index_values].unique())\n",
    "#     else:\n",
    "#         x_values = x_col_or_index_values\n",
    "#         if isinstance(x_values, pd.Index):\n",
    "#             num_categories = len(x_values.unique())\n",
    "#         else:\n",
    "#             num_categories = len(np.unique(x_values))\n",
    "\n",
    "#     for y_col in y_cols:\n",
    "#         name_label = y_col.replace('_plot', '')\n",
    "#         fig.add_trace(go.Bar(name=name_label, x=x_values, y=df[y_col]))\n",
    "\n",
    "#     if line_data:\n",
    "#         fig.add_shape(type=\"line\", x0=-0.5, y0=line_data['y_value'], x1=num_categories - 0.5, y1=line_data['y_value'],\n",
    "#                       line=dict(color=line_data['color'], width=line_data['width'], dash=line_data['dash']),\n",
    "#                       name=line_data['name'])\n",
    "        \n",
    "#     all_y_values = pd.concat([df[col].replace([np.inf, -np.inf], np.nan).dropna() for col in y_cols if col in df.columns])\n",
    "#     y_max = all_y_values.max() * 1.1 if not all_y_values.empty else 100\n",
    "#     y_min = all_y_values.min() * 0.9 if not all_y_values.empty and all_y_values.min() < 0 else 0\n",
    "    \n",
    "#     for y_col in y_cols:\n",
    "#         if y_col in df.columns and np.isinf(df[y_col]).any():\n",
    "#             inf_rows = df[df[y_col] == np.inf]\n",
    "#             for idx, row in inf_rows.iterrows():\n",
    "#                 x_val_for_annotation = row[x_col_or_index_values] if isinstance(x_col_or_index_values, str) else idx\n",
    "                \n",
    "#                 fig.add_annotation(\n",
    "#                     x=x_val_for_annotation, y=y_max,\n",
    "#                     text=\"Inf.\",\n",
    "#                     showarrow=False,\n",
    "#                     yshift=10,\n",
    "#                     font=dict(color=\"red\", size=10)\n",
    "#                 )\n",
    "\n",
    "#     fig.update_layout(\n",
    "#         title=title,\n",
    "#         yaxis_title=y_axis_title,\n",
    "#         xaxis_title=x_axis_title,\n",
    "#         barmode=barmode,\n",
    "#         xaxis_tickangle=45,\n",
    "#         yaxis_range=[y_min, y_max],\n",
    "#         legend_title_text='Metric'\n",
    "#     )\n",
    "    \n",
    "#     if save_plots_to_html and filename_suffix:\n",
    "#         fig.write_html(f\"{filename_suffix}.html\")\n",
    "#         print(f\"Plot saved to {filename_suffix}.html\")\n",
    "    \n",
    "#     fig.show()\n",
    "\n",
    "def create_bar_chart(\n",
    "    df: pd.DataFrame,\n",
    "    x_col_or_index_values: Union[str, pd.Series, pd.Index],\n",
    "    y_cols: List[str],\n",
    "    title: str,\n",
    "    y_axis_title: str,\n",
    "    x_axis_title: str,\n",
    "    barmode: str = 'group',\n",
    "    line_data: Dict[str, Any] = None,\n",
    "    filename_suffix: str = \"\",\n",
    "    save_plots_to_html: bool = False,\n",
    "    text_labels: Dict[str, List[str]] = None  # <-- NEW ARGUMENT\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a Plotly bar chart with optional line data and value labels.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    if isinstance(x_col_or_index_values, str):\n",
    "        x_values = df[x_col_or_index_values]\n",
    "        num_categories = len(df[x_col_or_index_values].unique())\n",
    "    else:\n",
    "        x_values = x_col_or_index_values\n",
    "        if isinstance(x_values, pd.Index):\n",
    "            num_categories = len(x_values.unique())\n",
    "        else:\n",
    "            num_categories = len(np.unique(x_values))\n",
    "\n",
    "    for y_col in y_cols:\n",
    "        name_label = y_col.replace('_plot', '')\n",
    "        fig.add_trace(go.Bar(\n",
    "            name=name_label,\n",
    "            x=x_values,\n",
    "            y=df[y_col],\n",
    "            text=text_labels[y_col] if text_labels and y_col in text_labels else None,\n",
    "            textposition='auto' if text_labels and y_col in text_labels else None\n",
    "        ))\n",
    "\n",
    "    if line_data:\n",
    "        fig.add_shape(type=\"line\", x0=-0.5, y0=line_data['y_value'], x1=num_categories - 0.5, y1=line_data['y_value'],\n",
    "                      line=dict(color=line_data['color'], width=line_data['width'], dash=line_data['dash']),\n",
    "                      name=line_data['name'])\n",
    "\n",
    "    all_y_values = pd.concat([df[col].replace([np.inf, -np.inf], np.nan).dropna() for col in y_cols if col in df.columns])\n",
    "    y_max = all_y_values.max() * 1.1 if not all_y_values.empty else 100\n",
    "    y_min = all_y_values.min() * 0.9 if not all_y_values.empty and all_y_values.min() < 0 else 0\n",
    "\n",
    "    for y_col in y_cols:\n",
    "        if y_col in df.columns and np.isinf(df[y_col]).any():\n",
    "            inf_rows = df[df[y_col] == np.inf]\n",
    "            for idx, row in inf_rows.iterrows():\n",
    "                x_val_for_annotation = row[x_col_or_index_values] if isinstance(x_col_or_index_values, str) else idx\n",
    "                fig.add_annotation(\n",
    "                    x=x_val_for_annotation, y=y_max,\n",
    "                    text=\"Inf.\",\n",
    "                    showarrow=False,\n",
    "                    yshift=10,\n",
    "                    font=dict(color=\"red\", size=10)\n",
    "                )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        yaxis_title=y_axis_title,\n",
    "        xaxis_title=x_axis_title,\n",
    "        barmode=barmode,\n",
    "        xaxis_tickangle=45,\n",
    "        yaxis_range=[y_min, y_max],\n",
    "        legend_title_text='Metric'\n",
    "    )\n",
    "\n",
    "    if save_plots_to_html and filename_suffix:\n",
    "        fig.write_html(f\"{filename_suffix}.html\")\n",
    "        print(f\"Plot saved to {filename_suffix}.html\")\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "def create_heatmap(df: pd.DataFrame, title: str, filename_suffix: str = \"\", save_plots_to_html: bool = False):\n",
    "    \"\"\"\n",
    "    Generates a Plotly heatmap.\n",
    "    \"\"\"\n",
    "    fig = px.imshow(df, text_auto=True, color_continuous_scale='Viridis',\n",
    "                     title=title,\n",
    "                     labels=dict(x='Simulation', y='Dimension', color='Count'))\n",
    "    fig.update_layout(xaxis_tickangle=45)\n",
    "    if save_plots_to_html and filename_suffix:\n",
    "        fig.write_html(f\"{filename_suffix}.html\")\n",
    "        print(f\"Plot saved to {filename_suffix}.html\")\n",
    "    fig.show()\n",
    "\n",
    "def create_combined_bar_line_chart(df: pd.DataFrame, bar_cols: List[str], line_col: str,\n",
    "                                   title: str, y1_axis_title: str, y2_axis_title: str,\n",
    "                                   x_col_or_index_values: Union[str, pd.Series, pd.Index] = None, filename_suffix: str = \"\", save_plots_to_html: bool = False):\n",
    "    \"\"\"\n",
    "    Generates a Plotly chart with bars and an overlaid line.\n",
    "    \"\"\"\n",
    "    if x_col_or_index_values is None:\n",
    "        x_values = df.index\n",
    "    elif isinstance(x_col_or_index_values, str):\n",
    "        x_values = df[x_col_or_index_values]\n",
    "    else:\n",
    "        x_values = x_col_or_index_values\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for b_col in bar_cols:\n",
    "        fig.add_trace(go.Bar(name=b_col.replace('_coverage', ' Coverage'), x=x_values, y=df[b_col]))\n",
    "    \n",
    "    if line_col in df.columns and df[line_col].dropna().any():\n",
    "        fig.add_trace(go.Scatter(name=line_col.replace('_perc', ' (%)').replace('_', ' ').title(),\n",
    "                                 x=x_values, y=df[line_col], mode='lines+markers', yaxis='y2', line=dict(color='orange')))\n",
    "        fig.update_layout(yaxis2=dict(title=y2_axis_title, overlaying='y', side='right'))\n",
    "\n",
    "    fig.update_layout(title=title,\n",
    "                      yaxis=dict(title=y1_axis_title),\n",
    "                      xaxis_title='Simulation',\n",
    "                      xaxis_tickangle=45,\n",
    "                      barmode='group',\n",
    "                      legend_title_text='Metrics & Coverage',\n",
    "                      legend=dict(\n",
    "                        orientation=\"h\",  \n",
    "                        yanchor=\"top\",    \n",
    "                        y=1.08,            \n",
    "                        xanchor=\"center\",\n",
    "                        x=0.5\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    if save_plots_to_html and filename_suffix:\n",
    "        fig.write_html(f\"{filename_suffix}.html\")\n",
    "        print(f\"Plot saved to {filename_suffix}.html\")\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "def display_metric_with_error_bars(\n",
    "    population_df: pd.DataFrame,\n",
    "    selected_metric: str,\n",
    "    metric_properties: Dict[str, Dict[str, str]],\n",
    "    reporting_dimensions: list,\n",
    "    top_n: int,\n",
    "    global_avg_metrics: Dict[str, float],\n",
    "    z_score: float,\n",
    "    save_plots_to_html: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Displays a point plot (scatter) with error bars (confidence intervals) for the metric per group in each reporting dimension.\n",
    "    Error bars are clipped so the lower bound is never below zero.\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    analysis_dimensions = list(set(reporting_dimensions + ['primary_product', 'customer']))\n",
    "\n",
    "    for dim_name in analysis_dimensions:\n",
    "        if dim_name not in population_df.columns:\n",
    "            print(f\"Warning: Dimension '{dim_name}' not found in population data. Skipping visualization for this dimension.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Metric with Error Bars for Dimension: {dim_name} ---\")\n",
    "\n",
    "        num_col = metric_properties[selected_metric]['numerator_col']\n",
    "        den_col = metric_properties[selected_metric]['denominator_col']\n",
    "\n",
    "        if num_col not in population_df.columns or den_col not in population_df.columns:\n",
    "            warnings.warn(f\"Required columns for selected metric not found in DataFrame for dimension '{dim_name}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        current_dim_agg = population_df.groupby(dim_name).agg(\n",
    "            numerator_x=(num_col, 'sum'),\n",
    "            denominator_n=(den_col, 'sum'),\n",
    "            volume_sum=('volume', 'sum')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Calculate metric rate and error margin for each group\n",
    "        current_dim_agg['metric_rate'] = current_dim_agg['numerator_x'] / current_dim_agg['denominator_n'].replace(0, np.nan)\n",
    "        current_dim_agg['metric_rate'] = current_dim_agg['metric_rate'].fillna(global_avg_metrics[selected_metric])\n",
    "\n",
    "        current_margins = get_margins_df(current_dim_agg, 'numerator_x', 'denominator_n', z_score)\n",
    "        current_dim_agg['error_margin'] = current_margins['margin']\n",
    "\n",
    "        # Top N + Rest, recalculate metric and margin for Rest group from summed numerators/denominators\n",
    "        def recalc_metric_and_margin(rest_df):\n",
    "            total_num = rest_df['numerator_x'].sum()\n",
    "            total_den = rest_df['denominator_n'].sum()\n",
    "            metric = total_num / total_den if total_den > 0 else 0\n",
    "            margin = calculate_margin_wilson(total_num, total_den, z_score)['margin']\n",
    "            return pd.Series({'metric_rate': metric, 'error_margin': margin, 'numerator_x': total_num, 'denominator_n': total_den, 'volume_sum': rest_df['volume_sum'].sum()})\n",
    "\n",
    "        if len(current_dim_agg) > top_n:\n",
    "            top_n_df = current_dim_agg.sort_values('volume_sum', ascending=False).head(top_n).copy()\n",
    "            rest_df = current_dim_agg.sort_values('volume_sum', ascending=False).iloc[top_n:]\n",
    "            rest_row = recalc_metric_and_margin(rest_df)\n",
    "            rest_row[dim_name] = 'Rest'\n",
    "            agg_metric_for_plot = pd.concat([top_n_df, pd.DataFrame([rest_row])], ignore_index=True)\n",
    "        else:\n",
    "            agg_metric_for_plot = current_dim_agg.copy()\n",
    "\n",
    "        # Round metrics to 3 decimals\n",
    "        agg_metric_for_plot['metric_rate'] = pd.to_numeric(agg_metric_for_plot['metric_rate'], errors='coerce').round(3)\n",
    "        agg_metric_for_plot['error_margin'] = pd.to_numeric(agg_metric_for_plot['error_margin'], errors='coerce').round(3)\n",
    "        \n",
    "\n",
    "        # # Calculate lower and upper bounds, clip lower at 0 - CHANGE IF YOU WANT TO SHOW DECIMALS\n",
    "        # y = agg_metric_for_plot['metric_rate']\n",
    "        # error = agg_metric_for_plot['error_margin']\n",
    "        # lower = (y - error).clip(lower=0)\n",
    "        # upper = y + error\n",
    "        # error_array = upper - y\n",
    "        # error_array_minus = y - lower\n",
    "\n",
    "        # Convert to percent\n",
    "        agg_metric_for_plot['metric_rate_perc'] = agg_metric_for_plot['metric_rate'] * 100\n",
    "        agg_metric_for_plot['error_margin_perc'] = agg_metric_for_plot['error_margin'] * 100\n",
    "\n",
    "        y = agg_metric_for_plot['metric_rate_perc']\n",
    "        error = agg_metric_for_plot['error_margin_perc']\n",
    "        lower = (y - error).clip(lower=0)\n",
    "        upper = y + error\n",
    "        error_array = upper - y\n",
    "        error_array_minus = y - lower\n",
    "\n",
    "        # Hover text as percent\n",
    "        hover_text = agg_metric_for_plot.apply(\n",
    "            lambda row: f\"{row[dim_name]}<br>{row['metric_rate_perc']:.1f}% ± {row['error_margin_perc']:.1f}%\", axis=1\n",
    "        )\n",
    "\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=agg_metric_for_plot[dim_name],\n",
    "            y=y,\n",
    "            error_y=dict(\n",
    "                type='data',\n",
    "                array=error_array,\n",
    "                arrayminus=error_array_minus,\n",
    "                visible=True,\n",
    "                color='black',\n",
    "                thickness=2,\n",
    "                width=8\n",
    "            ),\n",
    "            mode='markers',\n",
    "            marker=dict(size=8, color='royalblue', line=dict(width=1, color='black')),\n",
    "            name=f\"{selected_metric} (with CI)\",\n",
    "            text=hover_text,\n",
    "            hoverinfo='text'\n",
    "        ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{selected_metric} per {dim_name} (Top {top_n} by Volume + Rest) with Error Bars\",\n",
    "            yaxis_title=f\"{selected_metric} (%)\",\n",
    "            xaxis_title=dim_name,\n",
    "            template='plotly_white'\n",
    "        )\n",
    "\n",
    "        if save_plots_to_html:\n",
    "            filename = f\"metric_with_errorbars_{dim_name}.html\"\n",
    "            fig.write_html(filename)\n",
    "            print(f\"Plot saved to {filename}\")\n",
    "\n",
    "        fig.show()\n",
    "        \n",
    "# --- Main Report Generation Functions ---\n",
    "\n",
    "def display_current_state_analysis(population_df: pd.DataFrame, selected_metric: str, metric_properties: Dict[str, Dict[str, str]],\n",
    "                                   predefined_total_cost_per_review: float, target_overall_error_percent: float, z_score: float,\n",
    "                                   save_plots_to_html: bool) -> None:\n",
    "    \"\"\"\n",
    "    Displays a summary of the current state of error margins and cost for the selected metric.\n",
    "    Now includes the overall metric rate.\n",
    "    \"\"\"\n",
    "    current_qa_reviewed_sum = population_df['qa_reviewed'].sum()\n",
    "    \n",
    "    selected_num_sum = population_df[metric_properties[selected_metric]['numerator_col']].sum()\n",
    "    selected_den_sum = population_df[metric_properties[selected_metric]['denominator_col']].sum()\n",
    "    \n",
    "    current_overall_margins = calculate_margin_wilson(selected_num_sum, selected_den_sum, z_score)\n",
    "    overall_margin_selected_metric = current_overall_margins['margin']\n",
    "    overall_metric_rate = selected_num_sum / selected_den_sum if selected_den_sum > 0 else np.nan\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        'Metric': [selected_metric],\n",
    "        'Overall Metric Rate (%)': [round(overall_metric_rate * 100, 2)],\n",
    "        'Overall Error Margin (%)': [round(overall_margin_selected_metric * 100, 2)]\n",
    "    })\n",
    "    print(f\"\\n## Current State Summary Table: {selected_metric} (Overall)\")\n",
    "    display(summary)\n",
    "\n",
    "    cost_current = predefined_total_cost_per_review * current_qa_reviewed_sum if current_qa_reviewed_sum > 0 else np.inf\n",
    "    print(f\"\\nTotal Cost (Current State): ${cost_current:.2f} EUR\")\n",
    "    \n",
    "    plot_df = pd.DataFrame({\n",
    "        'Metric': [selected_metric],\n",
    "        'Current Error Margin (%)': [overall_margin_selected_metric * 100],\n",
    "        'Target Overall Error Margin (%)': [target_overall_error_percent]\n",
    "    })\n",
    "    \n",
    "    # Prepare text labels rounded to 0.00\n",
    "    text_labels = {\n",
    "        'Current Error Margin (%)': plot_df['Current Error Margin (%)'].apply(lambda x: f\"{x:.2f}\"),\n",
    "        'Target Overall Error Margin (%)': plot_df['Target Overall Error Margin (%)'].apply(lambda x: f\"{x:.2f}\")\n",
    "    }\n",
    "\n",
    "    create_bar_chart(\n",
    "        df=plot_df,\n",
    "        x_col_or_index_values='Metric',\n",
    "        y_cols=['Current Error Margin (%)', 'Target Overall Error Margin (%)'],\n",
    "        title=f'Overall Error Margin vs Target for {selected_metric} (Current State)',\n",
    "        y_axis_title='Error Margin (%)',\n",
    "        x_axis_title='Metric',\n",
    "        filename_suffix='current_state_overall_error_margin',\n",
    "        text_labels=text_labels,\n",
    "        save_plots_to_html=save_plots_to_html\n",
    "    )\n",
    "\n",
    "def display_current_dimension_metrics_and_margins(population_df: pd.DataFrame, selected_metric: str, metric_properties: Dict[str, Dict[str, str]],\n",
    "                                                  reporting_dimensions: List[str], top_n: int, target_overall_error_percent: float,\n",
    "                                                  global_avg_metrics: Dict[str, float], z_score: float, save_plots_to_html: bool) -> None:\n",
    "    \"\"\"\n",
    "    Displays visualizations of current metric values and error margins per reporting dimension and primary product.\n",
    "    \"\"\"\n",
    "    analysis_dimensions = list(set(reporting_dimensions + ['primary_product', 'customer']))\n",
    "\n",
    "    for dim_name in analysis_dimensions:\n",
    "        if dim_name not in population_df.columns:\n",
    "            print(f\"Warning: Dimension '{dim_name}' not found in population data. Skipping visualization for this dimension.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Current Metric and Error Margin for Dimension: {dim_name} ---\")\n",
    "\n",
    "        num_col = metric_properties[selected_metric]['numerator_col']\n",
    "        den_col = metric_properties[selected_metric]['denominator_col']\n",
    "        \n",
    "        if num_col not in population_df.columns or den_col not in population_df.columns:\n",
    "            warnings.warn(f\"Required columns for selected metric not found in DataFrame for dimension '{dim_name}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        current_dim_agg = population_df.groupby(dim_name).agg(\n",
    "            numerator_x=(num_col, 'sum'),\n",
    "            denominator_n=(den_col, 'sum'),\n",
    "            volume_sum=('volume', 'sum')\n",
    "        ).reset_index()\n",
    "\n",
    "        current_dim_agg['original_metric_rate'] = current_dim_agg['numerator_x'] / current_dim_agg['denominator_n'].replace(0, np.nan)\n",
    "        current_dim_agg['original_metric_rate'] = current_dim_agg['original_metric_rate'].fillna(global_avg_metrics[selected_metric])\n",
    "\n",
    "        current_margins = get_margins_df(current_dim_agg, 'numerator_x', 'denominator_n', z_score)\n",
    "        current_dim_agg['error_margin_perc'] = current_margins['margin'] * 100\n",
    "\n",
    "        agg_metric_for_plot = _aggregate_top_n_for_table_and_plot(\n",
    "            current_dim_agg, dim_name, top_n, 'volume_sum',\n",
    "            value_cols=['original_metric_rate', 'numerator_x', 'denominator_n', 'volume_sum'],\n",
    "            rest_agg_logic={'original_metric_rate': 'mean', 'numerator_x': 'sum', 'denominator_n': 'sum', 'volume_sum': 'sum'},\n",
    "            recalculate_margin_func=lambda rest_df_group: rest_df_group['numerator_x'].sum() / rest_df_group['denominator_n'].sum() if rest_df_group['denominator_n'].sum() > 0 else np.nan\n",
    "        )\n",
    "\n",
    "        create_bar_chart(\n",
    "            df=agg_metric_for_plot,\n",
    "            x_col_or_index_values=dim_name,\n",
    "            y_cols=['original_metric_rate'],\n",
    "            title=f'Current {selected_metric} per {dim_name} (Top {top_n} by Volume + Rest)',\n",
    "            y_axis_title=f'Current {selected_metric}',\n",
    "            x_axis_title=dim_name,\n",
    "            filename_suffix=f'current_metric_{dim_name}',\n",
    "            save_plots_to_html=save_plots_to_html\n",
    "        )\n",
    "\n",
    "        agg_margin_for_plot = _aggregate_top_n_for_table_and_plot(\n",
    "            current_dim_agg, dim_name, top_n, 'volume_sum',\n",
    "            value_cols=['error_margin_perc', 'numerator_x', 'denominator_n', 'volume_sum'],\n",
    "            rest_agg_logic={'error_margin_perc': 'mean', 'numerator_x': 'sum', 'denominator_n': 'sum', 'volume_sum': 'sum'},\n",
    "            recalculate_margin_func=lambda rest_df_group: calculate_margin_wilson(rest_df_group['numerator_x'].sum(), rest_df_group['denominator_n'].sum(), z_score)['margin'] * 100\n",
    "        )\n",
    "        create_bar_chart(\n",
    "            df=agg_margin_for_plot,\n",
    "            x_col_or_index_values=dim_name,\n",
    "            y_cols=['error_margin_perc'],\n",
    "            title=f'Current Error Margin (%) for {selected_metric} per {dim_name} (Top {top_n} by Volume + Rest)',\n",
    "            y_axis_title='Error Margin (%)',\n",
    "            x_axis_title=dim_name,\n",
    "            line_data={\n",
    "                'y_value': target_overall_error_percent,\n",
    "                'name': f'Target {target_overall_error_percent}%',\n",
    "                'color': \"Red\", 'width': 2, 'dash': \"dash\"\n",
    "            },\n",
    "            filename_suffix=f'current_error_margin_{dim_name}',\n",
    "            save_plots_to_html=save_plots_to_html\n",
    "        )\n",
    "\n",
    "def display_sample_size_calculations(\n",
    "    population_df: pd.DataFrame, selected_metric: str, metric_properties: Dict[str, Dict[str, str]],\n",
    "    reporting_dimensions: List[str], error_margin_logic: str, absolute_error_margin_value: float,\n",
    "    relative_error_margin_value: float, min_absolute_error_margin: float,\n",
    "    top_n: int, global_rates: Dict[str, float], overall_avg_frequencies_in_qa: Dict[str, float],\n",
    "    confidence_level: float, z_score: float, save_plots_to_html: bool\n",
    ") -> None:\n",
    "    print(f\"\\n## Required Sample Size Calculations (Current State for Target Error Margin for {selected_metric})\")\n",
    "\n",
    "    num_col = metric_properties[selected_metric]['numerator_col']\n",
    "    den_col = metric_properties[selected_metric]['denominator_col']\n",
    "    freq_base_col = metric_properties[selected_metric]['freq_denominator_base_col']\n",
    "\n",
    "    analysis_dimensions = list(set(reporting_dimensions + ['customer', 'primary_product']))\n",
    "\n",
    "    for dim_name in analysis_dimensions:\n",
    "        if dim_name not in population_df.columns:\n",
    "            print(f\"Warning: Dimension '{dim_name}' not found in population data. Skipping sample size calculation for this dimension.\")\n",
    "            continue\n",
    "\n",
    "        aggregation_cols = {\n",
    "            num_col: (num_col, 'sum'),\n",
    "            den_col: (den_col, 'sum'),\n",
    "            freq_base_col: (freq_base_col, 'sum'),\n",
    "            'volume': ('volume', 'sum'),\n",
    "            'qa_reviewed_for_reporting': ('qa_reviewed_for_reporting', 'sum')\n",
    "        }\n",
    "        existing_aggregation_cols = {k: v for k, v in aggregation_cols.items() if k in population_df.columns}\n",
    "\n",
    "        if not existing_aggregation_cols:\n",
    "            print(f\"Warning: No relevant columns found for aggregation for dimension '{dim_name}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        temp_df_all_groups = population_df.groupby(dim_name).agg(\n",
    "            **existing_aggregation_cols\n",
    "        ).reset_index()\n",
    "\n",
    "        if temp_df_all_groups.empty:\n",
    "            print(f\"No data after grouping by '{dim_name}'. Skipping sample size calculation for this dimension.\")\n",
    "            continue\n",
    "\n",
    "        rest_agg_logic_for_raw_counts = {\n",
    "            num_col: 'sum',\n",
    "            den_col: 'sum',\n",
    "            freq_base_col: 'sum',\n",
    "            'volume': 'sum',\n",
    "            'qa_reviewed_for_reporting': 'sum'\n",
    "        }\n",
    "\n",
    "        aggregated_df_for_calculations = _aggregate_top_n_for_table_and_plot(\n",
    "            temp_df_all_groups,\n",
    "            group_col=dim_name,\n",
    "            n=top_n,\n",
    "            sort_col='volume',\n",
    "            value_cols=[num_col, den_col, freq_base_col, 'volume', 'qa_reviewed_for_reporting'],\n",
    "            rest_agg_logic=rest_agg_logic_for_raw_counts,\n",
    "            recalculate_margin_func=lambda df: np.nan\n",
    "        )\n",
    "\n",
    "        original_metric_rate = aggregated_df_for_calculations[num_col] / aggregated_df_for_calculations[den_col].round(3).replace(0, np.nan)\n",
    "        global_metric_rate = global_rates[selected_metric]\n",
    "        # Use global rate if numerator==0 or denominator==0 or metric_rate==0\n",
    "        metric_rate = np.where(\n",
    "            (aggregated_df_for_calculations[den_col] == 0) | \n",
    "            (aggregated_df_for_calculations[num_col] == 0) | \n",
    "            (original_metric_rate.fillna(0) == 0),\n",
    "            global_metric_rate,\n",
    "            original_metric_rate\n",
    "        )\n",
    "\n",
    "        # Add this line to indicate if global rate was used:\n",
    "        aggregated_df_for_calculations['Used Global Rate'] = (\n",
    "            (aggregated_df_for_calculations[den_col] == 0) | \n",
    "            (aggregated_df_for_calculations[num_col] == 0) | \n",
    "            (original_metric_rate.fillna(0) == 0)\n",
    "        )\n",
    "\n",
    "        aggregated_df_for_calculations['Metric Rate'] = original_metric_rate.round(2)\n",
    "        aggregated_df_for_calculations['Global Metric Rate'] = global_metric_rate.round(2)\n",
    "\n",
    "        # Use adj_error_margin from calculate_required_sessions\n",
    "        req_size_list, extra_reviews_list, adj_error_margin = calculate_required_sessions(\n",
    "            aggregated_df_for_calculations, selected_metric, global_rates, overall_avg_frequencies_in_qa,\n",
    "            error_margin_logic, absolute_error_margin_value, relative_error_margin_value, min_absolute_error_margin,\n",
    "            confidence_level, z_score\n",
    "        )\n",
    "        aggregated_df_for_calculations['Target Error Margin'] = adj_error_margin\n",
    "\n",
    "        aggregated_df_for_calculations['Required QA Reviewed Sessions'] = req_size_list\n",
    "        aggregated_df_for_calculations['Extra Reviews Needed'] = extra_reviews_list\n",
    "\n",
    "        current_margins = get_margins_df(aggregated_df_for_calculations, num_col, den_col, z_score)\n",
    "        aggregated_df_for_calculations['Current Error Margin'] = current_margins['margin']\n",
    "\n",
    "        aggregated_df_for_calculations['Meets Target Margin'] = (\n",
    "            aggregated_df_for_calculations['Current Error Margin'] <= aggregated_df_for_calculations['Target Error Margin']\n",
    "        )\n",
    "\n",
    "        if freq_base_col in aggregated_df_for_calculations.columns:\n",
    "        # Use the reporting column for current QA reviewed sessions\n",
    "            aggregated_df_for_calculations['Current QA Reviewed Sessions'] = aggregated_df_for_calculations['qa_reviewed_for_reporting']\n",
    "        else:\n",
    "            warnings.warn(f\"'{freq_base_col}' not found in aggregated_df_for_calculations. 'Current QA Reviewed Sessions' will be 0.\")\n",
    "            aggregated_df_for_calculations['Current QA Reviewed Sessions'] = 0.0\n",
    "\n",
    "        # if freq_base_col in aggregated_df_for_calculations.columns:\n",
    "        #     aggregated_df_for_calculations.rename(columns={freq_base_col: 'Current QA Reviewed Sessions'}, inplace=True)\n",
    "        # else:\n",
    "        #     warnings.warn(f\"'{freq_base_col}' not found in aggregated_df_for_calculations. 'Current QA Reviewed Sessions' will be 0.\")\n",
    "        #     aggregated_df_for_calculations['Current QA Reviewed Sessions'] = 0.0\n",
    "        display_cols = [\n",
    "            dim_name, 'Metric Rate', 'Used Global Rate', 'Global Metric Rate', 'Target Error Margin', 'Current QA Reviewed Sessions',\n",
    "            'Required QA Reviewed Sessions', 'Extra Reviews Needed', 'Current Error Margin', 'Meets Target Margin'\n",
    "        ]\n",
    "        existing_display_cols = [col for col in display_cols if col in aggregated_df_for_calculations.columns]\n",
    "\n",
    "         # Calculate totals for the relevant columns\n",
    "        total_current_sessions = aggregated_df_for_calculations['Current QA Reviewed Sessions'].sum()\n",
    "        total_required_sessions = aggregated_df_for_calculations['Required QA Reviewed Sessions'].sum()\n",
    "        total_extra_reviews = aggregated_df_for_calculations['Extra Reviews Needed'].sum()\n",
    "\n",
    "        # Prepare a total row (fill other columns with empty or suitable values)\n",
    "        total_row = {col: '' for col in existing_display_cols}\n",
    "        if 'Current QA Reviewed Sessions' in existing_display_cols:\n",
    "            total_row['Current QA Reviewed Sessions'] = total_current_sessions\n",
    "        if 'Required QA Reviewed Sessions' in existing_display_cols:\n",
    "            total_row['Required QA Reviewed Sessions'] = total_required_sessions\n",
    "        if 'Extra Reviews Needed' in existing_display_cols:\n",
    "            total_row['Extra Reviews Needed'] = total_extra_reviews\n",
    "        # For the dimension column, label as 'Total'\n",
    "        if dim_name in existing_display_cols:\n",
    "            total_row[dim_name] = 'Total'\n",
    "\n",
    "        # Append the total row to the DataFrame for display\n",
    "        display_df = aggregated_df_for_calculations[existing_display_cols].round(3)\n",
    "        display_df = pd.concat([display_df, pd.DataFrame([total_row])], ignore_index=True)\n",
    "\n",
    "        print(f\"\\n--- Required Sample Size per Group for {dim_name} (Top {top_n} by Volume + Rest) ---\")\n",
    "        display(display_df)\n",
    "\n",
    "        # print(f\"\\n--- Required Sample Size per Group for {dim_name} (Top {top_n} by Volume + Rest) ---\")\n",
    "        # display(aggregated_df_for_calculations[existing_display_cols].round(3))\n",
    "\n",
    "        plot_df = aggregated_df_for_calculations[[dim_name, 'Required QA Reviewed Sessions', 'Current QA Reviewed Sessions']].copy()\n",
    "\n",
    "        create_bar_chart(\n",
    "            df=plot_df,\n",
    "            x_col_or_index_values=dim_name,\n",
    "            y_cols=['Required QA Reviewed Sessions', 'Current QA Reviewed Sessions'],\n",
    "            title=f'Required vs. Current QA Reviewed Sessions for {selected_metric} per {dim_name} (Top {top_n} by Volume + Rest)',\n",
    "            y_axis_title='Number of Sessions',\n",
    "            x_axis_title=dim_name,\n",
    "            filename_suffix=f'required_vs_current_sessions_{dim_name}',\n",
    "            save_plots_to_html=save_plots_to_html\n",
    "        )\n",
    "        \n",
    "def _prepare_volume_stratified_df(df_input: pd.DataFrame, target_sessions_for_strat: int) -> pd.DataFrame:\n",
    "    \"\"\"Helper for Volume-Based Stratified Sampling.\"\"\"\n",
    "    df_copy = df_input.copy()\n",
    "    try:\n",
    "        num_quantiles = 5\n",
    "        if len(df_copy['volume'].unique()) < num_quantiles:\n",
    "            num_quantiles = len(df_copy['volume'].unique())\n",
    "            if num_quantiles < 2: raise ValueError(\"Not enough unique values for stratification.\")\n",
    "\n",
    "        df_copy['strata'] = pd.qcut(df_copy['volume'], q=num_quantiles, labels=False, duplicates='drop')\n",
    "        \n",
    "        if 'strata' in df_copy.columns:\n",
    "            strata_volumes_sum_per_row = df_copy.groupby('strata')['volume'].transform('sum')\n",
    "            total_pop_volume = df_copy['volume'].sum()\n",
    "            \n",
    "            df_copy['temp_sampling_volume_strat'] = df_copy.apply(\n",
    "                lambda row: (row['volume'] / strata_volumes_sum_per_row.loc[row.name]) * ((df_copy.groupby('strata')['volume'].sum().loc[row['strata']] / total_pop_volume) * target_sessions_for_strat)\n",
    "                if strata_volumes_sum_per_row.loc[row.name] > 0 and total_pop_volume > 0 else 0, axis=1\n",
    "            )\n",
    "            df_copy['temp_sampling_volume_strat'] = df_copy['temp_sampling_volume_strat'].fillna(0)\n",
    "        else:\n",
    "            warnings.warn(\"Could not create 'strata' for volume stratification. Falling back to simple random sampling.\")\n",
    "            df_copy['temp_sampling_volume_strat'] = df_copy['volume']\n",
    "    except (ValueError, Exception) as e:\n",
    "        warnings.warn(f\"Volume stratification error: {e}. Falling back to simple random sampling.\")\n",
    "        df_copy['temp_sampling_volume_strat'] = df_copy['volume']\n",
    "    return df_copy\n",
    "\n",
    "# def _run_single_simulation_scenario(population_df: pd.DataFrame, target_sessions: int, sampling_type: str, exponent: float,\n",
    "#                                     volume_col_for_sampling: str, selected_metric: str, global_avg_metrics: Dict[str, float],\n",
    "#                                     overall_avg_frequencies_in_qa: Dict[str, float], reporting_dimensions: List[str],\n",
    "#                                     error_margin_logic: str, absolute_error_margin_value: float, relative_error_margin_value: float,\n",
    "#                                     z_score: float, population_df_original_for_metrics: pd.DataFrame) -> Dict[str, Any]:\n",
    "#     \"\"\"\n",
    "#     Helper function to run a single simulation scenario and calculate its results.\n",
    "#     Returns a dictionary of results, does not print.\n",
    "#     \"\"\"\n",
    "#     scenario_results = {}\n",
    "\n",
    "#     def get_coverage_and_list(sim_df: pd.DataFrame, groupby_dims: List[str], margin_type: str, threshold: float) -> Tuple[int, str]:\n",
    "#         \"\"\"\n",
    "#         Calculates the number of groups whose error margin is below the target threshold.\n",
    "#         Also returns a comma-separated list of these groups.\n",
    "#         \"\"\"\n",
    "#         if sim_df.empty:\n",
    "#             return 0, \"\"\n",
    "            \n",
    "#         if margin_type not in sim_df.columns:\n",
    "#             warnings.warn(f\"Warning: Margin type '{margin_type}' not found in simulation DataFrame for coverage calculation. Coverage will be 0.\")\n",
    "#             return 0, \"\"\n",
    "\n",
    "#         condition_met = (sim_df[margin_type] <= threshold)\n",
    "#         coverage = sim_df[condition_met].shape[0] \n",
    "        \n",
    "#         if not groupby_dims: \n",
    "#             return coverage, \"\"\n",
    "\n",
    "#         existing_groupby_dims = [dim for dim in groupby_dims if dim in sim_df.columns]\n",
    "#         if not existing_groupby_dims:\n",
    "#             return coverage, \"\" \n",
    "\n",
    "#         values_list = sim_df.loc[condition_met, existing_groupby_dims].apply(\n",
    "#             lambda x: ':: '.join(map(str, x)) if isinstance(x.values[0], (tuple, list)) else str(x.values[0]), axis=1\n",
    "#         ).tolist()\n",
    "#         values_str = ', '.join(values_list)\n",
    "#         return coverage, values_str\n",
    "    \n",
    "#     overall_sim_df = calculate_sample_counts(\n",
    "#         population_df=population_df, total_target_sessions=target_sessions, \n",
    "#         groupby_dimensions=[], sampling_type=sampling_type, exponent=exponent, \n",
    "#         volume_col_for_sampling=volume_col_for_sampling,\n",
    "#         selected_metric=selected_metric, global_rates=global_avg_metrics, \n",
    "#         overall_avg_frequencies_in_qa=overall_avg_frequencies_in_qa, z_score=z_score\n",
    "#     )\n",
    "#     scenario_results['overall_margin'] = overall_sim_df['margin'].iloc[0] if not overall_sim_df.empty else np.inf\n",
    "#     scenario_results['total_sampled'] = target_sessions\n",
    "#     cost_total = REVIEW_COST_PER_SESSION * target_sessions if target_sessions > 0 else np.inf\n",
    "#     scenario_results['cost_total'] = cost_total\n",
    "\n",
    "#     margin_type_for_coverage = 'margin' if error_margin_logic == 'Absolute Error Margin' else 'margin_rel'\n",
    "#     threshold_for_coverage = absolute_error_margin_value if error_margin_logic == 'Absolute Error Margin' else relative_error_margin_value\n",
    "\n",
    "#     sim_dfs_for_metrics = {}\n",
    "    \n",
    "#     # Use population_df_original_for_metrics for calculating top_3_customers and strategic_clients\n",
    "#     top_3_customers = population_df_original_for_metrics.groupby('customer')['volume'].sum().nlargest(3).index.tolist()\n",
    "#     strategic_clients = population_df_original_for_metrics[population_df_original_for_metrics['customer_priority'] == 'Tier Platinum']['customer'].unique().tolist()\n",
    "#     total_strategic_clients_in_population = len(strategic_clients)\n",
    "\n",
    "#     for group_dim_list, key_suffix in [\n",
    "#         (reporting_dimensions, 'dimension'),\n",
    "#         (['customer'], 'customer'),\n",
    "#         (['primary_product'], 'product'),\n",
    "#         (['industry'], 'industry'),\n",
    "#         (['document_country'], 'document_country'),\n",
    "#         (['region'], 'region')\n",
    "#     ]:\n",
    "#         if all(dim in population_df.columns for dim in group_dim_list):\n",
    "#             sim_df = calculate_sample_counts(\n",
    "#                 population_df=population_df, total_target_sessions=target_sessions, \n",
    "#                 groupby_dimensions=group_dim_list, sampling_type=sampling_type, exponent=exponent, \n",
    "#                 volume_col_for_sampling=volume_col_for_sampling,\n",
    "#                 selected_metric=selected_metric, global_rates=global_avg_metrics, \n",
    "#                 overall_avg_frequencies_in_qa=overall_avg_frequencies_in_qa, z_score=z_score\n",
    "#             )\n",
    "#             coverage, values_str = get_coverage_and_list(sim_df, group_dim_list, margin_type_for_coverage, threshold_for_coverage)\n",
    "#             scenario_results[f\"{key_suffix}_coverage\"] = coverage\n",
    "#             scenario_results[f\"{key_suffix}_list\"] = values_str\n",
    "#             sim_dfs_for_metrics[key_suffix] = sim_df\n",
    "#         else:\n",
    "#             scenario_results[f\"{key_suffix}_coverage\"] = 0\n",
    "#             scenario_results[f\"{key_suffix}_list\"] = \"\"\n",
    "#             sim_dfs_for_metrics[key_suffix] = pd.DataFrame(columns=group_dim_list + ['margin', 'margin_rel', 'simulated_total_sessions'])\n",
    "\n",
    "#     if 'customer' in sim_dfs_for_metrics and not sim_dfs_for_metrics['customer'].empty:\n",
    "#         top_3_customers_in_sample_df = sim_dfs_for_metrics['customer'][sim_dfs_for_metrics['customer']['customer'].isin(top_3_customers)]\n",
    "#         share_top_3_customers = top_3_customers_in_sample_df['simulated_total_sessions'].sum() / target_sessions if target_sessions > 0 else 0\n",
    "#     else: share_top_3_customers = 0\n",
    "#     scenario_results['share_top_3_customers'] = share_top_3_customers * 100\n",
    "\n",
    "#     if 'customer' in sim_dfs_for_metrics and not sim_dfs_for_metrics['customer'].empty:\n",
    "#         strategic_clients_in_sample_df = sim_dfs_for_metrics['customer'][sim_dfs_for_metrics['customer']['customer'].isin(strategic_clients)]\n",
    "#         strategic_clients_met_threshold = strategic_clients_in_sample_df[strategic_clients_in_sample_df[margin_type_for_coverage] <= threshold_for_coverage].shape[0]\n",
    "#         share_strategic_clients_met_sla = (strategic_clients_met_threshold / total_strategic_clients_in_population) * 100 if total_strategic_clients_in_population > 0 else 0\n",
    "#     else: share_strategic_clients_met_sla = 0\n",
    "#     scenario_results['share_strategic_clients_met_sla'] = share_strategic_clients_met_sla\n",
    "\n",
    "#     total_coverage_for_cost = scenario_results['customer_coverage'] + scenario_results['product_coverage'] + scenario_results['industry_coverage'] + scenario_results['document_country_coverage'] + scenario_results['region_coverage']\n",
    "#     total_cost = REVIEW_COST_PER_SESSION * target_sessions if target_sessions > 0 else np.nan\n",
    "#     if total_coverage_for_cost > 0:\n",
    "#         cost_over_coverage = total_cost / total_coverage_for_cost\n",
    "#     else:\n",
    "#         cost_over_coverage = total_cost  # Full price if nothing is covered\n",
    "#     scenario_results['cost_over_coverage'] = cost_over_coverage\n",
    "\n",
    "#     return scenario_results\n",
    "\n",
    "def _run_single_simulation_scenario(\n",
    "    population_df: pd.DataFrame,\n",
    "    target_sessions: int,\n",
    "    sampling_type: str,\n",
    "    exponent: float,\n",
    "    volume_col_for_sampling: str,\n",
    "    selected_metric: str,\n",
    "    global_avg_metrics: Dict[str, float],\n",
    "    overall_avg_frequencies_in_qa: Dict[str, float],\n",
    "    reporting_dimensions: List[str],\n",
    "    error_margin_logic: str,\n",
    "    absolute_error_margin_value: float,\n",
    "    relative_error_margin_value: float,\n",
    "    z_score: float,\n",
    "    population_df_original_for_metrics: pd.DataFrame\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Helper function to run a single simulation scenario and calculate its results.\n",
    "    Returns a dictionary of results, does not print.\n",
    "    \"\"\"\n",
    "    scenario_results = {}\n",
    "\n",
    "    def aggregate_and_calc_margin(sim_df, group_col, num_col, den_col, z_score):\n",
    "        \"\"\"Aggregate by group_col and recalculate margin.\"\"\"\n",
    "        agg = sim_df.groupby(group_col).agg(\n",
    "            total_num=(num_col, 'sum'),\n",
    "            total_den=(den_col, 'sum')\n",
    "        ).reset_index()\n",
    "        margins = calculate_margin_wilson(agg['total_num'], agg['total_den'], z_score)\n",
    "        agg['margin'] = margins['margin']\n",
    "        agg['margin_rel'] = margins['margin_rel']\n",
    "        return agg\n",
    "    \n",
    "    def get_coverage_and_list(sim_df: pd.DataFrame, groupby_dims: List[str], margin_type: str, threshold: float) -> Tuple[int, str]:\n",
    "        \"\"\"\n",
    "        Calculates the number of groups whose error margin is below the target threshold.\n",
    "        Returns a comma-separated list of these groups, excluding those not in customer_top_20 if applicable.\n",
    "        \"\"\"\n",
    "        import warnings\n",
    "\n",
    "        if sim_df.empty:\n",
    "            return 0, \"\"\n",
    "        if margin_type not in sim_df.columns:\n",
    "            warnings.warn(f\"Warning: Margin type '{margin_type}' not found in simulation DataFrame for coverage calculation. Coverage will be 0.\")\n",
    "            return 0, \"\"\n",
    "\n",
    "        mask = sim_df[margin_type] <= threshold\n",
    "\n",
    "        # Exclude global rate fallback if column exists\n",
    "        if 'used_global_rate_fallback' in sim_df.columns:\n",
    "            mask &= ~sim_df['used_global_rate_fallback']\n",
    "\n",
    "        # Exclude groups with zero denominator if column exists\n",
    "        if 'simulated_den_n' in sim_df.columns:\n",
    "            mask &= sim_df['simulated_den_n'] > 10\n",
    "\n",
    "        # Exclude customers not in customer_top_20 if applicable\n",
    "        if 'customer_top_20' in sim_df.columns:\n",
    "            mask &= sim_df['customer_top_20'] != 'Other'\n",
    "\n",
    "        coverage = mask.sum()\n",
    "\n",
    "        if not groupby_dims:\n",
    "            return coverage, \"\"\n",
    "\n",
    "        existing_groupby_dims = [dim for dim in groupby_dims if dim in sim_df.columns]\n",
    "        if not existing_groupby_dims:\n",
    "            return coverage, \"\"\n",
    "\n",
    "        selected = sim_df.loc[mask, existing_groupby_dims].astype(str)\n",
    "        if selected.empty:\n",
    "            values_list = []\n",
    "        elif len(existing_groupby_dims) == 1:\n",
    "            values_list = selected.iloc[:, 0].tolist()\n",
    "        else:\n",
    "            values_list = selected.apply(lambda row: ':: '.join(row.values), axis=1).tolist()\n",
    "        values_str = ', '.join(values_list)\n",
    "        return coverage, values_str\n",
    "\n",
    "    \n",
    "    overall_sim_df = calculate_sample_counts(\n",
    "        population_df=population_df, total_target_sessions=target_sessions, \n",
    "        groupby_dimensions=[], sampling_type=sampling_type, exponent=exponent, \n",
    "        volume_col_for_sampling=volume_col_for_sampling,\n",
    "        selected_metric=selected_metric, global_rates=global_avg_metrics, \n",
    "        overall_avg_frequencies_in_qa=overall_avg_frequencies_in_qa, z_score=z_score\n",
    "    )\n",
    "    scenario_results['overall_margin'] = overall_sim_df['margin'].iloc[0] if not overall_sim_df.empty else np.inf\n",
    "    scenario_results['total_sampled'] = target_sessions\n",
    "    cost_total = REVIEW_COST_PER_SESSION * target_sessions if target_sessions > 0 else np.inf\n",
    "    scenario_results['cost_total'] = cost_total\n",
    "\n",
    "    margin_type_for_coverage = 'margin' if error_margin_logic == 'Absolute Error Margin' else 'margin_rel'\n",
    "    threshold_for_coverage = absolute_error_margin_value if error_margin_logic == 'Absolute Error Margin' else relative_error_margin_value\n",
    "\n",
    "    sim_dfs_for_metrics = {}\n",
    "\n",
    "    # Use population_df_original_for_metrics for calculating top_3_customers and strategic_clients\n",
    "    top_3_customers = population_df_original_for_metrics.groupby('customer')['volume'].sum().nlargest(3).index.tolist()\n",
    "\n",
    "    if 'customer_priority' in population_df_original_for_metrics.columns:\n",
    "        strategic_clients = population_df_original_for_metrics[population_df_original_for_metrics['customer_priority'] == 'Tier Platinum']['customer'].unique().tolist()\n",
    "    else:\n",
    "        print(\"WARNING: 'customer_priority' column missing in population_df_original_for_metrics\")\n",
    "        strategic_clients = []\n",
    "    \n",
    "    total_strategic_clients_in_population = len(strategic_clients)\n",
    "\n",
    "    # For each dimension, aggregate and recalculate margin for coverage\n",
    "    dimension_configs = [\n",
    "        (reporting_dimensions, 'dimension'),\n",
    "        (['customer'], 'customer'),\n",
    "        (['primary_product'], 'product'),\n",
    "        (['industry'], 'industry'),\n",
    "        (['document_country'], 'document_country'),\n",
    "        (['region'], 'region')\n",
    "    ]\n",
    "    for group_dim_list, key_suffix in dimension_configs:\n",
    "        if all(dim in population_df.columns for dim in group_dim_list):\n",
    "            sim_df = calculate_sample_counts(\n",
    "                population_df=population_df, total_target_sessions=target_sessions, \n",
    "                groupby_dimensions=group_dim_list, sampling_type=sampling_type, exponent=exponent, \n",
    "                volume_col_for_sampling=volume_col_for_sampling,\n",
    "                selected_metric=selected_metric, global_rates=global_avg_metrics, \n",
    "                overall_avg_frequencies_in_qa=overall_avg_frequencies_in_qa, z_score=z_score\n",
    "            )\n",
    "            # If group_dim_list is a single main group, aggregate by that group (to avoid slice double-counting)\n",
    "            if len(group_dim_list) == 1:\n",
    "                main_group = group_dim_list\n",
    "                sim_df_agg = aggregate_and_calc_margin(\n",
    "                    sim_df, group_col=main_group, num_col='simulated_num_x', den_col='simulated_den_n', z_score=z_score\n",
    "                )\n",
    "                # Add back simulated_total_sessions if possible\n",
    "                if 'simulated_total_sessions' in sim_df.columns:\n",
    "                    total_sessions = sim_df.groupby(main_group)['simulated_total_sessions'].sum().reset_index()\n",
    "                    sim_df_agg = sim_df_agg.merge(total_sessions, on=main_group, how='left')\n",
    "                else:\n",
    "                    sim_df_agg['simulated_total_sessions'] = np.nan\n",
    "                coverage, values_str = get_coverage_and_list(sim_df_agg, main_group, margin_type_for_coverage, threshold_for_coverage)\n",
    "                sim_dfs_for_metrics[key_suffix] = sim_df_agg\n",
    "            else:\n",
    "                # For multi-dim slices, keep as is\n",
    "                coverage, values_str = get_coverage_and_list(sim_df, group_dim_list, margin_type_for_coverage, threshold_for_coverage)\n",
    "                sim_dfs_for_metrics[key_suffix] = sim_df\n",
    "            scenario_results[f\"{key_suffix}_coverage\"] = coverage\n",
    "            scenario_results[f\"{key_suffix}_list\"] = values_str\n",
    "        else:\n",
    "            scenario_results[f\"{key_suffix}_coverage\"] = 0\n",
    "            scenario_results[f\"{key_suffix}_list\"] = \"\"\n",
    "            sim_dfs_for_metrics[key_suffix] = pd.DataFrame(columns=group_dim_list + ['margin', 'margin_rel', 'simulated_total_sessions'])\n",
    "\n",
    "    if 'customer' in sim_dfs_for_metrics and not sim_dfs_for_metrics['customer'].empty:\n",
    "        top_3_customers_in_sample_df = sim_dfs_for_metrics['customer'][sim_dfs_for_metrics['customer']['customer'].isin(top_3_customers)]\n",
    "        share_top_3_customers = top_3_customers_in_sample_df['simulated_total_sessions'].sum() / target_sessions if target_sessions > 0 else 0\n",
    "    else:\n",
    "        share_top_3_customers = 0\n",
    "    scenario_results['share_top_3_customers'] = share_top_3_customers * 100\n",
    "\n",
    "    if 'customer' in sim_dfs_for_metrics and not sim_dfs_for_metrics['customer'].empty:\n",
    "        strategic_clients_in_sample_df = sim_dfs_for_metrics['customer'][sim_dfs_for_metrics['customer']['customer'].isin(strategic_clients)]\n",
    "        strategic_clients_met_threshold = strategic_clients_in_sample_df[strategic_clients_in_sample_df[margin_type_for_coverage] <= threshold_for_coverage].shape[0]\n",
    "        share_strategic_clients_met_sla = (strategic_clients_met_threshold / total_strategic_clients_in_population) * 100 if total_strategic_clients_in_population > 0 else 0\n",
    "    else:\n",
    "        share_strategic_clients_met_sla = 0\n",
    "    scenario_results['share_strategic_clients_met_sla'] = share_strategic_clients_met_sla\n",
    "\n",
    "    total_coverage_for_cost = (\n",
    "        scenario_results['customer_coverage'] +\n",
    "        scenario_results['product_coverage'] +\n",
    "        scenario_results['industry_coverage'] +\n",
    "        scenario_results['document_country_coverage'] +\n",
    "        scenario_results['region_coverage']\n",
    "    )\n",
    "    total_cost = REVIEW_COST_PER_SESSION * target_sessions if target_sessions > 0 else np.nan\n",
    "    if total_coverage_for_cost > 0:\n",
    "        cost_over_coverage = total_cost / total_coverage_for_cost\n",
    "    else:\n",
    "        cost_over_coverage = total_cost  # Full price if nothing is covered\n",
    "    scenario_results['cost_over_coverage'] = cost_over_coverage\n",
    "\n",
    "    return scenario_results\n",
    "\n",
    "# def _run_single_simulation_scenario(\n",
    "#     population_df: pd.DataFrame,\n",
    "#     target_sessions: int,\n",
    "#     sampling_type: str,\n",
    "#     exponent: float,\n",
    "#     volume_col_for_sampling: str,\n",
    "#     selected_metric: str,\n",
    "#     global_avg_metrics: dict,\n",
    "#     overall_avg_frequencies_in_qa: dict,\n",
    "#     reporting_dimensions: list,\n",
    "#     error_margin_logic: str,\n",
    "#     absolute_error_margin_value: float,\n",
    "#     relative_error_margin_value: float,\n",
    "#     z_score: float,\n",
    "#     population_df_original_for_metrics: pd.DataFrame\n",
    "# ) -> dict:\n",
    "#     \"\"\"\n",
    "#     Run a single simulation scenario, calculating overall margin and per-dimension coverage.\n",
    "#     \"\"\"\n",
    "#     scenario_results = {}\n",
    "\n",
    "#     # 1. Calculate overall margin (no grouping)\n",
    "#     overall_sim_df = calculate_sample_counts(\n",
    "#         population_df=population_df,\n",
    "#         total_target_sessions=target_sessions,\n",
    "#         groupby_dimensions=[],\n",
    "#         sampling_type=sampling_type,\n",
    "#         exponent=exponent,\n",
    "#         volume_col_for_sampling=volume_col_for_sampling,\n",
    "#         selected_metric=selected_metric,\n",
    "#         global_rates=global_avg_metrics,\n",
    "#         overall_avg_frequencies_in_qa=overall_avg_frequencies_in_qa,\n",
    "#         z_score=z_score\n",
    "#     )\n",
    "#     scenario_results['overall_margin'] = overall_sim_df['margin'].iloc[0] if not overall_sim_df.empty else np.inf\n",
    "#     scenario_results['total_sampled'] = target_sessions\n",
    "#     cost_total = REVIEW_COST_PER_SESSION * target_sessions if target_sessions > 0 else np.inf\n",
    "#     scenario_results['cost_total'] = cost_total\n",
    "\n",
    "#     margin_type_for_coverage = 'margin' if error_margin_logic == 'Absolute Error Margin' else 'margin_rel'\n",
    "#     threshold_for_coverage = absolute_error_margin_value if error_margin_logic == 'Absolute Error Margin' else relative_error_margin_value\n",
    "\n",
    "#     sim_dfs_for_metrics = {}\n",
    "\n",
    "#     # 2. Prepare dimension list for per-dimension coverage\n",
    "#     dimension_list = [\n",
    "#         (reporting_dimensions, 'dimension'),\n",
    "#         (['customer'], 'customer'),\n",
    "#         (['primary_product'], 'product'),\n",
    "#         (['industry'], 'industry'),\n",
    "#         (['document_country'], 'document_country'),\n",
    "#         (['region'], 'region')\n",
    "#     ]\n",
    "\n",
    "#     for dims, key_suffix in dimension_list:\n",
    "#         available_dims = [dim for dim in dims if dim in population_df.columns]\n",
    "#         if dims and available_dims:\n",
    "#             # Calculate sample counts for this dimension\n",
    "#             sim_df = calculate_sample_counts(\n",
    "#                 population_df=population_df,\n",
    "#                 total_target_sessions=target_sessions,\n",
    "#                 groupby_dimensions=available_dims,\n",
    "#                 sampling_type=sampling_type,\n",
    "#                 exponent=exponent,\n",
    "#                 volume_col_for_sampling=volume_col_for_sampling,\n",
    "#                 selected_metric=selected_metric,\n",
    "#                 global_rates=global_avg_metrics,\n",
    "#                 overall_avg_frequencies_in_qa=overall_avg_frequencies_in_qa,\n",
    "#                 z_score=z_score\n",
    "#             )\n",
    "#             # Coverage mask\n",
    "#             mask = (sim_df[margin_type_for_coverage] <= threshold_for_coverage)\n",
    "#             if 'used_global_rate_fallback' in sim_df.columns:\n",
    "#                 mask &= ~sim_df['used_global_rate_fallback']\n",
    "#             if 'simulated_den_n' in sim_df.columns:\n",
    "#                 mask &= sim_df['simulated_den_n'] > 0\n",
    "#             if 'customer_top_20' in sim_df.columns and 'customer' in available_dims:\n",
    "#                 mask &= sim_df['customer_top_20'] != 'Rest'\n",
    "#             mask &= sim_df['simulated_den_n'] >= 50\n",
    "\n",
    "#             coverage = mask.sum()\n",
    "#             # List of covered values\n",
    "#             if len(available_dims) == 1:\n",
    "#                 values_str = ', '.join(sim_df.loc[mask, available_dims[0]].astype(str).tolist())\n",
    "#             else:\n",
    "#                 values_str = ', '.join(sim_df.loc[mask, available_dims].astype(str).agg('::'.join, axis=1).tolist())\n",
    "#             scenario_results[f\"{key_suffix}_coverage\"] = coverage\n",
    "#             scenario_results[f\"{key_suffix}_list\"] = values_str\n",
    "#             sim_dfs_for_metrics[key_suffix] = sim_df\n",
    "#         else:\n",
    "#             scenario_results[f\"{key_suffix}_coverage\"] = 0\n",
    "#             scenario_results[f\"{key_suffix}_list\"] = \"\"\n",
    "#             sim_dfs_for_metrics[key_suffix] = pd.DataFrame(columns=dims + ['margin', 'margin_rel', 'simulated_total_sessions'])\n",
    "\n",
    "#     # 3. Top customers and strategic clients logic (unchanged)\n",
    "#     top_3_customers = population_df_original_for_metrics.groupby('customer')['volume'].sum().nlargest(3).index.tolist()\n",
    "#     if 'customer_priority' in population_df_original_for_metrics.columns:\n",
    "#         strategic_clients = population_df_original_for_metrics[population_df_original_for_metrics['customer_priority'] == 'Tier Platinum']['customer'].unique().tolist()\n",
    "#     else:\n",
    "#         print(\"WARNING: 'customer_priority' column missing in population_df_original_for_metrics\")\n",
    "#         strategic_clients = []\n",
    "#     total_strategic_clients_in_population = len(strategic_clients)\n",
    "\n",
    "#     # Share of top 3 customers\n",
    "#     if 'customer' in sim_dfs_for_metrics and not sim_dfs_for_metrics['customer'].empty:\n",
    "#         top_3_customers_in_sample_df = sim_dfs_for_metrics['customer'][sim_dfs_for_metrics['customer']['customer'].isin(top_3_customers)]\n",
    "#         share_top_3_customers = top_3_customers_in_sample_df['simulated_total_sessions'].sum() / target_sessions if target_sessions > 0 else 0\n",
    "#     else:\n",
    "#         share_top_3_customers = 0\n",
    "#     scenario_results['share_top_3_customers'] = share_top_3_customers * 100\n",
    "\n",
    "#     # Share of strategic clients meeting SLA\n",
    "#     if 'customer' in sim_dfs_for_metrics and not sim_dfs_for_metrics['customer'].empty:\n",
    "#         strategic_clients_in_sample_df = sim_dfs_for_metrics['customer'][sim_dfs_for_metrics['customer']['customer'].isin(strategic_clients)]\n",
    "#         strategic_clients_met_threshold = strategic_clients_in_sample_df[strategic_clients_in_sample_df[margin_type_for_coverage] <= threshold_for_coverage].shape[0]\n",
    "#         share_strategic_clients_met_sla = (strategic_clients_met_threshold / total_strategic_clients_in_population) * 100 if total_strategic_clients_in_population > 0 else 0\n",
    "#     else:\n",
    "#         share_strategic_clients_met_sla = 0\n",
    "#     scenario_results['share_strategic_clients_met_sla'] = share_strategic_clients_met_sla\n",
    "\n",
    "#     # Cost per coverage\n",
    "#     total_coverage_for_cost = (\n",
    "#         scenario_results.get('customer_coverage', 0) +\n",
    "#         scenario_results.get('product_coverage', 0) +\n",
    "#         scenario_results.get('industry_coverage', 0) +\n",
    "#         scenario_results.get('document_country_coverage', 0) +\n",
    "#         scenario_results.get('region_coverage', 0)\n",
    "#     )\n",
    "#     total_cost = REVIEW_COST_PER_SESSION * target_sessions if target_sessions > 0 else np.nan\n",
    "#     if total_coverage_for_cost > 0:\n",
    "#         cost_over_coverage = total_cost / total_coverage_for_cost\n",
    "#     else:\n",
    "#         cost_over_coverage = total_cost\n",
    "#     scenario_results['cost_over_coverage'] = cost_over_coverage\n",
    "\n",
    "#     return scenario_results\n",
    "\n",
    "\n",
    "def _calculate_proportional_stratified_allocation(\n",
    "    df: pd.DataFrame, \n",
    "    stratify_by_dimension: List[str], \n",
    "    total_qa_sessions: int, \n",
    "    min_per_stratum: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates sample allocation weights for proportional stratified sampling.\n",
    "\n",
    "    This corrected logic performs a two-step allocation:\n",
    "    1. Allocates the total QA budget to each stratum proportional to its volume.\n",
    "    2. Applies a minimum sample count per stratum and re-normalizes to stay within budget.\n",
    "    3. Distributes the final stratum allocation to each row within it, proportional to the row's volume.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The population DataFrame.\n",
    "        stratify_by_dimension (List[str]): The dimensions to stratify by.\n",
    "        total_qa_sessions (int): The total number of QA sessions to allocate.\n",
    "        min_per_stratum (int): The minimum number of sessions to allocate to any stratum.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with an added column 'temp_sampling_volume_stratified_proportional'.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Step 1: Create a unique 'strata' identifier\n",
    "    valid_stratify_dims = [col for col in stratify_by_dimension if col in df_copy.columns]\n",
    "    if not valid_stratify_dims:\n",
    "        warnings.warn(\"Stratification dimensions not found. Falling back to volume.\")\n",
    "        df_copy['temp_sampling_volume_stratified_proportional'] = df_copy['volume']\n",
    "        return df_copy\n",
    "        \n",
    "    df_copy['strata'] = df_copy[valid_stratify_dims].apply(lambda row: tuple(row), axis=1)\n",
    "\n",
    "    # Step 2: Calculate initial allocation at the STRATUM level\n",
    "    strata_summary = df_copy.groupby('strata').agg(\n",
    "        stratum_volume=('volume', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    total_volume = df_copy['volume'].sum()\n",
    "    if total_volume == 0:\n",
    "        df_copy['temp_sampling_volume_stratified_proportional'] = 0\n",
    "        return df_copy\n",
    "        \n",
    "    # Proportional allocation\n",
    "    strata_summary['initial_allocation'] = (strata_summary['stratum_volume'] / total_volume) * total_qa_sessions\n",
    "    \n",
    "    # Step 3: Apply minimum per stratum and re-normalize to respect the budget\n",
    "    strata_summary['floored_allocation'] = np.maximum(strata_summary['initial_allocation'], min_per_stratum)\n",
    "    \n",
    "    total_floored_allocation = strata_summary['floored_allocation'].sum()\n",
    "    if total_floored_allocation > 0:\n",
    "        # Scale down allocations so their sum equals the total QA budget\n",
    "        strata_summary['final_allocation'] = (strata_summary['floored_allocation'] / total_floored_allocation) * total_qa_sessions\n",
    "    else:\n",
    "        strata_summary['final_allocation'] = 0\n",
    "        \n",
    "    # Step 4: Map the final STRATUM allocation back to each ROW\n",
    "    df_copy = df_copy.merge(strata_summary[['strata', 'final_allocation', 'stratum_volume']], on='strata', how='left')\n",
    "    \n",
    "    # Step 5: Distribute the stratum's allocation to rows within it, proportional to row volume\n",
    "    # This correctly calculates the weight for each individual row.\n",
    "    df_copy['row_allocation_weight'] = (df_copy['volume'] / df_copy['stratum_volume'].replace(0, 1)) * df_copy['final_allocation']\n",
    "    \n",
    "    # Final step: Ensure a row is not allocated more samples than its total volume\n",
    "    df_copy['temp_sampling_volume_stratified_proportional'] = np.minimum(df_copy['row_allocation_weight'], df_copy['volume'])\n",
    "    \n",
    "    return df_copy.drop(columns=['strata', 'final_allocation', 'stratum_volume', 'row_allocation_weight'])\n",
    "\n",
    "def _calculate_equal_stratified_allocation(\n",
    "    df: pd.DataFrame, \n",
    "    stratify_by_dimension: List[str], \n",
    "    total_qa_sessions: int, \n",
    "    min_per_stratum: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates sample allocation weights for equal stratified sampling.\n",
    "\n",
    "    This logic allocates the total QA budget to each stratum equally,\n",
    "    applies a minimum sample count per stratum, re-normalizes to stay within budget,\n",
    "    and distributes the final stratum allocation to each row within it, proportional to the row's volume.\n",
    "    It also ensures that no row is allocated more samples than its natural volume.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The population DataFrame.\n",
    "        stratify_by_dimension (List[str]): The dimensions to stratify by.\n",
    "        total_qa_sessions (int): The total number of QA sessions to allocate.\n",
    "        min_per_stratum (int): The minimum number of sessions to allocate to any stratum.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with an added column 'temp_sampling_volume_stratified_equal'.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Step 1: Create a unique 'strata' identifier\n",
    "    valid_stratify_dims = [col for col in stratify_by_dimension if col in df_copy.columns]\n",
    "    if not valid_stratify_dims:\n",
    "        warnings.warn(\"Stratification dimensions not found. Falling back to volume.\")\n",
    "        df_copy['temp_sampling_volume_stratified_equal'] = df_copy['volume']\n",
    "        return df_copy\n",
    "        \n",
    "    df_copy['strata'] = df_copy[valid_stratify_dims].apply(lambda row: tuple(row), axis=1)\n",
    "\n",
    "    # Step 2: Calculate initial allocation at the STRATUM level\n",
    "    strata_summary = df_copy.groupby('strata').agg(\n",
    "        stratum_volume=('volume', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    num_strata = len(strata_summary)\n",
    "    if num_strata == 0:\n",
    "        df_copy['temp_sampling_volume_stratified_equal'] = 0\n",
    "        return df_copy\n",
    "\n",
    "    # Equal allocation\n",
    "    strata_summary['initial_allocation'] = total_qa_sessions / num_strata\n",
    "    \n",
    "    # Step 3: Apply minimum per stratum and re-normalize to respect the budget\n",
    "    strata_summary['floored_allocation'] = np.maximum(strata_summary['initial_allocation'], min_per_stratum)\n",
    "    \n",
    "    total_floored_allocation = strata_summary['floored_allocation'].sum()\n",
    "    if total_floored_allocation > 0:\n",
    "        # Scale down allocations so their sum equals the total QA budget\n",
    "        strata_summary['final_allocation'] = (strata_summary['floored_allocation'] / total_floored_allocation) * total_qa_sessions\n",
    "    else:\n",
    "        strata_summary['final_allocation'] = 0\n",
    "        \n",
    "    # Step 4: Map the final STRATUM allocation back to each ROW\n",
    "    df_copy = df_copy.merge(strata_summary[['strata', 'final_allocation', 'stratum_volume']], on='strata', how='left')\n",
    "    \n",
    "    # Step 5: Distribute the stratum's allocation to rows within it, proportional to row volume\n",
    "    # Ensure a row is not allocated more samples than its natural volume\n",
    "    df_copy['row_allocation_weight'] = (df_copy['volume'] / df_copy['stratum_volume'].replace(0, 1)) * df_copy['final_allocation']\n",
    "    df_copy['temp_sampling_volume_stratified_equal'] = np.minimum(df_copy['row_allocation_weight'], df_copy['volume'])\n",
    "\n",
    "    # Additionally, ensure that the sum of allocations for each customer does not exceed their total volume\n",
    "    if 'customer' in df_copy.columns:\n",
    "        customer_total_volume = df_copy.groupby('customer')['volume'].transform('sum')\n",
    "        customer_alloc_sum = df_copy.groupby('customer')['temp_sampling_volume_stratified_equal'].transform('sum')\n",
    "        excess = np.maximum(customer_alloc_sum - customer_total_volume, 0)\n",
    "        # If excess exists, proportionally reduce allocations for that customer\n",
    "        if excess.any():\n",
    "            reduction_factor = np.where(customer_alloc_sum > 0, \n",
    "                                       (customer_total_volume / customer_alloc_sum), \n",
    "                                       1)\n",
    "            df_copy['temp_sampling_volume_stratified_equal'] = df_copy['temp_sampling_volume_stratified_equal'] * reduction_factor\n",
    "\n",
    "    return df_copy.drop(columns=['strata', 'final_allocation', 'stratum_volume', 'row_allocation_weight'])\n",
    "\n",
    "\n",
    "# def _calculate_equal_stratified_allocation(\n",
    "#     df: pd.DataFrame, \n",
    "#     stratify_by_dimension: List[str], \n",
    "#     total_qa_sessions: int, \n",
    "#     min_per_stratum: int\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Calculates sample allocation weights for equal stratified sampling.\n",
    "\n",
    "#     This logic allocates the total QA budget to each stratum equally,\n",
    "#     applies a minimum sample count per stratum, re-normalizes to stay within budget,\n",
    "#     and distributes the final stratum allocation to each row within it, proportional to the row's volume.\n",
    "#     It also ensures that no row is allocated more samples than its natural volume.\n",
    "\n",
    "#     Args:\n",
    "#         df (pd.DataFrame): The population DataFrame.\n",
    "#         stratify_by_dimension (List[str]): The dimensions to stratify by.\n",
    "#         total_qa_sessions (int): The total number of QA sessions to allocate.\n",
    "#         min_per_stratum (int): The minimum number of sessions to allocate to any stratum.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: The original DataFrame with an added column 'temp_sampling_volume_stratified_equal'.\n",
    "#     \"\"\"\n",
    "#     df_copy = df.copy()\n",
    "\n",
    "#     # Step 1: Create a unique 'strata' identifier\n",
    "#     valid_stratify_dims = [col for col in stratify_by_dimension if col in df_copy.columns]\n",
    "#     if not valid_stratify_dims:\n",
    "#         warnings.warn(\"Stratification dimensions not found. Falling back to volume.\")\n",
    "#         df_copy['temp_sampling_volume_stratified_equal'] = df_copy['volume']\n",
    "#         return df_copy\n",
    "        \n",
    "#     df_copy['strata'] = df_copy[valid_stratify_dims].apply(lambda row: tuple(row), axis=1)\n",
    "\n",
    "#     # Step 2: Calculate initial allocation at the STRATUM level\n",
    "#     strata_summary = df_copy.groupby('strata').agg(\n",
    "#         stratum_volume=('volume', 'sum')\n",
    "#     ).reset_index()\n",
    "    \n",
    "#     num_strata = len(strata_summary)\n",
    "#     if num_strata == 0:\n",
    "#         df_copy['temp_sampling_volume_stratified_equal'] = 0\n",
    "#         return df_copy\n",
    "\n",
    "#     # Equal allocation\n",
    "#     strata_summary['initial_allocation'] = total_qa_sessions / num_strata\n",
    "    \n",
    "#     # Step 3: Apply minimum per stratum and re-normalize to respect the budget\n",
    "#     strata_summary['floored_allocation'] = np.maximum(strata_summary['initial_allocation'], min_per_stratum)\n",
    "    \n",
    "#     total_floored_allocation = strata_summary['floored_allocation'].sum()\n",
    "#     if total_floored_allocation > 0:\n",
    "#         # Scale down allocations so their sum equals the total QA budget\n",
    "#         strata_summary['final_allocation'] = (strata_summary['floored_allocation'] / total_floored_allocation) * total_qa_sessions\n",
    "#     else:\n",
    "#         strata_summary['final_allocation'] = 0\n",
    "        \n",
    "#     # Step 4: Map the final STRATUM allocation back to each ROW\n",
    "#     df_copy = df_copy.merge(strata_summary[['strata', 'final_allocation', 'stratum_volume']], on='strata', how='left')\n",
    "    \n",
    "#     # Step 5: Distribute the stratum's allocation to rows within it, proportional to row volume\n",
    "#     df_copy['row_allocation_weight'] = (df_copy['volume'] / df_copy['stratum_volume'].replace(0, 1)) * df_copy['final_allocation']\n",
    "    \n",
    "#     # Final step: Ensure a row is not allocated more samples than its total volume\n",
    "#     df_copy['temp_sampling_volume_stratified_equal'] = np.minimum(df_copy['row_allocation_weight'], df_copy['volume'])\n",
    "    \n",
    "#     return df_copy.drop(columns=['strata', 'final_allocation', 'stratum_volume', 'row_allocation_weight'])\n",
    "\n",
    "def run_simulations(population_df: pd.DataFrame, global_avg_metrics: Dict[str, float], overall_avg_frequencies_in_qa: Dict[str, float],\n",
    "                    selected_metric: str, reporting_dimensions: List[str],\n",
    "                    error_margin_logic: str, absolute_error_margin_value: float,\n",
    "                    relative_error_margin_value: float, min_absolute_error_margin: float,\n",
    "                    target_overall_error_percent: float, target_dimension_error_percent: float,\n",
    "                    weight_factor_small_customers: float,\n",
    "                    small_customer_volume_quantile_threshold: float, stratify_by_dimension: List[str],\n",
    "                    sampling_types_to_run: List[str], top_n: int, z_score: float, min_per_stratum: int = 500, verbose = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Runs various sampling simulations to evaluate different strategies\n",
    "    for achieving target error margins and cost efficiency.\n",
    "    \"\"\"\n",
    "    simulation_results = {}\n",
    "    current_total_qa = population_df['qa_reviewed_for_reporting'].sum()\n",
    "    \n",
    "    def filter_existing_columns(df, columns):\n",
    "        return [col for col in columns if col in df.columns]\n",
    "\n",
    "    SCENARIO_CONFIGS = {\n",
    "        # 'Random': {\n",
    "        #     'type': 'random_multiples',\n",
    "        #     'scales': [0.3, 0.5, 0.7, 1, 1.5, 2, 3]\n",
    "        # },\n",
    "        'Current Random': {\n",
    "            'type': 'single_run',\n",
    "            'scales': [1, 0.3, 0.5, 0.7, 2, 3],\n",
    "            'sampling_type': 'random',\n",
    "            'exponent': 1,\n",
    "            'volume_col_modifier': 'qa_reviewed',\n",
    "            'df_modifier_func': lambda df: df\n",
    "        },\n",
    "        'True Random': {\n",
    "            'type': 'single_run',\n",
    "            'scales': [1, 0.3, 0.5, 0.7, 2, 3],\n",
    "            'sampling_type': 'random',\n",
    "            'exponent': 1,\n",
    "            'volume_col_modifier': 'qa_reviewed_for_reporting',\n",
    "            'df_modifier_func': lambda df: df\n",
    "        },\n",
    "        'Oversample Small Traditional': {\n",
    "            'type': 'single_run',\n",
    "            'scales': [1],\n",
    "            'sampling_type': 'random',\n",
    "            'exponent': 1,\n",
    "            'volume_col_modifier': 'temp_sampling_volume_small',\n",
    "            'df_modifier_func': lambda df: (\n",
    "                df.merge(\n",
    "                    df.groupby('customer')['volume'].sum().rename('customer_total_volume'),\n",
    "                    left_on='customer', right_index=True\n",
    "                ).assign(\n",
    "                    temp_sampling_volume_small=lambda x: np.where(\n",
    "                        x['customer_total_volume'] < x['customer_total_volume'].quantile(small_customer_volume_quantile_threshold),\n",
    "                        x['volume'] * weight_factor_small_customers,\n",
    "                        x['volume']\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        },\n",
    "        'Client-Tier Weighting': {\n",
    "            'type': 'single_run',\n",
    "            'scales': [1],\n",
    "            'sampling_type': 'random',\n",
    "            'exponent': 1,\n",
    "            'volume_col_modifier': 'temp_sampling_volume_tier',\n",
    "            # 'df_modifier_func': lambda df: df.assign(\n",
    "            #     temp_sampling_volume_tier=df['customer_priority'].map(\n",
    "            #         {'Tier Platinum': 3, 'Tier 1': 2, 'Tier 2': 1, 'Tier 3': 1, 'Tier 4': 1, 'Tier 5': 1, '': 1}\n",
    "            #     ).fillna(1) * df['volume']\n",
    "            #)\n",
    "            'df_modifier_func': lambda df: (\n",
    "                lambda weights: df.assign(\n",
    "                    temp_sampling_volume_tier=df['customer_priority'].map(weights).fillna(1) * df['volume']\n",
    "                )\n",
    "            )(get_tier_weights_by_target_proportion(df, TARGET_QA_PROPORTIONS))\n",
    "        },\n",
    "        'Proportional Stratified Sampling': {\n",
    "            'type': 'single_run',\n",
    "            'scales': [1,2],\n",
    "            'sampling_type': 'random',\n",
    "            'exponent': 1,\n",
    "            'volume_col_modifier': 'temp_sampling_volume_stratified_proportional',\n",
    "            'df_modifier_func': lambda df: _calculate_proportional_stratified_allocation(\n",
    "                df=df,\n",
    "                stratify_by_dimension=stratify_by_dimension,\n",
    "                total_qa_sessions=current_total_qa,\n",
    "                min_per_stratum=min_per_stratum\n",
    "            )\n",
    "            # 'df_modifier_func': lambda df, stratify_by_dimension=stratify_by_dimension: (\n",
    "            #     (lambda _df, strata_volumes, total_pop_volume, min_per_stratum, customer_total_volumes: \n",
    "            #         _df\n",
    "            #         .groupby('strata', group_keys=False)\n",
    "            #         .apply(lambda g: (\n",
    "            #             g.assign(\n",
    "            #                 temp_sampling_volume_stratified_proportional=(\n",
    "            #                     np.minimum(\n",
    "            #                         np.maximum(\n",
    "            #                             g['volume'] / g['volume'].sum() * (strata_volumes[g.name] / total_pop_volume * current_total_qa)\n",
    "            #                             if g['volume'].sum() > 0 and total_pop_volume > 0 else 0,\n",
    "            #                             min_per_stratum / len(g) if len(g) > 0 else 0\n",
    "            #                         ),\n",
    "            #                         g['volume']\n",
    "            #                     )\n",
    "            #                 )\n",
    "            #             )\n",
    "            #             # Cap at customer's total volume (across all strata)\n",
    "            #             .assign(\n",
    "            #                 temp_sampling_volume_stratified_proportional=lambda x: np.minimum(\n",
    "            #                     x['temp_sampling_volume_stratified_proportional'],\n",
    "            #                     x['customer'].map(customer_total_volumes)\n",
    "            #                 )\n",
    "            #             )\n",
    "            #         ))\n",
    "            #         .reset_index(drop=True)\n",
    "            #     )(\n",
    "            #         df.assign(strata=df[filter_existing_columns(df, stratify_by_dimension)].apply(lambda row: tuple(row), axis=1)),\n",
    "            #         df.groupby(df[filter_existing_columns(df, stratify_by_dimension)].apply(lambda row: tuple(row), axis=1))['volume'].sum(),\n",
    "            #         df['volume'].sum(),\n",
    "            #         min_per_stratum,\n",
    "            #         df.groupby('customer')['volume'].sum().to_dict()\n",
    "            #     )\n",
    "            # )\n",
    "        },\n",
    "        'Equal Stratified Sampling': {\n",
    "            'type': 'single_run',\n",
    "            'scales': [1,2],\n",
    "            'sampling_type': 'random',\n",
    "            'exponent': 1,\n",
    "            'volume_col_modifier': 'temp_sampling_volume_stratified_equal',\n",
    "            'df_modifier_func': lambda df: _calculate_equal_stratified_allocation(\n",
    "                df=df,\n",
    "                stratify_by_dimension=stratify_by_dimension,\n",
    "                total_qa_sessions=current_total_qa,\n",
    "                min_per_stratum=min_per_stratum\n",
    "            )\n",
    "            # 'df_modifier_func': lambda df, stratify_by_dimension=stratify_by_dimension: (\n",
    "            #     (lambda _df, n_strata, min_per_stratum, customer_total_volumes:\n",
    "            #         _df\n",
    "            #         .groupby('strata', group_keys=False)\n",
    "            #         .apply(lambda g: (\n",
    "            #             g.assign(\n",
    "            #                 temp_sampling_volume_stratified_equal=(\n",
    "            #                     np.minimum(\n",
    "            #                         np.maximum(\n",
    "            #                             (current_total_qa // n_strata) / len(g) if len(g) > 0 else 0,\n",
    "            #                             min_per_stratum / len(g) if len(g) > 0 else 0\n",
    "            #                         ),\n",
    "            #                         g['volume']\n",
    "            #                     )\n",
    "            #                 )\n",
    "            #             )\n",
    "            #             .assign(\n",
    "            #                 temp_sampling_volume_stratified_equal=lambda x: np.minimum(\n",
    "            #                     x['temp_sampling_volume_stratified_equal'],\n",
    "            #                     x['customer'].map(customer_total_volumes)\n",
    "            #                 )\n",
    "            #             )\n",
    "            #         ))\n",
    "            #         .reset_index(drop=True)\n",
    "            #     )(\n",
    "            #         df.assign(strata=df[filter_existing_columns(df, stratify_by_dimension)].apply(lambda row: tuple(row), axis=1)),\n",
    "            #         df[filter_existing_columns(df, stratify_by_dimension)].apply(lambda row: tuple(row), axis=1).nunique(),\n",
    "            #         min_per_stratum,\n",
    "            #         df.groupby('customer')['volume'].sum().to_dict()\n",
    "            #     )\n",
    "            # )\n",
    "        },\n",
    "        'Volume-Based Stratified Sampling': {\n",
    "            'type': 'single_run',\n",
    "            'scales': [1],\n",
    "            'sampling_type': 'random',\n",
    "            'exponent': 1,\n",
    "            'volume_col_modifier': 'temp_sampling_volume_strat',\n",
    "            'df_modifier_func': _prepare_volume_stratified_df\n",
    "        },\n",
    "        'Biased with Exponents': {\n",
    "            'type': 'biased_multiples',\n",
    "            'scales': [1],\n",
    "            'exponents': [0.3, 0.7, 0.8, 1.5]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for scenario_name in sampling_types_to_run:\n",
    "        if scenario_name not in SCENARIO_CONFIGS:\n",
    "            if verbose:\n",
    "                print(f\"Warning: Unknown simulation scenario '{scenario_name}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        config = SCENARIO_CONFIGS[scenario_name]\n",
    "        \n",
    "        if config['type'] == 'random_multiples':\n",
    "            if verbose:\n",
    "                print(f'\\n--- Running {scenario_name} SIMULATIONS ---')\n",
    "            for multiplier in config['scales']:\n",
    "                target_sessions = int(current_total_qa * multiplier)\n",
    "                if target_sessions == 0 and current_total_qa != 0:\n",
    "                    continue\n",
    "                name = f'{scenario_name} {multiplier:.1f}x' if current_total_qa > 0 else f'{scenario_name} {target_sessions}'\n",
    "                if verbose:\n",
    "                    print(f\"  - Simulation: {name} (Target Sessions: {target_sessions})\")\n",
    "\n",
    "                scenario_results = _run_single_simulation_scenario(\n",
    "                    population_df, target_sessions, 'random', 1, 'volume',\n",
    "                    selected_metric, global_avg_metrics, overall_avg_frequencies_in_qa, reporting_dimensions,\n",
    "                    error_margin_logic, absolute_error_margin_value, relative_error_margin_value,\n",
    "                    z_score, population_df # Pass original population_df for metrics\n",
    "                )\n",
    "                simulation_results[name] = scenario_results\n",
    "\n",
    "        elif config['type'] == 'single_run':\n",
    "            # print(f'\\n--- Running {scenario_name} SIMULATION ---')\n",
    "            # target_sessions = current_total_qa \n",
    "            for multiplier in config['scales']:\n",
    "                target_sessions = int(current_total_qa * multiplier)\n",
    "                name = f'{scenario_name} {multiplier:.1f}x' if current_total_qa > 0 else f'{scenario_name} {target_sessions}'\n",
    "                if target_sessions == 0: \n",
    "                    if verbose:\n",
    "                        print(f\"  - Skipping '{scenario_name}' simulation due to 0 target sessions.\")\n",
    "                    continue\n",
    "                if verbose:\n",
    "                    print(f\"  - Simulation: {scenario_name} (Target Sessions: {target_sessions})\")\n",
    "\n",
    "                if scenario_name == 'Volume-Based Stratified Sampling':\n",
    "                    df_modified = config['df_modifier_func'](population_df.copy(), target_sessions)\n",
    "                elif scenario_name in ['Proportional Stratified Sampling', 'Equal Stratified Sampling']:\n",
    "                    if verbose:\n",
    "                        print(\"Stratified by:\", stratify_by_dimension)\n",
    "                    df_modified = config['df_modifier_func'](population_df.copy())\n",
    "                #     df_modified = config['df_modifier_func'](population_df.copy(), stratify_by_dimension)\n",
    "                else:\n",
    "                    df_modified = config['df_modifier_func'](population_df.copy())\n",
    "                \n",
    "                scenario_results = _run_single_simulation_scenario(\n",
    "                    df_modified, target_sessions, config['sampling_type'], config['exponent'], \n",
    "                    config['volume_col_modifier'],\n",
    "                    selected_metric, global_avg_metrics, overall_avg_frequencies_in_qa, reporting_dimensions,\n",
    "                    error_margin_logic, absolute_error_margin_value, relative_error_margin_value,\n",
    "                    z_score, population_df# Pass original population_df for metrics\n",
    "                )\n",
    "                simulation_results[name] = scenario_results\n",
    "\n",
    "        elif config['type'] == 'biased_multiples':\n",
    "            if verbose:\n",
    "                print(f'\\n--- Running {scenario_name} SIMULATIONS ---')\n",
    "            for exp in config['exponents']:\n",
    "                target_sessions = current_total_qa\n",
    "                if target_sessions == 0: continue\n",
    "                name = f'Biased Exp={exp}'\n",
    "                if verbose:\n",
    "                    print(f\"  - Simulation: {name} (Target Sessions: {target_sessions}, Exponent: {exp})\")\n",
    "\n",
    "                scenario_results = _run_single_simulation_scenario(\n",
    "                    population_df, target_sessions, 'biased', exp, 'volume',\n",
    "                    selected_metric, global_avg_metrics, overall_avg_frequencies_in_qa, reporting_dimensions,\n",
    "                    error_margin_logic, absolute_error_margin_value, relative_error_margin_value,\n",
    "                    z_score, population_df # Pass original population_df for metrics\n",
    "                )\n",
    "                simulation_results[name] = scenario_results\n",
    "    \n",
    "\n",
    "    # # User-defined constraints and weights (could be UI-driven)\n",
    "    # CONSTRAINTS = {\n",
    "    #     'overall_margin': lambda x: x <= TARGET_OVERALL_ERROR_PERCENT / 100,\n",
    "    #     'customer_coverage': lambda x: x >= 1,  # e.g., at least 1 customer below threshold\n",
    "    #     'product_coverage': lambda x: x >= 1,  # e.g., at least 1 product below threshold\n",
    "    #     'industry_coverage': lambda x: x >= 1,  # e.g., at least 1 industry below threshold\n",
    "    #     'document_country_coverage': lambda x: x >= 1,  # e.g., at least 1 document country below threshold\n",
    "    #     'region_coverage': lambda x: x >= 1,  # e.g., at least 1 region below threshold\n",
    "    #     'share_strategic_clients_met_sla': lambda x: x >= 0.2,  # e.g., at least 20% of strategic clients met SLA\n",
    "    # }\n",
    "    # WEIGHTS = {\n",
    "    #     'cost_total': -1,  # minimize\n",
    "    #     'customer_coverage': 2,  # maximize\n",
    "    #     'overall_margin': -1,  # minimize\n",
    "    #     'share_strategic_clients_met_sla': 1,  # maximize\n",
    "    #     'product_coverage': 1,  # maximize\n",
    "    #     'industry_coverage': 1,  # maximize\n",
    "    #     'document_country_coverage': 1,  # maximize\n",
    "    #     'region_coverage': 1,  # maximize\n",
    "    #     'share_top_3_customers': 1,  # maximize\n",
    "    #     'total_sampled': -1  # minimize total sampled sessions\n",
    "    # }\n",
    "\n",
    "    # def score_simulation(res):\n",
    "    #     score = 0\n",
    "    #     for k, w in WEIGHTS.items():\n",
    "    #         v = res.get(k, 0)\n",
    "    #         score += w * (v if pd.notnull(v) else 0)\n",
    "    #     return score\n",
    "\n",
    "    # # Filter by constraints\n",
    "    # filtered = [res for res in simulation_results.values() if all(f(res.get(k, np.nan)) for k, f in CONSTRAINTS.items())]\n",
    "    # if filtered:\n",
    "    #     best = max(filtered, key=score_simulation)\n",
    "    # else:\n",
    "    #     best = max(simulation_results.values(), key=score_simulation)\n",
    "    # print(\"Best scenario by multi-objective score:\", best.get('strategy_name', 'N/A'))\n",
    "\n",
    "    # --- Optimal Solution Determination ---\n",
    "    optimal_size_info = None\n",
    "    max_coverage = -1\n",
    "    \n",
    "    for sim_name, res in simulation_results.items():\n",
    "        if res['overall_margin'] == np.inf: continue\n",
    "\n",
    "        overall_margin_percent = res['overall_margin'] * 100\n",
    "        current_total_coverage = res['customer_coverage'] + res['product_coverage'] + res['industry_coverage'] + res['document_country_coverage'] + res['region_coverage']\n",
    "\n",
    "        if overall_margin_percent <= target_overall_error_percent:\n",
    "            if optimal_size_info is None or current_total_coverage > max_coverage:\n",
    "                max_coverage = current_total_coverage\n",
    "                optimal_size_info = res\n",
    "                optimal_size_info['strategy_name'] = sim_name\n",
    "            elif current_total_coverage == max_coverage:\n",
    "                if res['total_sampled'] < optimal_size_info['total_sampled']:\n",
    "                    optimal_size_info = res\n",
    "                    optimal_size_info['strategy_name'] = sim_name\n",
    "\n",
    "    if optimal_size_info is None:\n",
    "        if verbose:\n",
    "            print(\"\\nNo simulation met the overall error margin target. Finding the simulation with the highest coverage regardless of overall margin.\")\n",
    "        max_coverage_any_margin = -1\n",
    "        for sim_name, res in simulation_results.items():\n",
    "            if res['overall_margin'] == np.inf: continue\n",
    "            current_total_coverage = res['customer_coverage'] + res['product_coverage'] + res['industry_coverage'] + res['document_country_coverage'] + res['region_coverage']\n",
    "            if optimal_size_info is None or current_total_coverage > max_coverage_any_margin:\n",
    "                max_coverage_any_margin = current_total_coverage\n",
    "                optimal_size_info = res\n",
    "                optimal_size_info['strategy_name'] = sim_name\n",
    "            elif current_total_coverage == max_coverage_any_margin:\n",
    "                if res['total_sampled'] < optimal_size_info['total_sampled']:\n",
    "                    optimal_size_info = res\n",
    "                    optimal_size_info['strategy_name'] = sim_name\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\n## Optimal Solution Summary')\n",
    "        if optimal_size_info:\n",
    "            # Show which threshold was used\n",
    "            if ERROR_MARGIN_LOGIC == 'Absolute Error Margin':\n",
    "                threshold_type = \"Absolute Error Margin\"\n",
    "                threshold_value = ABSOLUTE_ERROR_MARGIN_VALUE * 100\n",
    "                threshold_unit = \"%\"\n",
    "            else:\n",
    "                threshold_type = \"Relative Error Margin\"\n",
    "                threshold_value = RELATIVE_ERROR_MARGIN_VALUE * 100\n",
    "                threshold_unit = \"% of group rate\"\n",
    "\n",
    "            print(f\"Threshold logic used: {threshold_type} (Threshold: {threshold_value:.2f}{threshold_unit})\")\n",
    "            optimal_size = optimal_size_info['total_sampled']\n",
    "            print(f\"Optimal sample size: {optimal_size} sessions (Strategy: {optimal_size_info['strategy_name']})\")\n",
    "            print(f\"  - Overall error margin for this strategy: {optimal_size_info['overall_margin']*100:.2f}%\")\n",
    "            print(f\"  - Covers {optimal_size_info['customer_coverage']} customers below threshold.\")\n",
    "            print(f\"  - Customers meeting target: {optimal_size_info['customer_list']}\")\n",
    "            print(f\"  - Covers {optimal_size_info['product_coverage']} products below threshold.\")\n",
    "            print(f\"  - Products meeting target: {optimal_size_info['product_list']}\")\n",
    "            print(f\"  - Covers {optimal_size_info['industry_coverage']} industries below threshold.\")\n",
    "            print(f\"  - Industries meeting target: {optimal_size_info['industry_list']}\")\n",
    "            print(f\"  - Covers {optimal_size_info['document_country_coverage']} document countries below threshold.\")\n",
    "            print(f\"  - Document countries meeting target: {optimal_size_info['document_country_list']}\")\n",
    "            print(f\"  - Covers {optimal_size_info['region_coverage']} regions below threshold.\")\n",
    "            print(f\"  - Regions meeting target: {optimal_size_info['region_list']}\")\n",
    "            print(f\"  - Share of top 3 customers: {optimal_size_info['share_top_3_customers']:.2f}%\")\n",
    "            print(f\"  - Share of strategic clients meeting SLA: {optimal_size_info['share_strategic_clients_met_sla']:.2f}%\")\n",
    "            print(f\"  - Total Cost for this strategy: ${optimal_size_info['cost_total']:.2f} EUR\")\n",
    "        else:\n",
    "            print(\"Could not determine an optimal sample size. Defaulting to current total QA reviewed sessions for plotting.\")\n",
    "\n",
    "    if verbose:\n",
    "        # Plotly Visualizations\n",
    "        summary_df = pd.DataFrame.from_dict(simulation_results, orient='index')\n",
    "        if summary_df.empty:\n",
    "            print(\"\\nNo simulation results to plot.\")\n",
    "            return simulation_results # Return empty results\n",
    "\n",
    "        summary_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        summary_df['overall_margin_perc'] = summary_df['overall_margin'] * 100\n",
    "\n",
    "        # Only include dimension_coverage if reporting_dimensions is not ['customer'] or ['product']\n",
    "        if REPORTING_DIMENSIONS == ['customer'] or REPORTING_DIMENSIONS == ['product']:\n",
    "            coverage_cols = ['customer_coverage', 'product_coverage', 'industry_coverage', 'document_country_coverage', 'region_coverage']\n",
    "        else:\n",
    "            coverage_cols = ['dimension_coverage', 'customer_coverage', 'product_coverage', 'industry_coverage', 'document_country_coverage', 'region_coverage']\n",
    "\n",
    "        if all(col in summary_df.columns for col in coverage_cols):\n",
    "            create_heatmap(\n",
    "                df=summary_df[coverage_cols].T,\n",
    "                title='Coverage Below Target Margin per Simulation',\n",
    "                filename_suffix='simulation_coverage_heatmap',\n",
    "                save_plots_to_html=SAVE_PLOTS_TO_HTML\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\"Missing coverage columns for imshow plot.\")\n",
    "\n",
    "        if all(col in summary_df.columns for col in coverage_cols + ['overall_margin_perc']):\n",
    "            create_combined_bar_line_chart(\n",
    "                df=summary_df,\n",
    "                x_col_or_index_values=summary_df.index,\n",
    "                bar_cols=coverage_cols,\n",
    "                line_col='overall_margin_perc',\n",
    "                title='Coverage and Overall Error Margins Across Simulations',\n",
    "                y1_axis_title='Coverage Count',\n",
    "                y2_axis_title='Overall Error Margin (%)',\n",
    "                filename_suffix='simulation_coverage_and_margin_bar',\n",
    "                save_plots_to_html=SAVE_PLOTS_TO_HTML\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\"Missing required columns for Coverage and Overall Error Margins plot.\")\n",
    "\n",
    "        if 'cost_total' in summary_df.columns and summary_df['cost_total'].dropna().any():\n",
    "            create_bar_chart(\n",
    "                df=summary_df,\n",
    "                x_col_or_index_values=summary_df.index,\n",
    "                y_cols=['cost_total'],\n",
    "                title='Total Cost Across Strategies',\n",
    "                y_axis_title='Total Cost (EUR)',\n",
    "                x_axis_title='Simulation',\n",
    "                filename_suffix='simulation_cost_total',\n",
    "                save_plots_to_html=SAVE_PLOTS_TO_HTML\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\"Missing 'cost_total' column or all values are NaN/inf for plotting.\")\n",
    "\n",
    "        print('\\n## Detailed Plots for Optimal Solution')\n",
    "\n",
    "        if optimal_size_info:\n",
    "            optimal_strategy_name = optimal_size_info['strategy_name']\n",
    "            optimal_target_sessions = optimal_size_info['total_sampled']\n",
    "\n",
    "            plot_sampling_type = 'random'\n",
    "            plot_exponent = 1.0\n",
    "            plot_volume_col = 'volume'\n",
    "            plot_df_source = population_df.copy()\n",
    "\n",
    "            if 'Oversample Small' in optimal_strategy_name:\n",
    "                plot_df_source = SCENARIO_CONFIGS['Oversample Small Traditional']['df_modifier_func'](plot_df_source)\n",
    "                plot_volume_col = SCENARIO_CONFIGS['Oversample Small Traditional']['volume_col_modifier']\n",
    "            elif 'Client-Tier Weighting' in optimal_strategy_name:\n",
    "                plot_df_source = SCENARIO_CONFIGS['Client-Tier Weighting']['df_modifier_func'](plot_df_source)\n",
    "                plot_volume_col = SCENARIO_CONFIGS['Client-Tier Weighting']['volume_col_modifier']\n",
    "            elif 'Proportional Stratified' in optimal_strategy_name:\n",
    "                plot_df_source = SCENARIO_CONFIGS['Proportional Stratified Sampling']['df_modifier_func'](plot_df_source)\n",
    "                plot_volume_col = SCENARIO_CONFIGS['Proportional Stratified Sampling']['volume_col_modifier']\n",
    "            elif 'Equal Stratified' in optimal_strategy_name:\n",
    "                plot_df_source = SCENARIO_CONFIGS['Equal Stratified Sampling']['df_modifier_func'](plot_df_source)\n",
    "                plot_volume_col = SCENARIO_CONFIGS['Equal Stratified Sampling']['volume_col_modifier']\n",
    "            elif 'Volume-Based Stratified' in optimal_strategy_name:\n",
    "                plot_df_source = SCENARIO_CONFIGS['Volume-Based Stratified Sampling']['df_modifier_func'](plot_df_source, optimal_target_sessions)\n",
    "                plot_volume_col = SCENARIO_CONFIGS['Volume-Based Stratified Sampling']['volume_col_modifier']\n",
    "            elif 'Biased Exp=' in optimal_strategy_name:\n",
    "                plot_sampling_type = 'biased'\n",
    "                plot_exponent = float(optimal_strategy_name.split('=')[1])\n",
    "                plot_volume_col = 'volume'\n",
    "\n",
    "            optimal_sim_df_for_plots = calculate_sample_counts(plot_df_source, total_target_sessions=optimal_target_sessions, \n",
    "                                                                groupby_dimensions=REPORTING_DIMENSIONS, \n",
    "                                                                sampling_type=plot_sampling_type, exponent=plot_exponent, \n",
    "                                                                volume_col_for_sampling=plot_volume_col,\n",
    "                                                                selected_metric=selected_metric, global_rates=global_avg_metrics,\n",
    "                                                                overall_avg_frequencies_in_qa=overall_avg_frequencies_in_qa, z_score=z_score)\n",
    "\n",
    "            sort_col_for_plots = 'simulated_total_sessions'\n",
    "\n",
    "            if 'customer' in optimal_sim_df_for_plots.columns:\n",
    "                # Aggregate by customer, sum numerators/denominators, recalculate margin\n",
    "                if 'used_global_rate_fallback' not in optimal_sim_df_for_plots.columns:\n",
    "                    optimal_sim_df_for_plots['used_global_rate_fallback'] = False\n",
    "\n",
    "                agg_customers = (\n",
    "                    optimal_sim_df_for_plots\n",
    "                    .groupby('customer', as_index=False)\n",
    "                    .agg({\n",
    "                        'simulated_num_x': 'sum',\n",
    "                        'simulated_den_n': 'sum',\n",
    "                        'simulated_total_sessions': 'sum',\n",
    "                        'used_global_rate_fallback': 'any'\n",
    "                    })\n",
    "                )\n",
    "                margins = calculate_margin_wilson(\n",
    "                    agg_customers['simulated_num_x'],\n",
    "                    agg_customers['simulated_den_n'],\n",
    "                    z_score\n",
    "                )\n",
    "                agg_customers['margin'] = margins['margin']\n",
    "\n",
    "                # Top N + Rest logic (optional, similar to your _aggregate_top_n_for_table_and_plot)\n",
    "                if len(agg_customers) > top_n:\n",
    "                    top_n_df = agg_customers.sort_values('simulated_total_sessions', ascending=False).head(top_n)\n",
    "                    rest_df = agg_customers.sort_values('simulated_total_sessions', ascending=False).iloc[top_n:]\n",
    "                    rest_row = {\n",
    "                        'customer': 'Rest',\n",
    "                        'simulated_num_x': rest_df['simulated_num_x'].sum(),\n",
    "                        'simulated_den_n': rest_df['simulated_den_n'].sum(),\n",
    "                        'simulated_total_sessions': rest_df['simulated_total_sessions'].sum()\n",
    "                    }\n",
    "                    rest_row['margin'] = calculate_margin_wilson(\n",
    "                        rest_row['simulated_num_x'],\n",
    "                        rest_row['simulated_den_n'],\n",
    "                        z_score\n",
    "                    )['margin']\n",
    "                    agg_customers_plot = pd.concat([top_n_df, pd.DataFrame([rest_row])], ignore_index=True)\n",
    "                else:\n",
    "                    agg_customers_plot = agg_customers\n",
    "\n",
    "                if not agg_customers_plot.empty:\n",
    "                    create_bar_chart(\n",
    "                        df=agg_customers_plot, x_col_or_index_values='customer', y_cols=['margin'],\n",
    "                        title=f'Error Margins for Top {top_n} Customers (Rest Aggregated) in Optimal Simulation ({optimal_strategy_name})',\n",
    "                        y_axis_title='Error Margin', x_axis_title='Customer',\n",
    "                        line_data={\n",
    "                            'y_value': ABSOLUTE_ERROR_MARGIN_VALUE,\n",
    "                            'name': f'Target {ABSOLUTE_ERROR_MARGIN_VALUE:.2f}',\n",
    "                            'color': \"Red\", 'width': 2, 'dash': \"dash\"\n",
    "                        },\n",
    "                        filename_suffix=\"optimal_sim_customers_margin\",\n",
    "                        save_plots_to_html=SAVE_PLOTS_TO_HTML\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"No customer data available for plotting in optimal simulation.\")\n",
    "            else:\n",
    "                warnings.warn(\"Customer dimension not found in optimal simulation DataFrame for plotting.\")\n",
    "\n",
    "            if 'primary_product' in optimal_sim_df_for_plots.columns:\n",
    "                agg_products = (\n",
    "                    optimal_sim_df_for_plots\n",
    "                    .groupby('primary_product', as_index=False)\n",
    "                    .agg({\n",
    "                        'simulated_num_x': 'sum',\n",
    "                        'simulated_den_n': 'sum',\n",
    "                        'simulated_total_sessions': 'sum'\n",
    "                    })\n",
    "                )\n",
    "                margins = calculate_margin_wilson(\n",
    "                    agg_products['simulated_num_x'],\n",
    "                    agg_products['simulated_den_n'],\n",
    "                    z_score\n",
    "                )\n",
    "                agg_products['margin'] = margins['margin']\n",
    "\n",
    "                if len(agg_products) > top_n:\n",
    "                    top_n_df = agg_products.sort_values('simulated_total_sessions', ascending=False).head(top_n)\n",
    "                    rest_df = agg_products.sort_values('simulated_total_sessions', ascending=False).iloc[top_n:]\n",
    "                    rest_row = {\n",
    "                        'primary_product': 'Rest',\n",
    "                        'simulated_num_x': rest_df['simulated_num_x'].sum(),\n",
    "                        'simulated_den_n': rest_df['simulated_den_n'].sum(),\n",
    "                        'simulated_total_sessions': rest_df['simulated_total_sessions'].sum()\n",
    "                    }\n",
    "                    rest_row['margin'] = calculate_margin_wilson(\n",
    "                        rest_row['simulated_num_x'],\n",
    "                        rest_row['simulated_den_n'],\n",
    "                        z_score\n",
    "                    )['margin']\n",
    "                    agg_products_plot = pd.concat([top_n_df, pd.DataFrame([rest_row])], ignore_index=True)\n",
    "                else:\n",
    "                    agg_products_plot = agg_products\n",
    "\n",
    "                if not agg_products_plot.empty:\n",
    "                    create_bar_chart(\n",
    "                        df=agg_products_plot, x_col_or_index_values='primary_product', y_cols=['margin'],\n",
    "                        title=f'Error Margins for Top {top_n} Primary Products (Rest Aggregated) in Optimal Simulation ({optimal_strategy_name})',\n",
    "                        y_axis_title='Error Margin', x_axis_title='Primary Product',\n",
    "                        line_data={\n",
    "                            'y_value': ABSOLUTE_ERROR_MARGIN_VALUE,\n",
    "                            'name': f'Target {ABSOLUTE_ERROR_MARGIN_VALUE:.2f}',\n",
    "                            'color': \"Red\", 'width': 2, 'dash': \"dash\"\n",
    "                        },\n",
    "                        filename_suffix=\"optimal_sim_products_margin\",\n",
    "                        save_plots_to_html=SAVE_PLOTS_TO_HTML\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"No primary product data available for plotting in optimal simulation.\")\n",
    "            else:\n",
    "                warnings.warn(\"Primary Product dimension not found in optimal simulation DataFrame for plotting.\")\n",
    "\n",
    "    return simulation_results\n",
    "\n",
    "\n",
    "# def display_simulation_summary(simulation_results: Dict[str, Any]) -> None:\n",
    "#     \"\"\"\n",
    "#     Displays a combined summary table of all simulation results, including additional metrics.\n",
    "#     \"\"\"\n",
    "#     summary_df = pd.DataFrame.from_dict(simulation_results, orient='index')\n",
    "\n",
    "#     if summary_df.empty:\n",
    "#         print(\"\\nNo simulation results to summarize.\")\n",
    "#         return\n",
    "\n",
    "#     # Calculate improvement if possible\n",
    "#     summary_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "#     if 'overall_margin' in summary_df.columns:\n",
    "#         summary_df['overall_margin_perc'] = summary_df['overall_margin'] * 100\n",
    "\n",
    "#     if 'overall_margin_perc' in summary_df.columns and not summary_df.empty:\n",
    "#         base_margin = summary_df.loc['Current Random 1.0x', 'overall_margin_perc'] if 'Current Random 1.0x' in summary_df.index else np.nan\n",
    "#         if pd.isna(base_margin) or base_margin == 0:\n",
    "#             valid_bases = summary_df['overall_margin_perc'].dropna()\n",
    "#             base_margin = valid_bases.iloc[0] if not valid_bases.empty else 0\n",
    "#         if base_margin > 0:\n",
    "#             summary_df['Improvement (%)'] = (base_margin - summary_df['overall_margin_perc']) / base_margin * 100\n",
    "#         else:\n",
    "#             summary_df['Improvement (%)'] = 0\n",
    "#     else:\n",
    "#         summary_df['Improvement (%)'] = np.nan\n",
    "\n",
    "#     # Rename additional metrics columns for clarity\n",
    "#     summary_df.rename(columns={\n",
    "#         'total_sampled': 'Total QA Sampled',\n",
    "#         'cost_total': 'Total Cost (EUR)',\n",
    "#         'overall_margin_perc': 'Overall Margin (%)',\n",
    "#         'dimension_coverage': 'Selected dimension covered below threshold',\n",
    "#         'customer_coverage': 'Customers below threshold',\n",
    "#         'product_coverage': 'Products below threshold',\n",
    "#         'Improvement (%)': 'Improvement in Error Margin (%)',\n",
    "#         'share_top_3_customers': 'Share of Top 3 Customers in Sample (%)',\n",
    "#         'share_strategic_clients_met_sla': 'Share of Platinum Clients Met SLA (%)',\n",
    "#         'cost_over_coverage': 'Cost per Actionable Unit (EUR)'\n",
    "#     }, inplace=True)\n",
    "\n",
    "#     # Select columns to display (add/remove as needed)\n",
    "#     display_cols = [\n",
    "#         'Total QA Sampled',\n",
    "#         'Total Cost (EUR)',\n",
    "#         'Overall Margin (%)',\n",
    "#         'Selected dimension covered below threshold',\n",
    "#         'Customers below threshold',\n",
    "#         'Products below threshold',\n",
    "#         'Improvement in Error Margin (%)',\n",
    "#         'Share of Top 3 Customers in Sample (%)',\n",
    "#         'Share of Platinum Clients Met SLA (%)',\n",
    "#         'Cost per Actionable Unit (EUR)'\n",
    "#     ]\n",
    "#     existing_display_cols = [col for col in display_cols if col in summary_df.columns]\n",
    "\n",
    "#     print(\"\\n## Simulation Summary Table (Including Additional Metrics)\")\n",
    "#     display(summary_df[existing_display_cols].round(2))\n",
    "\n",
    "def display_simulation_summary(simulation_results: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Displays a combined summary table of all simulation results, including additional metrics.\n",
    "    - Colors improvement in error margin green if positive, red if negative.\n",
    "    - Bolds Overall Margin (%) if below threshold.\n",
    "    - Orders by sum of dimension, customer, product below threshold (descending).\n",
    "    - Removes 'Selected dimension covered below threshold' if reporting dimension is only customer or product.\n",
    "    - Rounds all columns to 0.00.\n",
    "    \"\"\"\n",
    "    summary_df = pd.DataFrame.from_dict(simulation_results, orient='index')\n",
    "\n",
    "    if summary_df.empty:\n",
    "        print(\"\\nNo simulation results to summarize.\")\n",
    "        return\n",
    "\n",
    "    # Calculate improvement if possible\n",
    "    summary_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    if 'overall_margin' in summary_df.columns:\n",
    "        summary_df['overall_margin_perc'] = summary_df['overall_margin'] * 100\n",
    "\n",
    "    if 'overall_margin_perc' in summary_df.columns and not summary_df.empty:\n",
    "        base_margin = summary_df.loc['Current Random 1.0x', 'overall_margin_perc'] if 'Current Random 1.0x' in summary_df.index else np.nan\n",
    "        if pd.isna(base_margin) or base_margin == 0:\n",
    "            valid_bases = summary_df['overall_margin_perc'].dropna()\n",
    "            base_margin = valid_bases.iloc[0] if not valid_bases.empty else 0\n",
    "        if base_margin > 0:\n",
    "            summary_df['Improvement (%)'] = (base_margin - summary_df['overall_margin_perc']) / base_margin * 100\n",
    "        else:\n",
    "            summary_df['Improvement (%)'] = 0\n",
    "    else:\n",
    "        summary_df['Improvement (%)'] = np.nan\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    summary_df.rename(columns={\n",
    "        'total_sampled': 'Total QA Sampled',\n",
    "        'cost_total': 'Total Cost (EUR)',\n",
    "        'overall_margin_perc': 'Overall Margin (%)',\n",
    "        'dimension_coverage': 'Selected dimension covered below threshold',\n",
    "        'customer_coverage': 'Customers below threshold',\n",
    "        'product_coverage': 'Products below threshold',\n",
    "        'industry_coverage': 'Industries below threshold',\n",
    "        'document_country_coverage': 'Document Countries below threshold',\n",
    "        'region_coverage': 'Regions below threshold',\n",
    "        'Improvement (%)': 'Improvement in Error Margin (%)',\n",
    "        'share_top_3_customers': 'Share of Top 3 Customers in Sample (%)',\n",
    "        'share_strategic_clients_met_sla': 'Share of Platinum Clients Met SLA (%)',\n",
    "        'cost_over_coverage': 'Cost per Actionable Unit (EUR)'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Remove 'Selected dimension covered below threshold' if reporting dimension is only customer or product\n",
    "    reporting_dims = globals().get('REPORTING_DIMENSIONS', ['customer'])\n",
    "    if reporting_dims == ['customer'] or reporting_dims == ['product']:\n",
    "        display_cols = [\n",
    "            'Total QA Sampled',\n",
    "            'Total Cost (EUR)',\n",
    "            'Overall Margin (%)',\n",
    "            'Customers below threshold',\n",
    "            'Products below threshold',\n",
    "            'Industries below threshold',\n",
    "            'Document Countries below threshold',\n",
    "            'Regions below threshold',\n",
    "            'Improvement in Error Margin (%)',\n",
    "            'Share of Top 3 Customers in Sample (%)',\n",
    "            'Share of Platinum Clients Met SLA (%)',\n",
    "            'Cost per Actionable Unit (EUR)'\n",
    "        ]\n",
    "    else:\n",
    "        display_cols = [\n",
    "            'Total QA Sampled',\n",
    "            'Total Cost (EUR)',\n",
    "            'Overall Margin (%)',\n",
    "            'Selected dimension covered below threshold',\n",
    "            'Customers below threshold',\n",
    "            'Products below threshold',\n",
    "            'Industries below threshold',\n",
    "            'Document Countries below threshold',\n",
    "            'Regions below threshold',\n",
    "            'Improvement in Error Margin (%)',\n",
    "            'Share of Top 3 Customers in Sample (%)',\n",
    "            'Share of Platinum Clients Met SLA (%)',\n",
    "            'Cost per Actionable Unit (EUR)'\n",
    "        ]\n",
    "\n",
    "    coverage_cols = []\n",
    "    if reporting_dims == ['customer']:\n",
    "        coverage_cols = ['Customers below threshold', 'Products below threshold', 'Industries below threshold', 'Document Countries below threshold', 'Regions below threshold']\n",
    "    elif reporting_dims == ['product']:\n",
    "        coverage_cols = ['Products below threshold', 'Customers below threshold', 'Industries below threshold', 'Document Countries below threshold', 'Regions below threshold']\n",
    "    else:\n",
    "        coverage_cols = [\n",
    "            'Selected dimension covered below threshold',\n",
    "            'Customers below threshold',\n",
    "            'Products below threshold'\n",
    "            'Industries below threshold',\n",
    "            'Document Countries below threshold',\n",
    "            'Regions below threshold'\n",
    "        ]\n",
    "    for col in coverage_cols:\n",
    "        if col not in summary_df.columns:\n",
    "            summary_df[col] = 0\n",
    "    # Only sum each group once\n",
    "    summary_df['total_coverage'] = summary_df[coverage_cols].sum(axis=1)\n",
    "\n",
    "    # Sort by total_coverage descending, then by Improvement in Error Margin ascending\n",
    "    summary_df = summary_df.sort_values(['total_coverage', 'Improvement in Error Margin (%)'], ascending=[False, False])\n",
    "\n",
    "    existing_display_cols = [col for col in display_cols if col in summary_df.columns]\n",
    "\n",
    "    # Threshold for bolding overall margin\n",
    "    threshold = globals().get('TARGET_OVERALL_ERROR_PERCENT', 2.0)\n",
    "\n",
    "    def highlight_improvement(val):\n",
    "        if pd.isna(val):\n",
    "            return ''\n",
    "        color = 'green' if val > 0 else 'red' if val < 0 else ''\n",
    "        return f'color: {color}'\n",
    "\n",
    "    def bold_margin(val):\n",
    "        try:\n",
    "            if float(val) < threshold:\n",
    "                return 'font-weight: bold'\n",
    "        except Exception:\n",
    "            pass\n",
    "        return ''\n",
    "\n",
    "    # Round all columns to 2 decimals\n",
    "    rounded_df = summary_df[existing_display_cols].copy().applymap(lambda x: round(x, 2) if pd.notnull(x) else x)\n",
    "\n",
    "    print(\"\\n## Simulation Summary Table (Including Additional Metrics)\")\n",
    "    styled = (\n",
    "        rounded_df\n",
    "        .style\n",
    "        .format(\"{:.2f}\")\n",
    "        .applymap(highlight_improvement, subset=['Improvement in Error Margin (%)'] if 'Improvement in Error Margin (%)' in rounded_df.columns else [])\n",
    "        .applymap(bold_margin, subset=['Overall Margin (%)'] if 'Overall Margin (%)' in rounded_df.columns else [])\n",
    "    )\n",
    "    display(styled)\n",
    "\n",
    "def plot_customer_coverage_vs_exponent(\n",
    "    population_df: pd.DataFrame,\n",
    "    target_sessions: int,\n",
    "    selected_metric: str,\n",
    "    global_avg_metrics: dict,\n",
    "    overall_avg_frequencies_in_qa: dict,\n",
    "    reporting_dimensions: list,\n",
    "    error_margin_logic: str,\n",
    "    absolute_error_margin_value: float,\n",
    "    relative_error_margin_value: float,\n",
    "    z_score: float,\n",
    "    population_df_original_for_metrics: pd.DataFrame,\n",
    "    save_plots_to_html: bool = False,\n",
    "    scales: list = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs biased simulations for exponents from 0.1 to 1.5 (step 0.1) and multiple sample size scales,\n",
    "    collects number of customers under threshold, and plots the result with max points annotated.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    if scales is None:\n",
    "        scales = [0.5, 1, 2, 3]  # Default scales\n",
    "\n",
    "    exponents = np.arange(0.1, 1.51, 0.1)\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for scale in scales:\n",
    "        scaled_sessions = int(target_sessions * scale)\n",
    "        customer_coverage = []\n",
    "        for exp in exponents:\n",
    "            scenario_results = _run_single_simulation_scenario(\n",
    "                population_df=population_df,\n",
    "                target_sessions=scaled_sessions,\n",
    "                sampling_type='biased',\n",
    "                exponent=exp,\n",
    "                volume_col_for_sampling='volume',\n",
    "                selected_metric=selected_metric,\n",
    "                global_avg_metrics=global_avg_metrics,\n",
    "                overall_avg_frequencies_in_qa=overall_avg_frequencies_in_qa,\n",
    "                reporting_dimensions=['customer'],\n",
    "                error_margin_logic=error_margin_logic,\n",
    "                absolute_error_margin_value=absolute_error_margin_value,\n",
    "                relative_error_margin_value=relative_error_margin_value,\n",
    "                z_score=z_score,\n",
    "                population_df_original_for_metrics=population_df_original_for_metrics\n",
    "            )\n",
    "            customer_coverage.append(scenario_results.get('customer_coverage', 0))\n",
    "\n",
    "        # Plot the curve for this scale\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=exponents, y=customer_coverage, mode='lines+markers',\n",
    "            name=f'Scale {scale}x ({scaled_sessions} sessions)'\n",
    "        ))\n",
    "\n",
    "        # Annotate the maximum point with a star and text label\n",
    "        max_idx = int(np.argmax(customer_coverage))\n",
    "        max_exp = exponents[max_idx]\n",
    "        max_cov = customer_coverage[max_idx]\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[max_exp], y=[max_cov],\n",
    "            mode='markers',\n",
    "            marker=dict(color='red', size=14, symbol='star'),\n",
    "            showlegend=False\n",
    "        ))\n",
    "        fig.add_annotation(\n",
    "            x=max_exp, y=max_cov,\n",
    "            text=f\"★ Bias_exponent: {max_exp:.2f}<br>Max: {max_cov}<br>Scale: {scale}x\",\n",
    "            showarrow=False,\n",
    "            font=dict(color='red', size=12),\n",
    "            xanchor='left',\n",
    "            yanchor='bottom',\n",
    "            xshift=10,\n",
    "            yshift=10\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Number of Customers Under Target Threshold vs Bias Exponent (Multiple Sample Sizes)',\n",
    "        xaxis_title='Bias Exponent',\n",
    "        yaxis_title='Number of Customers Under Threshold',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    if save_plots_to_html:\n",
    "        fig.write_html(\"customer_coverage_vs_exponent.html\")\n",
    "        print(\"Plot saved to customer_coverage_vs_exponent.html\")\n",
    "    fig.show()\n",
    "\n",
    "def plot_customer_volume_distribution_binned(\n",
    "    df: pd.DataFrame,\n",
    "    quantile_threshold: float = 0.2,\n",
    "    bins: int = 20,\n",
    "    save_plots_to_html: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the distribution of customers by total volume, binned into buckets, and highlights quantile thresholds.\n",
    "    Shows selected quantile, 15%, 50%, 75%, and 90%.\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    # Aggregate total volume per customer\n",
    "    customer_volumes = df.groupby('customer')['volume'].sum()\n",
    "\n",
    "    # Ensure bins start at zero (or min value)\n",
    "    min_vol = customer_volumes.min()\n",
    "    max_vol = customer_volumes.max()\n",
    "    bin_start = 0 if min_vol >= 0 else min_vol\n",
    "    bins_arr = np.linspace(bin_start, max_vol, bins + 1)\n",
    "\n",
    "    # Bin the customer volumes\n",
    "    volume_bins = pd.cut(customer_volumes, bins=bins_arr, include_lowest=True)\n",
    "    binned_counts = volume_bins.value_counts().sort_index()\n",
    "    binned_counts = binned_counts[binned_counts > 0]\n",
    "\n",
    "    bin_labels = [str(interval) for interval in binned_counts.index]\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=bin_labels,\n",
    "        y=binned_counts.values,\n",
    "        name='Customer Count per Volume Bin',\n",
    "        marker_color='blue',\n",
    "        opacity=0.7\n",
    "    ))\n",
    "\n",
    "    # Quantiles to plot\n",
    "    quantiles = [\n",
    "        quantile_threshold,\n",
    "        0.15,\n",
    "        0.5,\n",
    "        0.75,\n",
    "        0.9\n",
    "    ]\n",
    "    quantile_colors = ['red', 'orange', 'green', 'purple', 'black']\n",
    "    quantile_names = [\n",
    "        f\"Selected ({int(quantile_threshold*100)}%)\",\n",
    "        \"15%\",\n",
    "        \"50%\",\n",
    "        \"75%\",\n",
    "        \"90%\"\n",
    "    ]\n",
    "\n",
    "    for q, color, name in zip(quantiles, quantile_colors, quantile_names):\n",
    "        q_value = customer_volumes.quantile(q)\n",
    "        q_bin = None\n",
    "        for interval in binned_counts.index:\n",
    "            if interval.left <= q_value <= interval.right:\n",
    "                q_bin = interval\n",
    "                break\n",
    "        if q_bin is not None:\n",
    "            bin_index = bin_labels.index(str(q_bin))\n",
    "            fig.add_vline(\n",
    "                x=bin_index,\n",
    "                line=dict(color=color, width=2, dash='dash'),\n",
    "                annotation_text=f\"{name}: {q_value:.0f}\",\n",
    "                annotation_position=\"top right\"\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Distribution of Customers by Total Volume (Binned)',\n",
    "        xaxis_title='Volume Bin',\n",
    "        yaxis_title='Number of Customers',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    if save_plots_to_html:\n",
    "        fig.write_html(\"customer_volume_distribution_binned.html\")\n",
    "        print(\"Plot saved to customer_volume_distribution_binned.html\")\n",
    "    fig.show()\n",
    "\n",
    "def plot_customer_volume_boxplot(\n",
    "    df: pd.DataFrame,\n",
    "    quantile_threshold: float = 0.2,\n",
    "    save_plots_to_html: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots a boxplot of customer volumes, overlays customer points, and marks selected and key quantiles.\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    # Aggregate total volume per customer\n",
    "    customer_volumes = df.groupby('customer')['volume'].sum()\n",
    "    customers = customer_volumes.index.tolist()\n",
    "    volumes = customer_volumes.values\n",
    "\n",
    "    # Quantiles to plot\n",
    "    quantiles = [\n",
    "        quantile_threshold,\n",
    "        0.15,\n",
    "        0.5,\n",
    "        0.75,\n",
    "        0.9\n",
    "    ]\n",
    "    quantile_colors = ['red', 'orange', 'green', 'purple', 'black']\n",
    "    quantile_names = [\n",
    "        f\"Selected ({int(quantile_threshold*100)}%)\",\n",
    "        \"15%\",\n",
    "        \"50%\",\n",
    "        \"75%\",\n",
    "        \"90%\"\n",
    "    ]\n",
    "    quantile_values = [customer_volumes.quantile(q) for q in quantiles]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Boxplot\n",
    "    fig.add_trace(go.Box(\n",
    "        y=volumes,\n",
    "        boxpoints='outliers',\n",
    "        name='Customer Volume',\n",
    "        marker_color='lightblue',\n",
    "        boxmean=True,\n",
    "        orientation='v'\n",
    "    ))\n",
    "\n",
    "    # Overlay customer points\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=volumes,\n",
    "        x=['']*len(volumes),  # All points on same x for vertical boxplot\n",
    "        mode='markers',\n",
    "        marker=dict(color='blue', size=6, opacity=0.6),\n",
    "        text=customers,\n",
    "        name='Customers'\n",
    "    ))\n",
    "\n",
    "    # Add quantile lines\n",
    "    for q_val, color, name in zip(quantile_values, quantile_colors, quantile_names):\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=-0.4, x1=0.4,\n",
    "            y0=q_val, y1=q_val,\n",
    "            line=dict(color=color, width=2, dash='dash'),\n",
    "        )\n",
    "        fig.add_annotation(\n",
    "            x=0.45, y=q_val,\n",
    "            text=f\"{name}: {q_val:.0f}\",\n",
    "            showarrow=False,\n",
    "            font=dict(color=color, size=10),\n",
    "            yshift=0\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Customer Volume Distribution (Boxplot with Quantiles)',\n",
    "        yaxis_title='Total Volume per Customer',\n",
    "        xaxis=dict(showticklabels=False),\n",
    "        template='plotly_white'\n",
    "    )\n",
    "\n",
    "    if save_plots_to_html:\n",
    "        fig.write_html(\"customer_volume_boxplot.html\")\n",
    "        print(\"Plot saved to customer_volume_boxplot.html\")\n",
    "    fig.show()\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_proportion_volume_vs_qa(population_df: pd.DataFrame, \n",
    "                                reporting_dimensions: list, \n",
    "                                top_n: int = 10, \n",
    "                                save_plots_to_html: bool = False):\n",
    "    \"\"\"\n",
    "    Plots the proportion of sessions in volume and in qa_reviewed per dimension.\n",
    "    \"\"\"\n",
    "    analysis_dimensions = list(set(reporting_dimensions + ['primary_product', 'customer']))\n",
    "\n",
    "    for dim in analysis_dimensions:\n",
    "        if dim not in population_df.columns:\n",
    "            print(f\"Dimension '{dim}' not found in data. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Aggregate sums\n",
    "        agg = population_df.groupby(dim).agg(\n",
    "            volume_sum=('volume', 'sum'),\n",
    "            qa_reviewed_sum=('qa_reviewed', 'sum')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Calculate proportions\n",
    "        total_volume = agg['volume_sum'].sum()\n",
    "        total_qa = agg['qa_reviewed_sum'].sum()\n",
    "        agg['proportion_of_total_volume'] = agg['volume_sum'] / total_volume\n",
    "        agg['proportion_of_qa_sample'] = agg['qa_reviewed_sum'] / total_qa\n",
    "\n",
    "        # Top N + Rest\n",
    "        if len(agg) > top_n:\n",
    "            top_n_df = agg.sort_values('volume_sum', ascending=False).head(top_n)\n",
    "            rest_df = agg.sort_values('volume_sum', ascending=False).iloc[top_n:]\n",
    "            rest_row = {\n",
    "                dim: 'Rest',\n",
    "                'volume_sum': rest_df['volume_sum'].sum(),\n",
    "                'qa_reviewed_sum': rest_df['qa_reviewed_sum'].sum(),\n",
    "            }\n",
    "            rest_row['proportion_of_total_volume'] = rest_row['volume_sum'] / total_volume\n",
    "            rest_row['proportion_of_qa_sample'] = rest_row['qa_reviewed_sum'] / total_qa\n",
    "            plot_df = pd.concat([top_n_df, pd.DataFrame([rest_row])], ignore_index=True)\n",
    "        else:\n",
    "            plot_df = agg.copy()\n",
    "\n",
    "        # Prepare text labels rounded to 0.00\n",
    "        text_labels = {\n",
    "            'proportion_of_total_volume': plot_df['proportion_of_total_volume'].apply(lambda x: f\"{x:.2f}\"),\n",
    "            'proportion_of_qa_sample': plot_df['proportion_of_qa_sample'].apply(lambda x: f\"{x:.2f}\")\n",
    "        }\n",
    "\n",
    "        create_bar_chart(\n",
    "            df=plot_df,\n",
    "            x_col_or_index_values=dim,\n",
    "            y_cols=['proportion_of_total_volume', 'proportion_of_qa_sample'],\n",
    "            title=f'Proportion of Volume vs QA Reviewed by {dim}',\n",
    "            y_axis_title='Proportion',\n",
    "            x_axis_title=dim,\n",
    "            barmode='group',\n",
    "            filename_suffix=f'proportion_volume_vs_qa_{dim}',\n",
    "            save_plots_to_html=save_plots_to_html,\n",
    "            text_labels=text_labels\n",
    "        )\n",
    "\n",
    "def generate_report(df_data: pd.DataFrame, params: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Main function to generate the comprehensive simulation report.\n",
    "    Accepts parameters as a dictionary from the UI.\n",
    "    \"\"\"\n",
    "    # Update global parameters based on UI input\n",
    "    global SELECTED_METRIC, TARGET_OVERALL_ERROR_PERCENT, TARGET_DIMENSION_ERROR_PERCENT, \\\n",
    "           CONFIDENCE_LEVEL, REVIEW_COST_PER_SESSION, PREDEFINED_TOTAL_COST_BUDGET, \\\n",
    "           WEIGHT_FACTOR_SMALL_CUSTOMERS, SMALL_CUSTOMER_VOLUME_QUANTILE_THRESHOLD, \\\n",
    "           ERROR_MARGIN_LOGIC, ABSOLUTE_ERROR_MARGIN_VALUE, RELATIVE_ERROR_MARGIN_VALUE, \\\n",
    "           MIN_ABSOLUTE_ERROR_MARGIN, REPORTING_DIMENSIONS, STRATIFY_BY_DIMENSION, TOP_N, Z_SCORE, \\\n",
    "           SAVE_PLOTS_TO_HTML, MIN_PER_STRATUM, TARGET_QA_PROPORTIONS\n",
    "    \n",
    "    if 'qa_proportions' in params:\n",
    "        TARGET_QA_PROPORTIONS = params['qa_proportions']\n",
    "\n",
    "    MIN_PER_STRATUM = params['min_per_stratum']\n",
    "    SELECTED_METRIC = params['selected_metric']\n",
    "    TARGET_OVERALL_ERROR_PERCENT = params['target_overall_error_percent']\n",
    "    TARGET_DIMENSION_ERROR_PERCENT = params['target_dimension_error_percent']\n",
    "    CONFIDENCE_LEVEL = params['confidence_level']\n",
    "    REVIEW_COST_PER_SESSION = params['review_cost_per_session']\n",
    "    PREDEFINED_TOTAL_COST_BUDGET = params['predefined_total_cost_budget']\n",
    "    WEIGHT_FACTOR_SMALL_CUSTOMERS = params['weight_factor_small_customers']\n",
    "    SMALL_CUSTOMER_VOLUME_QUANTILE_THRESHOLD = params['small_customer_volume_quantile_threshold']\n",
    "    ERROR_MARGIN_LOGIC = params['error_margin_logic']\n",
    "    ABSOLUTE_ERROR_MARGIN_VALUE = params['absolute_error_margin_value']\n",
    "    RELATIVE_ERROR_MARGIN_VALUE = params['relative_error_margin_value']\n",
    "    MIN_ABSOLUTE_ERROR_MARGIN = params['min_absolute_error_margin']\n",
    "    REPORTING_DIMENSIONS = params['reporting_dimensions']\n",
    "    STRATIFY_BY_DIMENSION = params['stratify_by_dimension']\n",
    "    TOP_N = params['top_n']\n",
    "    SAVE_PLOTS_TO_HTML = params['save_plots_to_html']\n",
    "    \n",
    "    # Recalculate Z_SCORE based on potentially new CONFIDENCE_LEVEL\n",
    "    Z_SCORE = stats.norm.ppf(1 - (1 - CONFIDENCE_LEVEL) / 2)\n",
    "\n",
    "    print(\"Starting QA Sampling Strategy Report Generation...\")\n",
    "\n",
    "    print(\"NOTE: Simulation scenarios are for strategy comparison only and do not produce unbiased population estimates unless weights are applied.\")\n",
    "    \n",
    "    # Calculate total QA reviewed sessions\n",
    "    # If daily_reviews and days are provided, calculate total_qa_reviewed\n",
    "    # Otherwise, fallback to sum of 'qa_reviewed' column in df_data\n",
    "\n",
    "    daily_reviews = params.get('daily_reviews', None)\n",
    "    days = params.get('days', None)\n",
    "\n",
    "    qa_total_source = params.get('qa_total_source', 'Use Current QA Reviewed')\n",
    "\n",
    "    # --- SCALE DATA IF USER SELECTS FEWER DAYS THAN DATA COVERS ---\n",
    "    # Assume your data covers 30 days by default (adjust if needed)\n",
    "\n",
    "    # Get unique days_count from the DataFrame\n",
    "    if 'days_count' in df_data.columns:\n",
    "        default_days = int(df_data['days_count'].iloc[0])\n",
    "    else:\n",
    "        default_days = 30  # fallback if not present\n",
    "\n",
    "    data_days = default_days\n",
    "\n",
    "    if days is not None and days < data_days:\n",
    "        scale_factor = days / data_days\n",
    "        cols_to_scale = [\n",
    "            'volume', 'qa_reviewed', 'qa_reviewed_for_reporting',\n",
    "            'decision_error_sessions', 'extraction_error_sessions',\n",
    "            'false_approves', 'false_declines', 'missed_fraud_sessions',\n",
    "            'declinable_sessions', 'approvable_sessions', 'fraud_sessions',\n",
    "            'full_auto_sessions', 'declined_sessions', 'approved_sessions',\n",
    "            'declined_due_to_fraud_sessions', 'approved_due_to_fraud_sessions'\n",
    "        ]\n",
    "        for col in cols_to_scale:\n",
    "            if col in df_data.columns:\n",
    "               df_data[col] = (df_data[col] * scale_factor).round().fillna(0).astype(int)\n",
    "        print(f\"[INFO] Scaled all relevant columns by {scale_factor:.2f} to match selected {days} days.\")\n",
    "\n",
    "    if qa_total_source == 'Input Daily Reviews * Days' and daily_reviews is not None and days is not None:\n",
    "        total_qa_reviewed = daily_reviews * days\n",
    "        print(f\"\\n[INFO] Using QA total from widget input: daily_reviews ({daily_reviews}) * days ({days}) = {total_qa_reviewed}\")\n",
    "    else:\n",
    "        total_qa_reviewed = df_data['qa_reviewed'].sum()\n",
    "        print(f\"\\n[INFO] Using QA total from current data: sum(df_data['qa_reviewed']) = {total_qa_reviewed}\")\n",
    "\n",
    "\n",
    "    # Data Preparation\n",
    "    population_df, global_rates, global_freq = prepare_population_data(df_data, SELECTED_METRIC, METRIC_PROPERTIES, Z_SCORE)\n",
    "    \n",
    "    total_qa = total_qa_reviewed #population_df['qa_reviewed'].sum()\n",
    "    population_df = assign_true_random_sample(population_df, int(total_qa))\n",
    "\n",
    "    # If widget is set to use Input Daily Reviews * Days, use true_random_qa_reviewed for reporting\n",
    "    if qa_total_source == 'Input Daily Reviews * Days' and 'true_random_qa_reviewed' in population_df.columns:\n",
    "        # For reporting, overwrite 'qa_reviewed' with 'true_random_qa_reviewed'\n",
    "        population_df['qa_reviewed_for_reporting'] = population_df['true_random_qa_reviewed']\n",
    "    else:\n",
    "        population_df['qa_reviewed_for_reporting'] = population_df['qa_reviewed']\n",
    "    \n",
    "    # Display Current State Analysis\n",
    "    display_current_state_analysis(population_df, SELECTED_METRIC, METRIC_PROPERTIES,\n",
    "                                   REVIEW_COST_PER_SESSION, TARGET_OVERALL_ERROR_PERCENT, Z_SCORE,\n",
    "                                   SAVE_PLOTS_TO_HTML)\n",
    "    \n",
    "    display_metric_with_error_bars(\n",
    "        population_df, SELECTED_METRIC, METRIC_PROPERTIES,\n",
    "        REPORTING_DIMENSIONS, TOP_N, global_rates, Z_SCORE, SAVE_PLOTS_TO_HTML\n",
    "    )\n",
    "\n",
    "    plot_proportion_volume_vs_qa(\n",
    "        population_df,\n",
    "        REPORTING_DIMENSIONS,\n",
    "        top_n=10,\n",
    "        save_plots_to_html=SAVE_PLOTS_TO_HTML\n",
    "    )\n",
    "    \n",
    "    # # Display Current Dimension Metrics and Margins\n",
    "    # display_current_dimension_metrics_and_margins(population_df, SELECTED_METRIC, METRIC_PROPERTIES,\n",
    "    #                                               REPORTING_DIMENSIONS, TOP_N, TARGET_OVERALL_ERROR_PERCENT,\n",
    "    #                                               global_rates, Z_SCORE, SAVE_PLOTS_TO_HTML)\n",
    "\n",
    "    # Display Sample Size Calculations\n",
    "    display_sample_size_calculations(population_df, SELECTED_METRIC, METRIC_PROPERTIES,\n",
    "                                     REPORTING_DIMENSIONS, ERROR_MARGIN_LOGIC, ABSOLUTE_ERROR_MARGIN_VALUE,\n",
    "                                     RELATIVE_ERROR_MARGIN_VALUE, MIN_ABSOLUTE_ERROR_MARGIN,\n",
    "                                     TOP_N,\n",
    "                                     global_rates, global_freq,\n",
    "                                     CONFIDENCE_LEVEL, Z_SCORE, SAVE_PLOTS_TO_HTML)\n",
    "\n",
    "    # Run Simulations\n",
    "    simulation_results = run_simulations(\n",
    "        population_df = population_df, \n",
    "        global_avg_metrics = global_rates, \n",
    "        overall_avg_frequencies_in_qa = global_freq, \n",
    "        selected_metric = SELECTED_METRIC, \n",
    "        reporting_dimensions = REPORTING_DIMENSIONS,\n",
    "        error_margin_logic = ERROR_MARGIN_LOGIC, \n",
    "        absolute_error_margin_value = ABSOLUTE_ERROR_MARGIN_VALUE, \n",
    "        relative_error_margin_value = RELATIVE_ERROR_MARGIN_VALUE, \n",
    "        min_absolute_error_margin = MIN_ABSOLUTE_ERROR_MARGIN,\n",
    "        target_overall_error_percent = TARGET_OVERALL_ERROR_PERCENT, \n",
    "        target_dimension_error_percent = TARGET_DIMENSION_ERROR_PERCENT,\n",
    "        weight_factor_small_customers = WEIGHT_FACTOR_SMALL_CUSTOMERS,\n",
    "        small_customer_volume_quantile_threshold = SMALL_CUSTOMER_VOLUME_QUANTILE_THRESHOLD, \n",
    "        stratify_by_dimension = STRATIFY_BY_DIMENSION,\n",
    "        sampling_types_to_run = params['sampling_types_to_run'], \n",
    "        top_n = TOP_N, \n",
    "        z_score = Z_SCORE,\n",
    "        min_per_stratum=MIN_PER_STRATUM\n",
    "    )\n",
    "    \n",
    "    # Display Simulation Summary\n",
    "    display_simulation_summary(simulation_results)\n",
    "\n",
    "    # Display Additional Metrics Summary\n",
    "    #display_additional_metrics_summary(simulation_results)\n",
    "\n",
    "    # Example usage (add to your report function where appropriate):\n",
    "    plot_customer_coverage_vs_exponent(\n",
    "        population_df=population_df,\n",
    "        target_sessions=int(population_df['qa_reviewed'].sum()),\n",
    "        selected_metric=SELECTED_METRIC,\n",
    "        global_avg_metrics=global_rates,\n",
    "        overall_avg_frequencies_in_qa=global_freq,\n",
    "        reporting_dimensions=REPORTING_DIMENSIONS,\n",
    "        error_margin_logic=ERROR_MARGIN_LOGIC,\n",
    "        absolute_error_margin_value=ABSOLUTE_ERROR_MARGIN_VALUE,\n",
    "        relative_error_margin_value=RELATIVE_ERROR_MARGIN_VALUE,\n",
    "        z_score=Z_SCORE,\n",
    "        population_df_original_for_metrics=population_df,\n",
    "        save_plots_to_html=SAVE_PLOTS_TO_HTML\n",
    "    )\n",
    "\n",
    "    # --- Save simulation summary to CSV ---\n",
    "    summary_df = pd.DataFrame.from_dict(simulation_results, orient='index')\n",
    "    summary_df.to_csv('simulation_summary.csv')\n",
    "    print(\"Simulation summary saved to simulation_summary.csv\")\n",
    "    # # Example usage (add to your report function where appropriate):\n",
    "    # plot_customer_volume_distribution_binned(df, quantile_threshold=SMALL_CUSTOMER_VOLUME_QUANTILE_THRESHOLD, bins=10, save_plots_to_html=SAVE_PLOTS_TO_HTML)\n",
    "\n",
    "    # # Example usage (add to your report function where appropriate):\n",
    "    # plot_customer_volume_boxplot(df, quantile_threshold=SMALL_CUSTOMER_VOLUME_QUANTILE_THRESHOLD, save_plots_to_html=SAVE_PLOTS_TO_HTML)\n",
    "\n",
    "    # print_simulation_summaries(\n",
    "    # simulation_results,\n",
    "    # ERROR_MARGIN_LOGIC,\n",
    "    # ABSOLUTE_ERROR_MARGIN_VALUE,\n",
    "    # RELATIVE_ERROR_MARGIN_VALUE\n",
    "    # )\n",
    "    # print(\"\\nQA Sampling Strategy Report Generation Completed.\")\n",
    "\n",
    "\n",
    "# def find_optimal_relative_error_margin(\n",
    "#     df_data,\n",
    "#     params,\n",
    "#     margin_range=(0.2, 0.5),\n",
    "#     step=0.1,\n",
    "#     verbose=True\n",
    "# ):\n",
    "#     import pandas as pd\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import sys\n",
    "#     import contextlib\n",
    "#     import io\n",
    "\n",
    "#     results = []\n",
    "#     for rel_margin in np.arange(margin_range[0], margin_range[1] + step, step):\n",
    "#         test_params = params.copy()\n",
    "#         test_params['error_margin_logic'] = 'Relative Error Margin'\n",
    "#         test_params['relative_error_margin_value'] = rel_margin\n",
    "#         test_params['absolute_error_margin_value'] = 0.001\n",
    "\n",
    "#         filtered_df = df_data\n",
    "#         if 'primary_product' in test_params:\n",
    "#             filtered_df = filtered_df[filtered_df['primary_product'].isin(test_params['primary_product'])]\n",
    "\n",
    "#         population_df, global_rates, global_freq = prepare_population_data(\n",
    "#             filtered_df, test_params['selected_metric'], METRIC_PROPERTIES, Z_SCORE\n",
    "#         )\n",
    "#         total_qa = population_df['qa_reviewed'].sum()\n",
    "#         population_df = assign_true_random_sample(population_df, int(total_qa))\n",
    "\n",
    "#         if test_params.get('qa_total_source', 'Use Current QA Reviewed') == 'Input Daily Reviews * Days' and 'true_random_qa_reviewed' in population_df.columns:\n",
    "#             population_df['qa_reviewed_for_reporting'] = population_df['true_random_qa_reviewed']\n",
    "#         else:\n",
    "#             population_df['qa_reviewed_for_reporting'] = population_df['qa_reviewed']\n",
    "\n",
    "#         with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n",
    "#             simulation_results = run_simulations(\n",
    "#                 population_df=population_df,\n",
    "#                 global_avg_metrics=global_rates,\n",
    "#                 overall_avg_frequencies_in_qa=global_freq,\n",
    "#                 selected_metric=test_params['selected_metric'],\n",
    "#                 reporting_dimensions=test_params['reporting_dimensions'],\n",
    "#                 error_margin_logic=test_params['error_margin_logic'],\n",
    "#                 absolute_error_margin_value=test_params['absolute_error_margin_value'],\n",
    "#                 relative_error_margin_value=test_params['relative_error_margin_value'],\n",
    "#                 min_absolute_error_margin=test_params['min_absolute_error_margin'],\n",
    "#                 target_overall_error_percent=test_params['target_overall_error_percent'],\n",
    "#                 target_dimension_error_percent=test_params['target_dimension_error_percent'],\n",
    "#                 weight_factor_small_customers=test_params['weight_factor_small_customers'],\n",
    "#                 small_customer_volume_quantile_threshold=test_params['small_customer_volume_quantile_threshold'],\n",
    "#                 stratify_by_dimension=test_params['stratify_by_dimension'],\n",
    "#                 sampling_types_to_run=test_params['sampling_types_to_run'],\n",
    "#                 top_n=test_params['top_n'],\n",
    "#                 z_score=Z_SCORE\n",
    "#             )\n",
    "#         summary_df = pd.DataFrame.from_dict(simulation_results, orient='index')\n",
    "\n",
    "#         # --- Patch: Avoid NaN for min cost per unit ---\n",
    "#         if 'Cost per Actionable Unit (EUR)' in summary_df.columns:\n",
    "#             min_cost = summary_df['Cost per Actionable Unit (EUR)'].min()\n",
    "#             if pd.isna(min_cost) or min_cost == 0:\n",
    "#                 min_cost = summary_df['Total Cost (EUR)'].iloc[0] if 'Total Cost (EUR)' in summary_df.columns else np.nan\n",
    "#         else:\n",
    "#             min_cost = summary_df['Total Cost (EUR)'].iloc[0] if 'Total Cost (EUR)' in summary_df.columns else np.nan\n",
    "#         # ------------------------------------------------\n",
    "\n",
    "#         main_row = summary_df.iloc[0] if not summary_df.empty else pd.Series()\n",
    "#         # Try to find a scenario with coverage > 0\n",
    "#         best_row = None\n",
    "#         for idx, row in summary_df.iterrows():\n",
    "#             cust_cov = row.get('customer_coverage', 0)\n",
    "#             prod_cov = row.get('product_coverage', 0)\n",
    "#             if (pd.notnull(cust_cov) and cust_cov > 0) or (pd.notnull(prod_cov) and prod_cov > 0):\n",
    "#                 best_row = row\n",
    "#                 break\n",
    "#         if best_row is not None:\n",
    "#             main_row = best_row\n",
    "\n",
    "#         results.append({\n",
    "#             'Scenario': main_row.name if not summary_df.empty else '',\n",
    "#             'Relative Error Margin': rel_margin,\n",
    "#             'Min Cost per Unit': min_cost,\n",
    "#             'Total QA Sampled': main_row.get('total_sampled', np.nan),\n",
    "#             'Overall Margin (%)': main_row.get('overall_margin', np.nan) * 100 if not pd.isna(main_row.get('overall_margin', np.nan)) else np.nan,\n",
    "#             'Customers below threshold': main_row.get('customer_coverage', np.nan),\n",
    "#             'Products below threshold': main_row.get('product_coverage', np.nan),\n",
    "#         })\n",
    "#         if verbose:\n",
    "#             print(f\"Tested margin {rel_margin:.3f}: min cost per unit = {min_cost:.2f}\")\n",
    "\n",
    "#     # Prefer results with coverage > 0\n",
    "#     results_with_coverage = [r for r in results if (pd.notnull(r['Customers below threshold']) and r['Customers below threshold'] > 0) or (pd.notnull(r['Products below threshold']) and r['Products below threshold'] > 0)]\n",
    "#     if results_with_coverage:\n",
    "#         best = min(results_with_coverage, key=lambda x: x['Min Cost per Unit'] if pd.notnull(x['Min Cost per Unit']) else np.inf)\n",
    "#     else:\n",
    "#         best = min(results, key=lambda x: x['Min Cost per Unit'] if pd.notnull(x['Min Cost per Unit']) else np.inf)\n",
    "\n",
    "#     print(f\"\\nOptimal relative error margin: {best['Relative Error Margin']:.3f} (min cost per unit: {best['Min Cost per Unit']:.2f})\")\n",
    "\n",
    "#     summary_table = pd.DataFrame(results)\n",
    "#     def highlight_optimal(row):\n",
    "#         return ['background-color: lightgreen' if row['Relative Error Margin'] == best['Relative Error Margin'] else '' for _ in row]\n",
    "\n",
    "#     display(summary_table.style.apply(highlight_optimal, axis=1).format(precision=2))\n",
    "\n",
    "#     plt.plot(summary_table['Relative Error Margin'], summary_table['Min Cost per Unit'], marker='o')\n",
    "#     plt.xlabel('Relative Error Margin')\n",
    "#     plt.ylabel('Min Cost per Actionable Unit (EUR)')\n",
    "#     plt.title('Cost per Unit vs. Relative Error Margin')\n",
    "#     plt.show()\n",
    "\n",
    "#     return best\n",
    "\n",
    "# def print_simulation_summaries(simulation_results, error_margin_logic, abs_margin_value, rel_margin_value):\n",
    "#     for sim_name, res in simulation_results.items():\n",
    "#         print(f\"\\n## Simulation Summary: {sim_name}\")\n",
    "#         # Determine threshold logic and value\n",
    "#         if error_margin_logic == 'Absolute Error Margin':\n",
    "#             threshold_type = \"Absolute Error Margin\"\n",
    "#             threshold_value = abs_margin_value * 100\n",
    "#             threshold_unit = \"%\"\n",
    "#         else:\n",
    "#             threshold_type = \"Relative Error Margin\"\n",
    "#             threshold_value = rel_margin_value * 100\n",
    "#             threshold_unit = \"% of group rate\"\n",
    "#         print(f\"Threshold logic used: {threshold_type} (Threshold: {threshold_value:.2f}{threshold_unit})\")\n",
    "#         print(f\"Sample size: {res.get('total_sampled', 'N/A')} sessions (Strategy: {sim_name})\")\n",
    "#         print(f\"  - Overall error margin for this strategy: {res.get('overall_margin', float('nan'))*100:.2f}%\")\n",
    "#         print(f\"  - Covers {res.get('customer_coverage', 0)} customers below threshold.\")\n",
    "#         print(f\"  - Customers meeting target: {res.get('customer_list', '')}\")\n",
    "#         print(f\"  - Covers {res.get('product_coverage', 0)} products below threshold.\")\n",
    "#         print(f\"  - Products meeting target: {res.get('product_list', '')}\")\n",
    "#         print(f\"  - Covers {res.get('industry_coverage', 0)} industries below threshold.\")\n",
    "#         print(f\"  - Industries meeting target: {res.get('industry_list', '')}\")\n",
    "#         print(f\"  - Covers {res.get('document_country_coverage', 0)} document countries below threshold.\")\n",
    "#         print(f\"  - Document countries meeting target: {res.get('document_country_list', '')}\")\n",
    "#         print(f\"  - Covers {res.get('region_coverage', 0)} regions below threshold.\")\n",
    "#         print(f\"  - Regions meeting target: {res.get('region_list', '')}\")\n",
    "#         print(f\"  - Share of top 3 customers: {res.get('share_top_3_customers', 0):.2f}%\")\n",
    "#         print(f\"  - Share of strategic clients meeting SLA: {res.get('share_strategic_clients_met_sla', 0):.2f}%\")\n",
    "#         print(f\"  - Total Cost for this strategy: ${res.get('cost_total', 0):.2f} EUR\")\n",
    "\n",
    "def print_simulation_summaries(simulation_results, error_margin_logic, abs_margin_value, rel_margin_value):\n",
    "    \"\"\"\n",
    "    Display simulation summaries as a table instead of printing line by line.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from IPython.display import display\n",
    "\n",
    "    rows = []\n",
    "    for sim_name, res in simulation_results.items():\n",
    "        if error_margin_logic == 'Absolute Error Margin':\n",
    "            threshold_type = \"Absolute Error Margin\"\n",
    "            threshold_value = abs_margin_value * 100\n",
    "            threshold_unit = \"%\"\n",
    "        else:\n",
    "            threshold_type = \"Relative Error Margin\"\n",
    "            threshold_value = rel_margin_value * 100\n",
    "            threshold_unit = \"% of group rate\"\n",
    "        rows.append({\n",
    "            \"Simulation\": sim_name,\n",
    "            \"Threshold Logic\": f\"{threshold_type} ({threshold_value:.2f}{threshold_unit})\",\n",
    "            \"Sample Size\": res.get('total_sampled', 'N/A'),\n",
    "            \"Overall Error Margin (%)\": f\"{res.get('overall_margin', float('nan'))*100:.2f}\",\n",
    "            \"Customers < Threshold\": res.get('customer_coverage', 0),\n",
    "            \"Customers Meeting Target\": res.get('customer_list', ''),\n",
    "            \"Products < Threshold\": res.get('product_coverage', 0),\n",
    "            \"Products Meeting Target\": res.get('product_list', ''),\n",
    "            \"Industries < Threshold\": res.get('industry_coverage', 0),\n",
    "            \"Industries Meeting Target\": res.get('industry_list', ''),\n",
    "            \"Doc Countries < Threshold\": res.get('document_country_coverage', 0),\n",
    "            \"Doc Countries Meeting Target\": res.get('document_country_list', ''),\n",
    "            \"Regions < Threshold\": res.get('region_coverage', 0),\n",
    "            \"Regions Meeting Target\": res.get('region_list', ''),\n",
    "            \"Share Top 3 Cust. (%)\": f\"{res.get('share_top_3_customers', 0):.2f}\",\n",
    "            \"Share Strat. Clients SLA (%)\": f\"{res.get('share_strategic_clients_met_sla', 0):.2f}\",\n",
    "            \"Total Cost (EUR)\": f\"{res.get('cost_total', 0):.2f}\",\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    display(df)\n",
    "\n",
    "\n",
    "def _run_single_hyperparam_sim(args):\n",
    "    import copy\n",
    "    import numpy as np\n",
    "\n",
    "    df_data, base_params, param_set = args\n",
    "    params = copy.deepcopy(base_params)\n",
    "    params.update(param_set)\n",
    "\n",
    "    # 1. Filter by primary_product if needed\n",
    "    filtered_df = df_data\n",
    "    if 'primary_product' in params and isinstance(params['primary_product'], list):\n",
    "        filtered_df = filtered_df[filtered_df['primary_product'].isin(params['primary_product'])]\n",
    "\n",
    "    # 2. Scale data if days < data_days (as in generate_report)\n",
    "    if 'days_count' in filtered_df.columns:\n",
    "        default_days = int(filtered_df['days_count'].iloc[0])\n",
    "    else:\n",
    "        default_days = 30\n",
    "    data_days = default_days\n",
    "    days = params.get('days', data_days)\n",
    "    if days < data_days:\n",
    "        scale_factor = days / data_days\n",
    "        cols_to_scale = [\n",
    "            'volume', 'qa_reviewed', 'qa_reviewed_for_reporting',\n",
    "            'decision_error_sessions', 'extraction_error_sessions',\n",
    "            'false_approves', 'false_declines', 'missed_fraud_sessions',\n",
    "            'declinable_sessions', 'approvable_sessions', 'fraud_sessions',\n",
    "            'full_auto_sessions', 'declined_sessions', 'approved_sessions',\n",
    "            'declined_due_to_fraud_sessions', 'approved_due_to_fraud_sessions'\n",
    "        ]\n",
    "        for col in cols_to_scale:\n",
    "            if col in filtered_df.columns:\n",
    "                filtered_df[col] = (filtered_df[col] * scale_factor).round().fillna(0).astype(int)\n",
    "\n",
    "    # 3. Calculate total QA reviewed\n",
    "    qa_total_source = params.get('qa_total_source', 'Use Current QA Reviewed')\n",
    "    daily_reviews = params.get('daily_reviews', None)\n",
    "    days = params.get('days', data_days)\n",
    "    total_qa_reviewed = daily_reviews * days\n",
    "\n",
    "    # 4. Prepare population data (aggregation, rates, etc.)\n",
    "    prepare_population_data = globals().get('prepare_population_data')\n",
    "    selected_metric = params.get('selected_metric', 'Missed Fraud Rate')\n",
    "    metric_properties = globals().get('METRIC_PROPERTIES', {})\n",
    "    reporting_dimensions = params.get('reporting_dimensions', ['customer'])\n",
    "\n",
    "    # Only add 'customer_priority' if running Client-Tier Weighting scenario\n",
    "    sampling_types = params.get('sampling_types_to_run', ['Current Random'])\n",
    "    needs_priority = 'Client-Tier Weighting' in sampling_types\n",
    "    if needs_priority and 'customer_priority' not in reporting_dimensions and 'customer_priority' in filtered_df.columns:\n",
    "        reporting_dimensions = list(reporting_dimensions) + ['customer_priority']\n",
    "\n",
    "    z_score = params.get('z_score', 1.96)\n",
    "    population_df, global_avg_metrics, overall_avg_frequencies_in_qa = prepare_population_data(\n",
    "        filtered_df, selected_metric, metric_properties, z_score, reporting_dimensions\n",
    "    )\n",
    "\n",
    "    # Defensive: If population_df is empty, return dummy row\n",
    "    if population_df.empty or population_df['volume'].sum() == 0:\n",
    "        return [{\n",
    "            **param_set,\n",
    "            'scenario': 'No Data',\n",
    "            'cost_total': np.nan,\n",
    "            'overall_margin': np.nan,\n",
    "            'share_top_3_customers': np.nan,\n",
    "            'customer_coverage': 0,\n",
    "            'product_coverage': 0,\n",
    "            'industry_coverage': 0,\n",
    "            'document_country_coverage': 0,\n",
    "            'region_coverage': 0\n",
    "        }]\n",
    "\n",
    "    # 5. Assign true random sample if needed\n",
    "    assign_true_random_sample = globals().get('assign_true_random_sample')\n",
    "    if assign_true_random_sample is not None:\n",
    "        population_df = assign_true_random_sample(population_df, int(total_qa_reviewed))\n",
    "\n",
    "    # 6. Set 'qa_reviewed_for_reporting'\n",
    "    population_df['qa_reviewed_for_reporting'] = population_df['true_random_qa_reviewed']\n",
    "\n",
    "    # 7. Run simulations as usual, but force each scenario to run only ONCE (no scales)\n",
    "    # Patch: For each scenario, run with current total QA reviewed only\n",
    "    simulation_results = {}\n",
    "    for scenario in sampling_types:\n",
    "        scenario_results = run_simulations(\n",
    "            population_df=population_df,\n",
    "            global_avg_metrics=global_avg_metrics,\n",
    "            overall_avg_frequencies_in_qa=overall_avg_frequencies_in_qa,\n",
    "            selected_metric=selected_metric,\n",
    "            reporting_dimensions=reporting_dimensions,\n",
    "            error_margin_logic=params.get('error_margin_logic', 'Absolute Error Margin'),\n",
    "            absolute_error_margin_value=params.get('absolute_error_margin_value', 0.02),\n",
    "            relative_error_margin_value=params.get('relative_error_margin_value', 0.2),\n",
    "            min_absolute_error_margin=params.get('min_absolute_error_margin', 0.005),\n",
    "            target_overall_error_percent=params.get('target_overall_error_percent', 2.0),\n",
    "            target_dimension_error_percent=params.get('target_dimension_error_percent', 2.0),\n",
    "            weight_factor_small_customers=params.get('weight_factor_small_customers', 10),\n",
    "            small_customer_volume_quantile_threshold=params.get('small_customer_volume_quantile_threshold', 0.99),\n",
    "            stratify_by_dimension=params.get('stratify_by_dimension', ['primary_product', 'industry', 'region']),\n",
    "            sampling_types_to_run=[scenario],  # Only run this scenario\n",
    "            top_n=params.get('top_n', 10),\n",
    "            z_score=z_score,\n",
    "            min_per_stratum=params.get('min_per_stratum', 500),\n",
    "            verbose=False\n",
    "        )\n",
    "        # Only keep the first result for this scenario\n",
    "        for scenario, res in scenario_results.items():\n",
    "            simulation_results[scenario] = res\n",
    "\n",
    "    rows = []\n",
    "    for scenario, res in simulation_results.items():\n",
    "        row = dict(param_set)\n",
    "        row['scenario'] = scenario\n",
    "        row.update(res)\n",
    "        for col in ['cost_total', 'overall_margin', 'share_top_3_customers']:\n",
    "            if col not in row:\n",
    "                row[col] = np.nan\n",
    "        rows.append(row)\n",
    "\n",
    "    # If all coverage columns are zero, print debug info\n",
    "    if all(row.get(cov, 0) == 0 for cov in ['customer_coverage', 'product_coverage', 'industry_coverage', 'document_country_coverage', 'region_coverage']):\n",
    "        print(\"[DEBUG] No coverage found for params:\", param_set)\n",
    "        print(\"[DEBUG] reporting_dimensions:\", reporting_dimensions)\n",
    "        print(\"[DEBUG] population_df shape:\", population_df.shape)\n",
    "        print(\"[DEBUG] population_df columns:\", list(population_df.columns))\n",
    "        print(\"[DEBUG] population_df volume sum:\", population_df['volume'].sum())\n",
    "        print(\"[DEBUG] simulation_results:\", simulation_results)\n",
    "\n",
    "    return rows\n",
    "\n",
    "# def _run_single_hyperparam_sim(args):\n",
    "#     import copy\n",
    "#     import numpy as np\n",
    "\n",
    "#     df_data, base_params, param_set = args\n",
    "#     params = copy.deepcopy(base_params)\n",
    "#     params.update(param_set)\n",
    "\n",
    "#     # 1. Filter by primary_product if needed\n",
    "#     filtered_df = df_data\n",
    "#     if 'primary_product' in params and isinstance(params['primary_product'], list):\n",
    "#         filtered_df = filtered_df[filtered_df['primary_product'].isin(params['primary_product'])]\n",
    "\n",
    "#     # 2. Scale data if days < data_days (as in generate_report)\n",
    "#     if 'days_count' in filtered_df.columns:\n",
    "#         default_days = int(filtered_df['days_count'].iloc[0])\n",
    "#     else:\n",
    "#         default_days = 30\n",
    "#     data_days = default_days\n",
    "#     days = params.get('days', data_days)\n",
    "#     if days < data_days:\n",
    "#         scale_factor = days / data_days\n",
    "#         cols_to_scale = [\n",
    "#             'volume', 'qa_reviewed', 'qa_reviewed_for_reporting',\n",
    "#             'decision_error_sessions', 'extraction_error_sessions',\n",
    "#             'false_approves', 'false_declines', 'missed_fraud_sessions',\n",
    "#             'declinable_sessions', 'approvable_sessions', 'fraud_sessions',\n",
    "#             'full_auto_sessions', 'declined_sessions', 'approved_sessions',\n",
    "#             'declined_due_to_fraud_sessions', 'approved_due_to_fraud_sessions'\n",
    "#         ]\n",
    "#         for col in cols_to_scale:\n",
    "#             if col in filtered_df.columns:\n",
    "#                 filtered_df[col] = (filtered_df[col] * scale_factor).round().fillna(0).astype(int)\n",
    "\n",
    "#     # 3. Calculate total QA reviewed\n",
    "#     qa_total_source = params.get('qa_total_source', 'Use Current QA Reviewed')\n",
    "#     daily_reviews = params.get('daily_reviews', None)\n",
    "#     days = params.get('days', data_days)\n",
    "#     # if qa_total_source == 'Input Daily Reviews * Days' and daily_reviews is not None and days is not None:\n",
    "#     #     total_qa_reviewed = daily_reviews * days\n",
    "#     # else:\n",
    "#     #     total_qa_reviewed = filtered_df['qa_reviewed'].sum()\n",
    "#     total_qa_reviewed = daily_reviews * days\n",
    "\n",
    "#     # 4. Prepare population data (aggregation, rates, etc.)\n",
    "#     prepare_population_data = globals().get('prepare_population_data')\n",
    "#     selected_metric = params.get('selected_metric', 'Missed Fraud Rate')\n",
    "#     metric_properties = globals().get('METRIC_PROPERTIES', {})\n",
    "#     reporting_dimensions = params.get('reporting_dimensions', ['customer'])\n",
    "\n",
    "#     # Only add 'customer_priority' if running Client-Tier Weighting scenario\n",
    "#     sampling_types = params.get('sampling_types_to_run', ['Current Random'])\n",
    "#     needs_priority = 'Client-Tier Weighting' in sampling_types\n",
    "#     if needs_priority and 'customer_priority' not in reporting_dimensions and 'customer_priority' in filtered_df.columns:\n",
    "#         reporting_dimensions = list(reporting_dimensions) + ['customer_priority']\n",
    "\n",
    "#     z_score = params.get('z_score', 1.96)\n",
    "#     population_df, global_avg_metrics, overall_avg_frequencies_in_qa = prepare_population_data(\n",
    "#         filtered_df, selected_metric, metric_properties, z_score, reporting_dimensions\n",
    "#     )\n",
    "\n",
    "#     # Defensive: If population_df is empty, return dummy row\n",
    "#     if population_df.empty or population_df['volume'].sum() == 0:\n",
    "#         return [{\n",
    "#             **param_set,\n",
    "#             'scenario': 'No Data',\n",
    "#             'cost_total': np.nan,\n",
    "#             'overall_margin': np.nan,\n",
    "#             'share_top_3_customers': np.nan,\n",
    "#             'customer_coverage': 0,\n",
    "#             'product_coverage': 0,\n",
    "#             'industry_coverage': 0,\n",
    "#             'document_country_coverage': 0,\n",
    "#             'region_coverage': 0\n",
    "#         }]\n",
    "\n",
    "#     # 5. Assign true random sample if needed\n",
    "#     assign_true_random_sample = globals().get('assign_true_random_sample')\n",
    "#     if assign_true_random_sample is not None:\n",
    "#         population_df = assign_true_random_sample(population_df, int(total_qa_reviewed))\n",
    "\n",
    "#     # 6. Set 'qa_reviewed_for_reporting'\n",
    "#     # if qa_total_source == 'Input Daily Reviews * Days' and 'true_random_qa_reviewed' in population_df.columns:\n",
    "#     #     population_df['qa_reviewed_for_reporting'] = population_df['true_random_qa_reviewed']\n",
    "#     # else:\n",
    "#     #     population_df['qa_reviewed_for_reporting'] = population_df['qa_reviewed']\n",
    "#     population_df['qa_reviewed_for_reporting'] = population_df['true_random_qa_reviewed']\n",
    "\n",
    "#     # 7. Run simulations as usual\n",
    "#     simulation_results = run_simulations(\n",
    "#         population_df=population_df,\n",
    "#         global_avg_metrics=global_avg_metrics,\n",
    "#         overall_avg_frequencies_in_qa=overall_avg_frequencies_in_qa,\n",
    "#         selected_metric=selected_metric,\n",
    "#         reporting_dimensions=reporting_dimensions,\n",
    "#         error_margin_logic=params.get('error_margin_logic', 'Absolute Error Margin'),\n",
    "#         absolute_error_margin_value=params.get('absolute_error_margin_value', 0.02),\n",
    "#         relative_error_margin_value=params.get('relative_error_margin_value', 0.2),\n",
    "#         min_absolute_error_margin=params.get('min_absolute_error_margin', 0.005),\n",
    "#         target_overall_error_percent=params.get('target_overall_error_percent', 2.0),\n",
    "#         target_dimension_error_percent=params.get('target_dimension_error_percent', 2.0),\n",
    "#         weight_factor_small_customers=params.get('weight_factor_small_customers', 10),\n",
    "#         small_customer_volume_quantile_threshold=params.get('small_customer_volume_quantile_threshold', 0.99),\n",
    "#         stratify_by_dimension=params.get('stratify_by_dimension', ['product_portfolio', 'industry', 'region']),\n",
    "#         sampling_types_to_run=sampling_types,\n",
    "#         top_n=params.get('top_n', 10),\n",
    "#         z_score=z_score,\n",
    "#         min_per_stratum=params.get('min_per_stratum', 500),\n",
    "#         verbose=False\n",
    "#     )\n",
    "#     rows = []\n",
    "#     for scenario, res in simulation_results.items():\n",
    "#         row = dict(param_set)\n",
    "#         row['scenario'] = scenario\n",
    "#         row.update(res)\n",
    "#         for col in ['cost_total', 'overall_margin', 'share_top_3_customers']:\n",
    "#             if col not in row:\n",
    "#                 row[col] = np.nan\n",
    "#         rows.append(row)\n",
    "\n",
    "#     # If all coverage columns are zero, print debug info\n",
    "#     if all(row.get(cov, 0) == 0 for cov in ['customer_coverage', 'product_coverage', 'industry_coverage', 'document_country_coverage', 'region_coverage']):\n",
    "#         print(\"[DEBUG] No coverage found for params:\", param_set)\n",
    "#         print(\"[DEBUG] reporting_dimensions:\", reporting_dimensions)\n",
    "#         print(\"[DEBUG] population_df shape:\", population_df.shape)\n",
    "#         print(\"[DEBUG] population_df columns:\", list(population_df.columns))\n",
    "#         print(\"[DEBUG] population_df volume sum:\", population_df['volume'].sum())\n",
    "#         print(\"[DEBUG] simulation_results:\", simulation_results)\n",
    "\n",
    "#     return rows\n",
    "        \n",
    "\n",
    "def hyperparameter_simulation_search(\n",
    "    df_data,\n",
    "    base_params,\n",
    "    param_grid,\n",
    "    constraints=None,\n",
    "    scoring_fn=None,\n",
    "    max_workers=8  # Adjust based on your CPU\n",
    "):\n",
    "    import concurrent.futures\n",
    "    import itertools\n",
    "    import pandas as pd\n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    args_list = [(df_data, base_params, param_set) for param_set in param_combinations]\n",
    "\n",
    "    all_results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(_run_single_hyperparam_sim, args) for args in args_list]\n",
    "        for i, future in enumerate(tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Hyperparameter Search\")):\n",
    "            rows = future.result()\n",
    "            all_results.extend(rows)\n",
    "\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "\n",
    "    if constraints:\n",
    "        mask = pd.Series([True] * len(results_df))\n",
    "        for k, fn in constraints.items():\n",
    "            mask &= results_df[k].apply(fn)\n",
    "        filtered = results_df[mask]\n",
    "    else:\n",
    "        filtered = results_df\n",
    "\n",
    "    # Improved scoring: maximize coverage, then minimize cost\n",
    "    def default_scoring(row):\n",
    "        coverage = sum([\n",
    "            row.get('customer_coverage', 0),\n",
    "            row.get('product_coverage', 0),\n",
    "            row.get('industry_coverage', 0),\n",
    "            row.get('document_country_coverage', 0),\n",
    "            row.get('region_coverage', 0)\n",
    "        ])\n",
    "        # Use negative cost so that lower cost is better for tie-breaking\n",
    "        return (coverage, -row.get('cost_total', 1e9))\n",
    "\n",
    "    def weighted_scoring(row):\n",
    "        # Example weights (tune as needed)\n",
    "        w_coverage = 2\n",
    "        w_cost = -0.01\n",
    "        w_margin = -1\n",
    "        w_top3 = -0.5\n",
    "        # Calculate coverage\n",
    "        coverage = sum([\n",
    "            row.get('customer_coverage', 0),\n",
    "            row.get('product_coverage', 0),\n",
    "            row.get('industry_coverage', 0),\n",
    "            row.get('document_country_coverage', 0),\n",
    "            row.get('region_coverage', 0)\n",
    "        ])\n",
    "        cost = row.get('cost_total', 1e9)\n",
    "        margin = row.get('overall_margin', 1)\n",
    "        top3 = row.get('share_top_3_customers', 100)\n",
    "        # Combine with weights\n",
    "        return w_coverage * coverage + w_cost * cost + w_margin * margin + w_top3 * top3\n",
    "\n",
    "    if scoring_fn is None:\n",
    "        scoring_fn = default_scoring\n",
    "    if scoring_fn == 'weighted_scoring':\n",
    "        scoring_fn = weighted_scoring\n",
    "\n",
    "    # Find best row: maximize coverage, then minimize cost\n",
    "    if not filtered.empty:\n",
    "        best_idx = filtered.apply(scoring_fn, axis=1).idxmax()\n",
    "        best_row = filtered.loc[best_idx]\n",
    "        print(\"\\nBest configuration found (meets constraints):\")\n",
    "    else:\n",
    "        print(\"\\n[WARNING] No configuration met all constraints. Showing best available result.\")\n",
    "        best_idx = results_df.apply(scoring_fn, axis=1).idxmax()\n",
    "        best_row = results_df.loc[best_idx]\n",
    "\n",
    "    print(best_row)\n",
    "    return results_df, best_row\n",
    "\n",
    "def display_hyperparam_search_summary(results_df):\n",
    "    \"\"\"\n",
    "    Display a styled summary table for hyperparameter search results, similar to display_simulation_summary.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from IPython.display import display\n",
    "\n",
    "    if results_df.empty:\n",
    "        print(\"\\nNo hyperparameter search results to summarize.\")\n",
    "        return\n",
    "\n",
    "    # Calculate improvement if possible\n",
    "    results_df.replace([np.inf, -np.inf], pd.NA, inplace=True)\n",
    "    if 'overall_margin' in results_df.columns:\n",
    "        results_df['overall_margin_perc'] = results_df['overall_margin'] * 100\n",
    "\n",
    "    if 'overall_margin_perc' in results_df.columns and not results_df.empty:\n",
    "        base_margin = results_df['overall_margin_perc'].iloc[0]\n",
    "        if pd.isna(base_margin) or base_margin == 0:\n",
    "            valid_bases = results_df['overall_margin_perc'].dropna()\n",
    "            base_margin = valid_bases.iloc[0] if not valid_bases.empty else 0\n",
    "        if base_margin > 0:\n",
    "            results_df['Improvement (%)'] = (base_margin - results_df['overall_margin_perc']) / base_margin * 100\n",
    "        else:\n",
    "            results_df['Improvement (%)'] = 0\n",
    "    else:\n",
    "        results_df['Improvement (%)'] = pd.NA\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    results_df.rename(columns={\n",
    "        'total_sampled': 'Total QA Sampled',\n",
    "        'cost_total': 'Total Cost (EUR)',\n",
    "        'overall_margin_perc': 'Overall Margin (%)',\n",
    "        'dimension_coverage': 'Selected dimension covered below threshold',\n",
    "        'customer_coverage': 'Customers below threshold',\n",
    "        'product_coverage': 'Products below threshold',\n",
    "        'industry_coverage': 'Industries below threshold',\n",
    "        'document_country_coverage': 'Document Countries below threshold',\n",
    "        'region_coverage': 'Regions below threshold',\n",
    "        'Improvement (%)': 'Improvement in Error Margin (%)',\n",
    "        'share_top_3_customers': 'Share of Top 3 Customers in Sample (%)',\n",
    "        'share_strategic_clients_met_sla': 'Share of Platinum Clients Met SLA (%)',\n",
    "        'cost_over_coverage': 'Cost per Actionable Unit (EUR)'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Choose columns to display (add/remove as needed)\n",
    "    display_cols = [\n",
    "        'scenario',\n",
    "        'min_per_stratum',\n",
    "        'absolute_error_margin_value',\n",
    "        'days',\n",
    "        'daily_reviews',\n",
    "        'Total QA Sampled',\n",
    "        'Total Cost (EUR)',\n",
    "        'Overall Margin (%)',\n",
    "        'Customers below threshold',\n",
    "        'Products below threshold',\n",
    "        'Industries below threshold',\n",
    "        'Document Countries below threshold',\n",
    "        'Regions below threshold',\n",
    "        'Improvement in Error Margin (%)',\n",
    "        'Share of Top 3 Customers in Sample (%)',\n",
    "        'Share of Platinum Clients Met SLA (%)',\n",
    "        'Cost per Actionable Unit (EUR)'\n",
    "    ]\n",
    "    existing_display_cols = [col for col in display_cols if col in results_df.columns]\n",
    "\n",
    "    # Round all columns to 2 decimals where appropriate\n",
    "    rounded_df = results_df[existing_display_cols].copy().applymap(\n",
    "        lambda x: round(x, 2) if isinstance(x, (float, int)) and pd.notnull(x) else x\n",
    "    )\n",
    "\n",
    "    print(\"\\n## Hyperparameter Search Summary Table\")\n",
    "    def highlight_improvement(val):\n",
    "        if pd.isna(val):\n",
    "            return ''\n",
    "        color = 'green' if val > 0 else 'red' if val < 0 else ''\n",
    "        return f'color: {color}'\n",
    "\n",
    "    def bold_margin(val):\n",
    "        try:\n",
    "            threshold = globals().get('TARGET_OVERALL_ERROR_PERCENT', 2.0)\n",
    "            if float(val) < threshold:\n",
    "                return 'font-weight: bold'\n",
    "        except Exception:\n",
    "            pass\n",
    "        return ''\n",
    "\n",
    "    styled = (\n",
    "        rounded_df\n",
    "        .style\n",
    "        .format(\"{:.2f}\")\n",
    "        .applymap(highlight_improvement, subset=['Improvement in Error Margin (%)'] if 'Improvement in Error Margin (%)' in rounded_df.columns else [])\n",
    "        .applymap(bold_margin, subset=['Overall Margin (%)'] if 'Overall Margin (%)' in rounded_df.columns else [])\n",
    "    )\n",
    "    display(styled)\n",
    "\n",
    "def print_hyper_search_summary(best_row, error_margin_logic, abs_margin_value, rel_margin_value, results_df=None):\n",
    "    \"\"\"\n",
    "    Print a summary for the best hyperparameter search result, or fallback to best available if none met constraints.\n",
    "    \"\"\"\n",
    "    print(\"\\n## Hyperparameter Search: Best Configuration Summary\")\n",
    "    if best_row is None or (hasattr(best_row, \"empty\") and best_row.empty):\n",
    "        print(\"No valid configuration met all constraints.\")\n",
    "        # Fallback: show best available by cost or coverage if results_df is provided\n",
    "        if results_df is not None and not results_df.empty:\n",
    "            print(\"\\nShowing best available configuration (did not meet all constraints):\")\n",
    "            # Use the same scoring as in hyperparameter_simulation_search\n",
    "            def scoring(row):\n",
    "                coverage = sum([\n",
    "                    row.get('customer_coverage', 0),\n",
    "                    row.get('product_coverage', 0),\n",
    "                    row.get('industry_coverage', 0),\n",
    "                    row.get('document_country_coverage', 0),\n",
    "                    row.get('region_coverage', 0)\n",
    "                ])\n",
    "                return (coverage, -row.get('cost_total', 1e9))\n",
    "            best_idx = results_df.apply(scoring, axis=1).idxmax()\n",
    "            best_row = results_df.loc[best_idx]\n",
    "        else:\n",
    "            print(\"No configurations available.\")\n",
    "            return\n",
    "\n",
    "    # Determine threshold logic and value\n",
    "    if error_margin_logic == 'Absolute Error Margin':\n",
    "        threshold_type = \"Absolute Error Margin\"\n",
    "        threshold_value = abs_margin_value * 100\n",
    "        threshold_unit = \"%\"\n",
    "    else:\n",
    "        threshold_type = \"Relative Error Margin\"\n",
    "        threshold_value = rel_margin_value * 100\n",
    "        threshold_unit = \"% of group rate\"\n",
    "\n",
    "    print(f\"Threshold logic used: {threshold_type} (Threshold: {threshold_value:.2f}{threshold_unit})\")\n",
    "    print(f\"Sample size: {best_row.get('total_sampled', 'N/A')} sessions (Scenario: {best_row.get('scenario', 'N/A')})\")\n",
    "    print(f\"  - Overall error margin: {best_row.get('overall_margin', float('nan'))*100:.2f}%\")\n",
    "    print(f\"  - Covers {best_row.get('customer_coverage', 0)} customers below threshold.\")\n",
    "    print(f\"  - Customers meeting target: {best_row.get('customer_list', '')}\")\n",
    "    print(f\"  - Covers {best_row.get('product_coverage', 0)} products below threshold.\")\n",
    "    print(f\"  - Products meeting target: {best_row.get('product_list', '')}\")\n",
    "    print(f\"  - Covers {best_row.get('industry_coverage', 0)} industries below threshold.\")\n",
    "    print(f\"  - Industries meeting target: {best_row.get('industry_list', '')}\")\n",
    "    print(f\"  - Covers {best_row.get('document_country_coverage', 0)} document countries below threshold.\")\n",
    "    print(f\"  - Document countries meeting target: {best_row.get('document_country_list', '')}\")\n",
    "    print(f\"  - Covers {best_row.get('region_coverage', 0)} regions below threshold.\")\n",
    "    print(f\"  - Regions meeting target: {best_row.get('region_list', '')}\")\n",
    "    print(f\"  - Share of top 3 customers: {best_row.get('share_top_3_customers', 0):.2f}%\")\n",
    "    print(f\"  - Share of strategic clients meeting SLA: {best_row.get('share_strategic_clients_met_sla', 0):.2f}%\")\n",
    "    print(f\"  - Total Cost for this configuration: ${best_row.get('cost_total', 0):.2f} EUR\")\n",
    "\n",
    "\n",
    "from skopt.space import Integer, Real\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def skopt_hyperparameter_search(\n",
    "    df_data,\n",
    "    base_params,\n",
    "    param_space,  # list of skopt.space objects\n",
    "    param_names,  # list of param names in order\n",
    "    n_calls=50,\n",
    "    constraints=None,  # dict of {col: lambda x: ...}\n",
    "    verbose=True,\n",
    "    scenario_names=None  # list of scenario names to run for each param set\n",
    "):\n",
    "    from tqdm.notebook import tqdm\n",
    "    import pandas as pd\n",
    "    from skopt import gp_minimize\n",
    "\n",
    "    tried_params = []\n",
    "    tried_results = []\n",
    "    tried_objectives = []\n",
    "    tried_scenarios = []\n",
    "\n",
    "    # Only run n_calls, not n_calls * len(scenario_names), since sample size is fixed by daily_reviews * days\n",
    "    pbar = tqdm(total=n_calls * (len(scenario_names) if scenario_names else 1), desc=\"skopt Optimization Progress\")\n",
    "\n",
    "    # Cache to avoid duplicate runs\n",
    "    cache = {}\n",
    "\n",
    "    def objective(params):\n",
    "        param_set = dict(zip(param_names, params))\n",
    "        # Always pass all scenario_names at once\n",
    "        if scenario_names:\n",
    "            param_set['sampling_types_to_run'] = scenario_names\n",
    "            cache_key = tuple(params) + tuple(scenario_names)\n",
    "        else:\n",
    "            cache_key = tuple(params)\n",
    "        if cache_key in cache:\n",
    "            rows = cache[cache_key]\n",
    "        else:\n",
    "            rows = _run_single_hyperparam_sim((df_data, base_params, param_set))\n",
    "            cache[cache_key] = rows\n",
    "        objs = []\n",
    "        for row in rows:\n",
    "            coverage = sum([\n",
    "                row.get('customer_coverage', 0),\n",
    "                row.get('product_coverage', 0),\n",
    "                row.get('industry_coverage', 0),\n",
    "                row.get('document_country_coverage', 0),\n",
    "                row.get('region_coverage', 0)\n",
    "            ])\n",
    "            share_sla = row.get('share_strategic_clients_met_sla', 0)\n",
    "            cost_per_unit = row.get('cost_over_coverage', 1e9)\n",
    "            penalty = 0\n",
    "            if constraints:\n",
    "                for k, fn in constraints.items():\n",
    "                    if not fn(row.get(k, None)):\n",
    "                        penalty += 1000\n",
    "            obj = -(2 * coverage + 1 * share_sla) + 0.1 * cost_per_unit + penalty\n",
    "            tried_params.append(param_set.copy())\n",
    "            tried_results.append(row)\n",
    "            tried_objectives.append(obj)\n",
    "            tried_scenarios.append(row.get('scenario', None))\n",
    "            objs.append(obj)\n",
    "            pbar.update(1)\n",
    "        return min(objs)  # or mean(objs), depending on your goal\n",
    "\n",
    "    # Run the optimization\n",
    "    result = gp_minimize(\n",
    "        objective,\n",
    "        param_space,\n",
    "        n_calls=n_calls,\n",
    "        random_state=42\n",
    "    )\n",
    "    pbar.close()\n",
    "\n",
    "    # --- Build results DataFrame robustly ---\n",
    "    results_records = []\n",
    "    for param_set, row, obj, scenario in zip(tried_params, tried_results, tried_objectives, tried_scenarios):\n",
    "        record = dict(param_set)\n",
    "        record.update(row)\n",
    "        record['scenario'] = scenario\n",
    "        record['objective'] = obj\n",
    "        record['stratify_by_dimension'] = param_set.get('stratify_by_dimension', row.get('stratify_by_dimension', None))\n",
    "        results_records.append(record)\n",
    "    results_df = pd.DataFrame(results_records)\n",
    "\n",
    "    # Apply constraints to filter best\n",
    "    if constraints:\n",
    "        mask = pd.Series([True] * len(results_df))\n",
    "        for k, fn in constraints.items():\n",
    "            mask &= results_df[k].apply(fn)\n",
    "        filtered = results_df[mask]\n",
    "    else:\n",
    "        filtered = results_df\n",
    "\n",
    "    # Find best row: lowest objective value\n",
    "    if not filtered.empty:\n",
    "        best_idx = filtered['objective'].idxmin()\n",
    "        best_row = filtered.loc[best_idx]\n",
    "        print(\"\\nBest configuration found (meets constraints):\")\n",
    "    else:\n",
    "        print(\"\\n[WARNING] No configuration met all constraints. Showing best available result.\")\n",
    "        best_idx = results_df['objective'].idxmin()\n",
    "        best_row = results_df.loc[best_idx]\n",
    "\n",
    "    print(\"\\nBest parameters found by skopt:\")\n",
    "    best_params = dict(zip(param_names, result.x))\n",
    "    print(best_params)\n",
    "    print(\"Best objective value:\", result.fun)\n",
    "    print(best_row)\n",
    "\n",
    "    cols = ['min_per_stratum', 'daily_reviews', 'scenario',\n",
    "       'overall_margin', 'total_sampled', 'cost_total', 'customer_coverage', 'customer_list',\n",
    "       'product_coverage', 'product_list', 'industry_coverage',\n",
    "       'industry_list', 'document_country_coverage', 'document_country_list',\n",
    "       'region_coverage', 'region_list', 'share_top_3_customers',\n",
    "       'share_strategic_clients_met_sla', 'cost_over_coverage', 'stratify_by_dimension']\n",
    "\n",
    "    results_df['is_best_row'] = results_df.index == best_row.name\n",
    "\n",
    "    # Automatically save results_df to CSV\n",
    "    results_df = results_df.sort_values(\n",
    "                by=['is_best_row', 'customer_coverage', 'product_coverage', 'industry_coverage', 'document_country_coverage', 'region_coverage', 'cost_total'],\n",
    "                ascending=[True, False, False, False, False, False, True]\n",
    "    )\n",
    "\n",
    "    results_df = results_df[cols].copy()\n",
    "    best_row = best_row[cols].copy()\n",
    "\n",
    "    results_df.to_csv('skopt_hyperparam_results.csv', index=False)\n",
    "    \n",
    "    print(\"skopt hyperparameter search results saved to skopt_hyperparam_results.csv\")\n",
    "\n",
    "    return results_df, best_row, results_df.columns\n",
    "\n",
    "#old\n",
    "# def skopt_hyperparameter_search(\n",
    "#     df_data,\n",
    "#     base_params,\n",
    "#     param_space,  # list of skopt.space objects\n",
    "#     param_names,  # list of param names in order\n",
    "#     n_calls=30,\n",
    "#     constraints=None,  # dict of {col: lambda x: ...}\n",
    "#     verbose=True,\n",
    "#     scenario_names=None  # list of scenario names to run for each param set\n",
    "# ):\n",
    "#     from tqdm.notebook import tqdm\n",
    "#     import pandas as pd\n",
    "#     from skopt import gp_minimize\n",
    "\n",
    "#     tried_params = []\n",
    "#     tried_results = []\n",
    "#     tried_objectives = []\n",
    "#     tried_scenarios = []\n",
    "\n",
    "#     #pbar = tqdm(desc=\"skopt Optimization Progress\")\n",
    "#     pbar = tqdm(total=n_calls * (len(scenario_names) if scenario_names else 1), desc=\"skopt Optimization Progress\")\n",
    "\n",
    "#     # Cache to avoid duplicate runs\n",
    "#     cache = {}\n",
    "\n",
    "#     def objective(params):\n",
    "#         param_set = dict(zip(param_names, params))\n",
    "#         # Always pass all scenario_names at once\n",
    "#         if scenario_names:\n",
    "#             param_set['sampling_types_to_run'] = scenario_names\n",
    "#             cache_key = tuple(params) + tuple(scenario_names)\n",
    "#         else:\n",
    "#             cache_key = tuple(params)\n",
    "#         if cache_key in cache:\n",
    "#             rows = cache[cache_key]\n",
    "#         else:\n",
    "#             rows = _run_single_hyperparam_sim((df_data, base_params, param_set))\n",
    "#             cache[cache_key] = rows\n",
    "#         objs = []\n",
    "#         for row in rows:\n",
    "#             coverage = sum([\n",
    "#                 row.get('customer_coverage', 0),\n",
    "#                 row.get('product_coverage', 0),\n",
    "#                 row.get('industry_coverage', 0),\n",
    "#                 row.get('document_country_coverage', 0),\n",
    "#                 row.get('region_coverage', 0)\n",
    "#             ])\n",
    "#             share_sla = row.get('share_strategic_clients_met_sla', 0)\n",
    "#             cost_per_unit = row.get('cost_over_coverage', 1e9)\n",
    "#             penalty = 0\n",
    "#             if constraints:\n",
    "#                 for k, fn in constraints.items():\n",
    "#                     if not fn(row.get(k, None)):\n",
    "#                         penalty += 1000\n",
    "#             obj = -(2 * coverage + 1 * share_sla) + 0.1 * cost_per_unit + penalty\n",
    "#             tried_params.append(param_set.copy())\n",
    "#             tried_results.append(row)\n",
    "#             tried_objectives.append(obj)\n",
    "#             tried_scenarios.append(row.get('scenario', None))\n",
    "#             objs.append(obj)\n",
    "#             pbar.update(1)\n",
    "#         return min(objs)  # or mean(objs), depending on your goal\n",
    "\n",
    "#     # Run the optimization\n",
    "#     result = gp_minimize(\n",
    "#         objective,\n",
    "#         param_space,\n",
    "#         n_calls=n_calls,\n",
    "#         random_state=42\n",
    "#     )\n",
    "#     pbar.close()\n",
    "\n",
    "#     # --- FIX: Build results DataFrame robustly ---\n",
    "#     results_records = []\n",
    "#     for param_set, row, obj, scenario in zip(tried_params, tried_results, tried_objectives, tried_scenarios):\n",
    "#         record = dict(param_set)\n",
    "#         record.update(row)\n",
    "#         record['scenario'] = scenario\n",
    "#         record['objective'] = obj\n",
    "#         results_records.append(record)\n",
    "#     results_df = pd.DataFrame(results_records)\n",
    "\n",
    "#     # Apply constraints to filter best\n",
    "#     if constraints:\n",
    "#         mask = pd.Series([True] * len(results_df))\n",
    "#         for k, fn in constraints.items():\n",
    "#             mask &= results_df[k].apply(fn)\n",
    "#         filtered = results_df[mask]\n",
    "#     else:\n",
    "#         filtered = results_df\n",
    "\n",
    "#     # Find best row: lowest objective value\n",
    "#     if not filtered.empty:\n",
    "#         best_idx = filtered['objective'].idxmin()\n",
    "#         best_row = filtered.loc[best_idx]\n",
    "#         print(\"\\nBest configuration found (meets constraints):\")\n",
    "#     else:\n",
    "#         print(\"\\n[WARNING] No configuration met all constraints. Showing best available result.\")\n",
    "#         best_idx = results_df['objective'].idxmin()\n",
    "#         best_row = results_df.loc[best_idx]\n",
    "\n",
    "#     print(\"\\nBest parameters found by skopt:\")\n",
    "#     best_params = dict(zip(param_names, result.x))\n",
    "#     print(best_params)\n",
    "#     print(\"Best objective value:\", result.fun)\n",
    "#     print(best_row)\n",
    "\n",
    "#     cols = ['min_per_stratum', 'daily_reviews', 'sampling_types_to_run', 'strategy_name','scenario',\n",
    "#        'overall_margin', 'total_sampled', 'cost_total', 'customer_coverage', 'customer_list',\n",
    "#        'product_coverage', 'product_list', 'industry_coverage',\n",
    "#        'industry_list', 'document_country_coverage', 'document_country_list',\n",
    "#        'region_coverage', 'region_list', 'share_top_3_customers',\n",
    "#        'share_strategic_clients_met_sla', 'cost_over_coverage']\n",
    "\n",
    "#     results_df = results_df[cols].copy()\n",
    "#     best_row = best_row[cols].copy()\n",
    "\n",
    "#     return results_df, best_row, results_df.columns\n",
    "\n",
    "# --- Interactive UI Setup ---\n",
    "def create_interactive_ui(df_data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Creates and displays an interactive UI using ipywidgets for configuring and running the report.\n",
    "    \"\"\"\n",
    "\n",
    "    run_button = widgets.Button(description=\"Create Report\", button_style='success', layout=widgets.Layout(width='auto'))\n",
    "    hyperparam_button = widgets.Button(description=\"Find Optimal Solution\", button_style='info', layout=widgets.Layout(width='auto'))\n",
    "    output_area = widgets.Output()\n",
    "    hyperparam_output_area = widgets.Output()\n",
    "\n",
    "    # Initial config to derive available dimensions and metrics (uses default global values)\n",
    "    metric_related_cols = set()\n",
    "    for props in METRIC_PROPERTIES.values():\n",
    "        metric_related_cols.add(props['numerator_col'])\n",
    "        metric_related_cols.add(props['denominator_col'])\n",
    "        metric_related_cols.add(props['freq_denominator_base_col'])\n",
    "\n",
    "    exclude_cols = metric_related_cols.union({\n",
    "        'volume', 'qa_reviewed', 'rand', 'bias_score', \n",
    "        'original_metric_rate', 'original_freq_den', \n",
    "        'simulated_total_sessions', 'simulated_num_x', 'simulated_den_n',\n",
    "        'margin', 'margin_rel', 'temp_sampling_volume_small', \n",
    "        'temp_sampling_volume_tier', 'temp_sampling_volume_stratified_proportional',\n",
    "        'temp_sampling_volume_stratified_equal',\n",
    "        'temp_sampling_volume_strat', 'strata', 'total_volume', 'total_qa_reviewed',\n",
    "        'proportion_of_total_volume', 'proportion_of_qa_sample', 'false_decline_rate',\n",
    "        'automation_rate', 'decline_rate_in_population', 'approve_rate_in_population', 'fraud_rate_in_population',\n",
    "        'decision_error_rate', 'extraction_error_rate', 'false_approve_rate',\n",
    "        'false_decline_rate', 'missed_fraud_rate', 'decline_rate_in_qa', 'approve_rate_in_qa', 'fraud_rate_in_qa'\n",
    "    })\n",
    "\n",
    "    available_products = sorted(df_data['primary_product'].dropna().unique())\n",
    "\n",
    "    available_dimensions = sorted(list(set(df_data.columns) - exclude_cols))\n",
    "    \n",
    "    initial_reporting_dims = [d for d in DEFAULT_REPORTING_DIMENSIONS if d in available_dimensions]\n",
    "    initial_stratify_dims = [d for d in DEFAULT_STRATIFY_BY_DIMENSION if d in available_dimensions]\n",
    "    initial_products = [p for p in DEFAULT_AVAILABLE_PRODUCTS if p in available_products]\n",
    "    \n",
    "    if not initial_reporting_dims and available_dimensions:\n",
    "        initial_reporting_dims = [available_dimensions[0]]\n",
    "    if not initial_stratify_dims and available_dimensions:\n",
    "        initial_stratify_dims = [available_dimensions[0]]\n",
    "    if not initial_products and available_products:\n",
    "        initial_products = [available_products[0]]\n",
    "\n",
    "\n",
    "    all_metrics = sorted(list(METRIC_PROPERTIES.keys()))\n",
    "    all_sampling_types = [\n",
    "        'Current Random', 'True Random', 'Oversample Small Traditional', 'Client-Tier Weighting', \n",
    "        'Proportional Stratified Sampling',\n",
    "        'Equal Stratified Sampling',\n",
    "        'Biased with Exponents'\n",
    "        #,'Volume-Based Stratified Sampling'\n",
    "        #, 'Random'\n",
    "    ]\n",
    "\n",
    "    qa_tiers = ['Tier Platinum', 'Tier 1', 'Tier 2', 'Tier 3', 'Tier 4', 'Tier 5', 'UNKNOWN', '']\n",
    "    editable_tiers = ['Tier Platinum', 'Tier 1']\n",
    "    auto_tiers = [t for t in qa_tiers if t not in editable_tiers]\n",
    "\n",
    "    # Sliders for editable tiers\n",
    "    qa_proportion_widgets = {\n",
    "        tier: widgets.FloatSlider(\n",
    "            value=TARGET_QA_PROPORTIONS.get(tier, 0.1),\n",
    "            min=0.0, max=1.0, step=0.01,\n",
    "            description=f\"{tier}:\",\n",
    "            continuous_update=True,\n",
    "            readout_format='.2f',\n",
    "            layout=widgets.Layout(width='90%'),\n",
    "            style={'description_width': '120px'}\n",
    "        )\n",
    "        for tier in editable_tiers\n",
    "    }\n",
    "\n",
    "    # Read-only displays for auto tiers\n",
    "    qa_proportion_labels = {\n",
    "        tier: widgets.Label(\n",
    "            value=f\"{tier or 'Unknown'}: {TARGET_QA_PROPORTIONS.get(tier, 0.1):.2f}\"\n",
    "        )\n",
    "        for tier in auto_tiers\n",
    "    }\n",
    "\n",
    "    def update_auto_tiers(*args):\n",
    "        platinum = qa_proportion_widgets['Tier Platinum'].value\n",
    "        tier1 = qa_proportion_widgets['Tier 1'].value\n",
    "        remaining = max(0.0, 1.0 - platinum - tier1)\n",
    "        per_tier = remaining / len(auto_tiers) if auto_tiers else 0.0\n",
    "        for tier in auto_tiers:\n",
    "            qa_proportion_labels[tier].value = f\"{tier or 'Unknown'}: {per_tier:.2f}\"\n",
    "\n",
    "    for w in qa_proportion_widgets.values():\n",
    "        w.observe(update_auto_tiers, names='value')\n",
    "\n",
    "    update_auto_tiers()  # Initialize\n",
    "\n",
    "    qa_proportion_box = widgets.VBox([\n",
    "        widgets.HTML(\"<h4>QA Proportions by Client Tier (For simulation) </h4>\")\n",
    "    ] + list(qa_proportion_widgets.values()) + list(qa_proportion_labels.values()))\n",
    "    \n",
    "    primary_product_widget = widgets.SelectMultiple(\n",
    "        options=available_products,\n",
    "        value=initial_products,\n",
    "        description='Primary Product(s):',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='90%', height='100px'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "\n",
    "    metric_widget = widgets.Dropdown(\n",
    "        options=all_metrics,\n",
    "        value=DEFAULT_SELECTED_METRIC,\n",
    "        description='Metric:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    overall_error_widget = widgets.FloatText(\n",
    "        value=DEFAULT_TARGET_OVERALL_ERROR_PERCENT, min=0.1, max=100.0, step=0.5,\n",
    "        description='Overall Error %:',\n",
    "        continuous_update=False, orientation='horizontal', readout=True, readout_format='.1f',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    dim_error_widget = widgets.FloatText(\n",
    "        value=DEFAULT_TARGET_DIMENSION_ERROR_PERCENT, min=0.1, max=100.0, step=0.5,\n",
    "        description='Dimension Error %:',\n",
    "        continuous_update=False, orientation='horizontal', readout=True, readout_format='.1f',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    confidence_widget = widgets.FloatText(\n",
    "        value=DEFAULT_CONFIDENCE_LEVEL, min=0.80, max=0.99, step=0.01,\n",
    "        description='Confidence Level:',\n",
    "        continuous_update=False, orientation='horizontal', readout=True, readout_format='.2f',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    cost_per_session_widget = widgets.FloatText(\n",
    "        value=DEFAULT_REVIEW_COST_PER_SESSION,\n",
    "        description='Cost/Session (EUR):',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    budget_widget = widgets.FloatText(\n",
    "        value=DEFAULT_PREDEFINED_TOTAL_COST_BUDGET,\n",
    "        description='Budget (EUR):',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    weight_small_customers_widget = widgets.FloatText(\n",
    "        value=DEFAULT_WEIGHT_FACTOR_SMALL_CUSTOMERS, min=1.0, max=50.0, step=0.1,\n",
    "        description='Weight for small customers:',\n",
    "        continuous_update=False, orientation='horizontal', readout=True, readout_format='.1f',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    small_customer_quantile_widget = widgets.FloatSlider(\n",
    "        value=DEFAULT_SMALL_CUSTOMER_VOLUME_QUANTILE_THRESHOLD, min=0.05, max=0.99, step=0.01,\n",
    "        description='Small Cust. Quantile:',\n",
    "        continuous_update=False, orientation='horizontal', readout=True, readout_format='.2f',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    error_margin_logic_widget = widgets.RadioButtons(\n",
    "        options=['Absolute Error Margin', 'Relative Error Margin'],\n",
    "        value=DEFAULT_ERROR_MARGIN_LOGIC,\n",
    "        description='Error Margin Logic:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    abs_error_value_widget = widgets.FloatText(\n",
    "        value=DEFAULT_ABSOLUTE_ERROR_MARGIN_VALUE,\n",
    "        description='Abs. Error Value:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    rel_error_value_widget = widgets.FloatText(\n",
    "        value=DEFAULT_RELATIVE_ERROR_MARGIN_VALUE,\n",
    "        description='Rel. Error Value:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    min_abs_error_widget = widgets.FloatText(\n",
    "        value=DEFAULT_MIN_ABSOLUTE_ERROR_MARGIN,\n",
    "        description='Min Abs. Error:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    reporting_dims_widget = widgets.SelectMultiple(\n",
    "        options=available_dimensions,\n",
    "        value=initial_reporting_dims,\n",
    "        description='Reporting Dimensions:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='90%', height='100px'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    stratify_dims_widget = widgets.SelectMultiple(\n",
    "        options=available_dimensions,\n",
    "        value=initial_stratify_dims,\n",
    "        description='Stratify Dimensions:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='90%', height='100px'),\n",
    "        style={'description_width': '200px'} \n",
    "    )\n",
    "    min_per_stratum_widget = widgets.IntText(\n",
    "    value=DEFAULT_MIN_PER_STRATUM,\n",
    "    description='Min per Stratum:',\n",
    "    layout=widgets.Layout(width='90%'),\n",
    "    style={'description_width': '200px'}\n",
    "    )\n",
    "    top_n_widget = widgets.IntSlider(\n",
    "        value=DEFAULT_TOP_N, min=1, max=20, step=1,\n",
    "        description='Top N for Plots:',\n",
    "        continuous_update=False, orientation='horizontal', readout=True,\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    \n",
    "    sampling_types_widget = widgets.SelectMultiple(\n",
    "        options=all_sampling_types,\n",
    "        value=all_sampling_types,\n",
    "        description='Run Scenarios:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='90%', height='150px'),\n",
    "        style={'description_width': '200px'} \n",
    "    )\n",
    "\n",
    "    save_plots_html_widget = widgets.Checkbox(\n",
    "        value=DEFAULT_SAVE_PLOTS_TO_HTML,\n",
    "        description='Save Plots to HTML Files',\n",
    "        disabled=False,\n",
    "        indent=False,\n",
    "        layout=widgets.Layout(width='auto'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "\n",
    "    daily_reviews_widget = widgets.IntText(\n",
    "    value=2000,\n",
    "    description='Daily Reviews:',\n",
    "    layout=widgets.Layout(width='90%'),\n",
    "    style={'description_width': '200px'}\n",
    "    )\n",
    "    days_widget = widgets.IntText(\n",
    "        value=30,\n",
    "        description='Days:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "    qa_total_source_widget = widgets.RadioButtons(\n",
    "        options=['Use Current QA Reviewed', 'Input Daily Reviews * Days'],\n",
    "        value='Use Current QA Reviewed',\n",
    "        description='QA Total Source:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '200px'}\n",
    "    )\n",
    "\n",
    "    # --- Hyperparameter Search Section ---\n",
    "\n",
    "        # Widgets for param grid\n",
    "    hyper_min_per_stratum = widgets.SelectMultiple(\n",
    "        options=[100, 300, 500, 600, 700, 800, 900, 1000, 2000, 3000, 4000, 5000],\n",
    "        value=[800, 1000, 2000, 3000],\n",
    "        description='min_per_stratum:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '150px'}\n",
    "    )\n",
    "    hyper_abs_error = widgets.SelectMultiple(\n",
    "        options=[0.01, 0.02, 0.25, 0.03],\n",
    "        value=[0.02],\n",
    "        description='abs_error_margin:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '150px'}\n",
    "    )\n",
    "    hyper_days = widgets.IntText(\n",
    "        value=30,\n",
    "        description='days:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '150px'}\n",
    "    )\n",
    "    hyper_daily_reviews = widgets.SelectMultiple(\n",
    "        options=[1000, 1500, 2000, 2500, 3000, 3500, 4000, 5000, 6000],\n",
    "        value=[2000, 2500, 3000, 4000],\n",
    "        description='daily_reviews:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '150px'}\n",
    "    )\n",
    "\n",
    "    # Widgets for constraints\n",
    "    constraint_cost = widgets.IntText(\n",
    "        value=100000,\n",
    "        description='Max Cost:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '150px'}\n",
    "    )\n",
    "    constraint_margin = widgets.FloatText(\n",
    "        value=0.02,\n",
    "        description='Max Overall Margin:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '150px'}\n",
    "    )\n",
    "    constraint_share_top3 = widgets.FloatText(\n",
    "        value=0.5,\n",
    "        description='Max Top3 Share:',\n",
    "        layout=widgets.Layout(width='90%'),\n",
    "        style={'description_width': '150px'}\n",
    "    )\n",
    "\n",
    "    hyperparam_section = widgets.VBox([\n",
    "        widgets.HTML(\"<h4>Hyperparameter Search Settings</h4>\"),\n",
    "        widgets.HTML(\"<b>Parameter Grid:</b>\"),\n",
    "        hyper_min_per_stratum,\n",
    "        hyper_abs_error,\n",
    "        hyper_days,\n",
    "        hyper_daily_reviews,\n",
    "        widgets.HTML(\"<b>Constraints:</b>\"),\n",
    "        constraint_cost,\n",
    "        constraint_margin,\n",
    "        constraint_share_top3\n",
    "    ])\n",
    "\n",
    "    # --- Group widgets into titled sections for better readability ---\n",
    "    section_metric = widgets.VBox([\n",
    "        widgets.HTML(\"<h4>Metric & Error Targets</h4>\"),\n",
    "        primary_product_widget,\n",
    "        metric_widget,\n",
    "        overall_error_widget,\n",
    "        qa_total_source_widget,\n",
    "        daily_reviews_widget,   \n",
    "        days_widget,           \n",
    "        # dim_error_widget,\n",
    "        top_n_widget\n",
    "    ])\n",
    "\n",
    "    section_price_per_session = widgets.VBox([\n",
    "        widgets.HTML(\"<h4>Price Per Reviewed Session</h4>\"),\n",
    "        cost_per_session_widget\n",
    "    ])\n",
    "\n",
    "    section_small_customer = widgets.VBox([\n",
    "        widgets.HTML(\"<h4>Small Customer Weighting (For Oversampling Simulation)</h4>\"),\n",
    "        weight_small_customers_widget,\n",
    "        small_customer_quantile_widget\n",
    "    ])\n",
    "\n",
    "    section_error_margin = widgets.VBox([\n",
    "        widgets.HTML(\"<h4>Error Margin Settings</h4>\"),\n",
    "        error_margin_logic_widget,\n",
    "        abs_error_value_widget,\n",
    "        rel_error_value_widget,\n",
    "        min_abs_error_widget\n",
    "    ])\n",
    "\n",
    "    section_dimensions = widgets.VBox([\n",
    "        widgets.HTML(\"<h4>Reporting & Stratification</h4>\"),\n",
    "        reporting_dims_widget,\n",
    "        stratify_dims_widget,\n",
    "        min_per_stratum_widget\n",
    "    ])\n",
    "\n",
    "    section_simulation = widgets.VBox([\n",
    "        widgets.HTML(\"<h4>Simulation Scenarios</h4>\"),\n",
    "        sampling_types_widget,\n",
    "        save_plots_html_widget\n",
    "    ])\n",
    "\n",
    "    main_layout = widgets.VBox([\n",
    "        section_metric,\n",
    "        section_price_per_session,\n",
    "        qa_proportion_box,\n",
    "        section_small_customer,\n",
    "        section_error_margin,\n",
    "        section_dimensions,\n",
    "        section_simulation,\n",
    "    ])\n",
    "\n",
    "    def on_run_report(b):\n",
    "        with output_area:\n",
    "            clear_output(wait=True)\n",
    "            filtered_df = df_data[df_data['primary_product'].isin(list(primary_product_widget.value))]\n",
    "            # Collect QA proportions from widgets\n",
    "            platinum = qa_proportion_widgets['Tier Platinum'].value\n",
    "            tier1 = qa_proportion_widgets['Tier 1'].value\n",
    "            remaining = max(0.0, 1.0 - platinum - tier1)\n",
    "            per_tier = remaining / len(auto_tiers) if auto_tiers else 0.0\n",
    "            qa_proportions = {\n",
    "                'Tier Platinum': platinum,\n",
    "                'Tier 1': tier1,\n",
    "            }\n",
    "            for tier in auto_tiers:\n",
    "                qa_proportions[tier] = per_tier\n",
    "            current_params = {\n",
    "                'qa_total_source': qa_total_source_widget.value,\n",
    "                'daily_reviews': daily_reviews_widget.value,\n",
    "                'days': days_widget.value,\n",
    "                'qa_proportions': qa_proportions,\n",
    "                'selected_metric': metric_widget.value,\n",
    "                'target_overall_error_percent': overall_error_widget.value,\n",
    "                'target_dimension_error_percent': dim_error_widget.value,\n",
    "                'confidence_level': confidence_widget.value,\n",
    "                'review_cost_per_session': cost_per_session_widget.value,\n",
    "                'predefined_total_cost_budget': budget_widget.value,\n",
    "                'weight_factor_small_customers': weight_small_customers_widget.value,\n",
    "                'small_customer_volume_quantile_threshold': small_customer_quantile_widget.value,\n",
    "                'error_margin_logic': error_margin_logic_widget.value,\n",
    "                'absolute_error_margin_value': abs_error_value_widget.value,\n",
    "                'relative_error_margin_value': rel_error_value_widget.value,\n",
    "                'min_absolute_error_margin': min_abs_error_widget.value,\n",
    "                'reporting_dimensions': list(reporting_dims_widget.value),\n",
    "                'stratify_by_dimension': list(stratify_dims_widget.value),\n",
    "                'min_per_stratum': min_per_stratum_widget.value,\n",
    "                'top_n': top_n_widget.value,\n",
    "                'sampling_types_to_run': list(sampling_types_widget.value),\n",
    "                'save_plots_to_html': save_plots_html_widget.value\n",
    "            }\n",
    "            #current_params_global = current_params\n",
    "            generate_report(filtered_df, current_params)\n",
    "            #best_result = find_optimal_relative_error_margin(df, current_params_global, margin_range=(0.2, 0.5), step=0.1, verbose=False)\n",
    "            #print(f\"\\nBest Relative Error Margin found: {best_result['Relative Error Margin']:.2f} with overall margin: {best_result['Overall Margin (%)']:.2f}% and min cost per unit: {best_result['Min Cost per Unit']:.2f} EUR\")\n",
    "\n",
    "    def on_hyperparam_search(b):\n",
    "        with hyperparam_output_area:\n",
    "            clear_output(wait=True)\n",
    "            filtered_df = df_data[df_data['primary_product'].isin(list(primary_product_widget.value))]\n",
    "            platinum = qa_proportion_widgets['Tier Platinum'].value\n",
    "            tier1 = qa_proportion_widgets['Tier 1'].value\n",
    "            remaining = max(0.0, 1.0 - platinum - tier1)\n",
    "            per_tier = remaining / len(auto_tiers) if auto_tiers else 0.0\n",
    "            qa_proportions = {'Tier Platinum': platinum, 'Tier 1': tier1}\n",
    "            for tier in auto_tiers:\n",
    "                qa_proportions[tier] = per_tier\n",
    "            current_params = {\n",
    "                'qa_total_source': qa_total_source_widget.value,\n",
    "                'daily_reviews': daily_reviews_widget.value,\n",
    "                'days': days_widget.value,\n",
    "                'qa_proportions': qa_proportions,\n",
    "                'selected_metric': metric_widget.value,\n",
    "                'target_overall_error_percent': overall_error_widget.value,\n",
    "                'target_dimension_error_percent': dim_error_widget.value,\n",
    "                'confidence_level': confidence_widget.value,\n",
    "                'review_cost_per_session': cost_per_session_widget.value,\n",
    "                'predefined_total_cost_budget': budget_widget.value,\n",
    "                'weight_factor_small_customers': weight_small_customers_widget.value,\n",
    "                'small_customer_volume_quantile_threshold': small_customer_quantile_widget.value,\n",
    "                'error_margin_logic': error_margin_logic_widget.value,\n",
    "                'absolute_error_margin_value': abs_error_value_widget.value,\n",
    "                'relative_error_margin_value': rel_error_value_widget.value,\n",
    "                'min_absolute_error_margin': min_abs_error_widget.value,\n",
    "                'reporting_dimensions': list(reporting_dims_widget.value),\n",
    "                'stratify_by_dimension': list(stratify_dims_widget.value),\n",
    "                'min_per_stratum': min_per_stratum_widget.value,\n",
    "                'top_n': top_n_widget.value,\n",
    "                'sampling_types_to_run': list(sampling_types_widget.value),\n",
    "                'save_plots_to_html': save_plots_html_widget.value\n",
    "            }\n",
    "\n",
    "            # Build param_grid and constraints from widget values\n",
    "            param_grid = {\n",
    "                'min_per_stratum': list(hyper_min_per_stratum.value),\n",
    "                'absolute_error_margin_value': list(hyper_abs_error.value),\n",
    "                'days': [hyper_days.value],\n",
    "                'daily_reviews': list(hyper_daily_reviews.value),\n",
    "            }\n",
    "            constraints = {\n",
    "                'cost_total': lambda x: x < constraint_cost.value,\n",
    "                'overall_margin': lambda x: x < constraint_margin.value,\n",
    "                'share_top_3_customers': lambda x: x < constraint_share_top3.value,\n",
    "            }\n",
    "\n",
    "            # results_df, best_row = hyperparameter_simulation_search(\n",
    "            #     df_data=filtered_df,\n",
    "            #     base_params=current_params,\n",
    "            #     param_grid=param_grid,\n",
    "            #     constraints=constraints,\n",
    "            #     scoring_fn=None,\n",
    "            #     max_workers=8\n",
    "            # )\n",
    "            from IPython.display import display, HTML\n",
    "            import io, sys\n",
    "            # old_stdout = sys.stdout\n",
    "            # captured_output = io.StringIO()\n",
    "            # sys.stdout = captured_output\n",
    "            # print(\"=== Best Row (Optimal Configuration) ===\")\n",
    "            # print_hyper_search_summary(\n",
    "            #     best_row,\n",
    "            #     error_margin_logic=current_params.get('error_margin_logic', 'Absolute Error Margin'),\n",
    "            #     abs_margin_value=current_params.get('absolute_error_margin_value', 0.02),\n",
    "            #     rel_margin_value=current_params.get('relative_error_margin_value', 0.2),\n",
    "            #     results_df=results_df\n",
    "            # )\n",
    "            # sys.stdout = old_stdout\n",
    "            # display(HTML(f\"<pre>{captured_output.getvalue()}</pre>\"))\n",
    "            # print(\"\\n=== Best Row Details ===\")\n",
    "            # if best_row is not None and not isinstance(best_row, pd.DataFrame):\n",
    "            #     display(pd.DataFrame([best_row]))\n",
    "            # else:\n",
    "            #     display(best_row)\n",
    "            # print(\"\\n=== All Results ===\")\n",
    "            # display(results_df)\n",
    "\n",
    "\n",
    "            from skopt.space import Integer, Real\n",
    "\n",
    "            # Example: dynamically build param_space based on widget values\n",
    "            param_space = []\n",
    "            param_names = []\n",
    "\n",
    "            if len(hyper_min_per_stratum.value) > 0:\n",
    "                min_val = min(hyper_min_per_stratum.value)\n",
    "                max_val = max(hyper_min_per_stratum.value)\n",
    "                param_space.append(Integer(min_val, max_val, name='min_per_stratum'))\n",
    "                param_names.append('min_per_stratum')\n",
    "\n",
    "            # if len(hyper_abs_error.value) > 0:\n",
    "            #     min_val = min(hyper_abs_error.value)\n",
    "            #     max_val = max(hyper_abs_error.value)\n",
    "            #     # Use Real if your error margin values are floats\n",
    "            #     param_space.append(Real(min_val, max_val, name='absolute_error_margin_value'))\n",
    "            #     param_names.append('absolute_error_margin_value')\n",
    "\n",
    "            if len(hyper_daily_reviews.value) > 0:\n",
    "                min_val = min(hyper_daily_reviews.value)\n",
    "                max_val = max(hyper_daily_reviews.value)\n",
    "                param_space.append(Integer(min_val, max_val, name='daily_reviews'))\n",
    "                param_names.append('daily_reviews')\n",
    "\n",
    "            # Now use param_space and param_names in skopt_hyperparameter_search\n",
    "            results_df, best_row, columns = skopt_hyperparameter_search(\n",
    "                filtered_df, current_params, param_space, param_names, n_calls=50, constraints=constraints, scenario_names=list(sampling_types_widget.value) if sampling_types_widget.value else None\n",
    "            )\n",
    "\n",
    "            from IPython.display import display\n",
    "            print(\"=== Best Row (Optimal Configuration) ===\")\n",
    "            display(best_row.to_frame().T)\n",
    "            print(\"=== Top 5 Results ===\")\n",
    "            display(results_df.sort_values(\n",
    "                by=['customer_coverage', 'product_coverage', 'industry_coverage', 'document_country_coverage', 'region_coverage', 'cost_total'],\n",
    "                ascending=[False, False, False, False, False, True]\n",
    "            ).head(100))\n",
    "            \n",
    "            display(columns)\n",
    "            \n",
    "    run_button.on_click(on_run_report)\n",
    "    hyperparam_button.on_click(on_hyperparam_search)\n",
    "\n",
    "    display(widgets.VBox([\n",
    "    main_layout,\n",
    "    widgets.HBox([run_button]),\n",
    "    hyperparam_section,\n",
    "    widgets.HBox([hyperparam_button]),\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    widgets.HTML(\"<h4>Output</h4>\"),\n",
    "    output_area,\n",
    "    hyperparam_output_area\n",
    "    ]))\n",
    "\n",
    "    #display(main_layout)\n",
    "\n",
    "\n",
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "# Expand the display to fit the content\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4672b698",
   "metadata": {},
   "source": [
    "# REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "52d04c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fec483c48f54667a65c56523e6d4611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(VBox(children=(HTML(value='<h4>Metric & Error Targets</h4>'), SelectMultiple(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_interactive_ui(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "867db713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_top_20\n",
       "Aplazo                                            165232\n",
       "Arab Banking Corporation (Bank ABC/ Ila Bank)     256088\n",
       "Bumble                                            624006\n",
       "FacePhi                                           452961\n",
       "Feeld                                             301438\n",
       "ID.me                                            2369938\n",
       "Instacart                                        1978005\n",
       "Kueski                                            300349\n",
       "Monzo                                             447456\n",
       "Paypal                                            906094\n",
       "Rest                                             3109786\n",
       "SoFi                                              163037\n",
       "Stake                                             720190\n",
       "Stripe - AWS                                      143322\n",
       "Tabby                                             347035\n",
       "TransferWise IDV                                  361826\n",
       "UK OneLogin                                       243409\n",
       "Uber Main Account                                8239257\n",
       "Uphold Live                                       156323\n",
       "Verimi Prod                                       177745\n",
       "krediya                                           214646\n",
       "Name: volume, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('customer_top_20').volume.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3c45faef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Top 10 Products by Count'}, xlabel='primary_product'>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby('primary_product').volume.sum().sort_values(ascending=False).head(10).plot(kind='bar', title='Top 10 Products by Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "650a84ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_top_20\n",
       "Aplazo                                                                                                                                                                                                                                                                                                                                                                                                                     [Software]\n",
       "Arab Banking Corporation (Bank ABC/ Ila Bank)                                                                                                                                                                                                                                                                                                                                                                               [Finance]\n",
       "Bumble                                                                                                                                                                                                                                                                                                                                                                                                             [Media & Internet]\n",
       "FacePhi                                                                                                                                                                                                                                                                                                                                                                                                                    [Software]\n",
       "Feeld                                                                                                                                                                                                                                                                                                                                                                                                             [Business Services]\n",
       "ID.me                                                                                                                                                                                                                                                                                                                                                                                                                      [Software]\n",
       "Instacart                                                                                                                                                                                                                                                                                                                                                                                                                    [Retail]\n",
       "Kueski                                                                                                                                                                                                                                                                                                                                                                                                                      [Finance]\n",
       "Monzo                                                                                                                                                                                                                                                                                                                                                                                                                       [Finance]\n",
       "Other                                            [Software, Hospitality, Retail, Telecommunications, Finance, UNKNOWN, Business Services, Real Estate, Manufacturing, Media & Internet, Law Firms & Legal Services, Consumer Services, Education, Transportation, Healthcare Services, Insurance, Construction, Energy, Utilities & Waste, Hospitals & Physicians Clinics, Minerals & Mining, Agriculture, Government, Organizations]\n",
       "Paypal                                                                                                                                                                                                                                                                                                                                                                                                                      [Finance]\n",
       "SoFi                                                                                                                                                                                                                                                                                                                                                                                                                        [Finance]\n",
       "Stake                                                                                                                                                                                                                                                                                                                                                                                                                   [Hospitality]\n",
       "Stripe - AWS                                                                                                                                                                                                                                                                                                                                                                                                               [Software]\n",
       "Tabby                                                                                                                                                                                                                                                                                                                                                                                                                      [Software]\n",
       "TransferWise IDV                                                                                                                                                                                                                                                                                                                                                                                                            [Finance]\n",
       "UK OneLogin                                                                                                                                                                                                                                                                                                                                                                                                       [Business Services]\n",
       "Uber Main Account                                                                                                                                                                                                                                                                                                                                                                                                    [Transportation]\n",
       "Uphold Live                                                                                                                                                                                                                                                                                                                                                                                                                [Software]\n",
       "Verimi Prod                                                                                                                                                                                                                                                                                                                                                                                                       [Consumer Services]\n",
       "krediya                                                                                                                                                                                                                                                                                                                                                                                                                     [Finance]\n",
       "Name: industry, dtype: object"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('customer_top_20').industry.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fb3a11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".coding-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
